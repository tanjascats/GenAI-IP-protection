
@misc{hanu_detoxify_2020,
	title = {Detoxify},
	copyright = {Apache License 2.0, Open Access},
	url = {https://zenodo.org/record/7925667},
	abstract = {Trained models \&amp; code to predict toxic comments on 3 Jigsaw challenges: Toxic comment classification, Unintended Bias in Toxic comments, Multilingual toxic comment classification.},
	urldate = {2024-04-20},
	publisher = {[object Object]},
	author = {Hanu, Laura and {Unitary Team}},
	collaborator = {Thewlis, James and Borovec, Jirka and Verö, Anita and Rossetti, Mike},
	month = nov,
	year = {2020},
	note = {https://github.com/unitaryai/detoxify (accessed 204-01-31)},
	keywords = {hate speech, toxici comment detection, toxicity, transformers},
}

@inproceedings{qin_destruction-restoration_2023,
	address = {Atlanta, GA, USA},
	title = {Destruction-{Restoration} {Suppresses} {Data} {Protection} {Perturbations} against {Diffusion} {Models}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350342734},
	url = {https://ieeexplore.ieee.org/document/10356476/},
	doi = {10.1109/ICTAI59109.2023.00093},
	urldate = {2024-04-19},
	booktitle = {35th {International} {Conference} on {Tools} with {Artificial} {Intelligence} ({ICTAI})},
	publisher = {IEEE},
	author = {Qin, Tianrui and Gao, Xitong and Zhao, Juanjuan and Ye, Kejiang},
	month = nov,
	year = {2023},
	pages = {586--594},
}

@inproceedings{meng_magnet_2017,
	address = {Dallas Texas USA},
	title = {{MagNet}: {A} {Two}-{Pronged} {Defense} against {Adversarial} {Examples}},
	isbn = {978-1-4503-4946-8},
	shorttitle = {{MagNet}},
	url = {https://dl.acm.org/doi/10.1145/3133956.3134057},
	doi = {10.1145/3133956.3134057},
	language = {en},
	urldate = {2024-04-17},
	booktitle = {{ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security} ({CCS})},
	publisher = {ACM},
	author = {Meng, Dongyu and Chen, Hao},
	month = oct,
	year = {2017},
	pages = {135--147},
}

@article{lemley_fair_2021,
	title = {Fair {Learning}},
	volume = {99},
	url = {https://heinonline.org/HOL/Page?handle=hein.journals/tlr99&id=777&div=&collection=},
	number = {4},
	journal = {Texas Law Review},
	author = {Lemley, Mark A. and Casey, Bryan},
	year = {2021},
	pages = {743},
}

@misc{koziol_stable_2023,
	title = {Stable {Attribution} {Identifies} the {Art} {Behind} {AI} {Images}},
	url = {https://spectrum.ieee.org/ai-art-generator},
	abstract = {Anton Troynikov hopes to keep both artists and engineers happy},
	language = {en},
	urldate = {2024-04-11},
	journal = {IEEE Spectrum},
	author = {Koziol, Michael},
	month = apr,
	year = {2023},
	note = {https://spectrum.ieee.org/ai-art-generator (accessed 2024-01-31)},
}

@inproceedings{wu_sepmark_2023,
	address = {Ottawa ON Canada},
	title = {{SepMark}: {Deep} {Separable} {Watermarking} for {Unified} {Source} {Tracing} and {Deepfake} {Detection}},
	isbn = {9798400701085},
	shorttitle = {{SepMark}},
	url = {https://dl.acm.org/doi/10.1145/3581783.3612471},
	doi = {10.1145/3581783.3612471},
	language = {en},
	urldate = {2024-04-20},
	booktitle = {31st {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Wu, Xiaoshuai and Liao, Xin and Ou, Bo},
	month = oct,
	year = {2023},
	keywords = {deep watermarking, deepfake forensics, watermarking robustness},
	pages = {1190--1201},
}

@inproceedings{liu_detecting_2022,
	address = {Tel Aviv, Israel},
	title = {Detecting {Generated} {Images} by {Real} {Images}},
	isbn = {978-3-031-19781-9},
	doi = {10.1007/978-3-031-19781-9_6},
	abstract = {The widespread of generative models have called into question the authenticity of many things on the web. In this situation, the task of image forensics is urgent. The existing methods examine generated images and claim a forgery by detecting visual artifacts or invisible patterns, resulting in generalization issues. We observed that the noise pattern of real images exhibits similar characteristics in the frequency domain, while the generated images are far different. Therefore, we can perform image authentication by checking whether an image follows the patterns of authentic images. The experiments show that a simple classifier using noise patterns can easily detect a wide range of generative models, including GAN and flow-based models. Our method achieves state-of-the-art performance on both low- and high-resolution images from a wide range of generative models and shows superior generalization ability to unseen models. The code is available at https://github.com/Tangsenghenshou/Detecting-Generated-Images-by-Real-Images.},
	language = {en},
	booktitle = {European {Conference} on {Computer} {Vision} ({ECCV})},
	publisher = {Springer Nature Switzerland},
	author = {Liu, Bo and Yang, Fan and Bi, Xiuli and Xiao, Bin and Li, Weisheng and Gao, Xinbo},
	year = {2022},
	keywords = {Forgery detection, Frequency domain analysis, GAN, Generated images, Image forensics, Image noise},
	pages = {95--110},
}

@article{tang_science_2024,
	title = {The {Science} of {Detecting} {LLM}-{Generated} {Text}},
	volume = {67},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3624725},
	doi = {10.1145/3624725},
	abstract = {While many detection methods have been proposed, understanding the challenges is far more daunting.},
	language = {en},
	number = {4},
	urldate = {2024-04-20},
	journal = {Communications of the ACM},
	author = {Tang, Ruixiang and Chuang, Yu-Neng and Hu, Xia},
	month = apr,
	year = {2024},
	pages = {50--59},
}

@inproceedings{wang_faketagger_2021,
	address = {Virtual Event China},
	title = {{FakeTagger}: {Robust} {Safeguards} against {DeepFake} {Dissemination} via {Provenance} {Tracking}},
	isbn = {978-1-4503-8651-7},
	shorttitle = {{FakeTagger}},
	url = {https://dl.acm.org/doi/10.1145/3474085.3475518},
	doi = {10.1145/3474085.3475518},
	language = {en},
	urldate = {2024-04-20},
	booktitle = {29th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Wang, Run and Juefei-Xu, Felix and Luo, Meng and Liu, Yang and Wang, Lina},
	month = oct,
	year = {2021},
	keywords = {deepfake forensics, image tagging, provenance tracking},
	pages = {3546--3555},
}

@inproceedings{ruiz_practical_2023,
	address = {Washington DC, USA},
	title = {Practical {Disruption} of {Image} {Translation} {Deepfake} {Networks}},
	volume = {37},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/26693},
	doi = {10.1609/aaai.v37i12.26693},
	abstract = {By harnessing the latest advances in deep learning, image-to-image translation architectures have recently achieved impressive capabilities. Unfortunately, the growing representational power of these architectures has prominent unethical uses. Among these, the threats of (1) face manipulation ("DeepFakes") used for misinformation or pornographic use (2) "DeepNude" manipulations of body images to remove clothes from individuals, etc. Several works tackle the task of disrupting such image translation networks by inserting imperceptible adversarial attacks into the input image. Nevertheless, these works have limitations that may result in disruptions that are not practical in the real world. Specifically, most works generate disruptions in a white-box scenario, assuming perfect knowledge about the image translation network. The few remaining works that assume a black-box scenario require a large number of queries to successfully disrupt the adversary's image translation network. In this work we propose Leaking Transferable Perturbations (LTP), an algorithm that significantly reduces the number of queries needed to disrupt an image translation network by dynamically re-purposing previous disruptions into new query efficient disruptions.},
	urldate = {2024-04-20},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Ruiz, Nataniel and Bargal, Sarah Adel and Xie, Cihang and Sclaroff, Stan},
	month = jun,
	year = {2023},
	keywords = {General},
	pages = {14478--14486},
}

@article{dong_restricted_2023,
	title = {Restricted {Black}-{Box} {Adversarial} {Attack} {Against} {DeepFake} {Face} {Swapping}},
	volume = {18},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1556-6013, 1556-6021},
	url = {https://ieeexplore.ieee.org/document/10100731/},
	doi = {10.1109/TIFS.2023.3266702},
	urldate = {2024-04-20},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Dong, Junhao and Wang, Yuan and Lai, Jianhuang and Xie, Xiaohua},
	year = {2023},
	keywords = {Closed box, DeepFake, Deepfakes, Faces, Glass box, Image reconstruction, Perturbation methods, Training, adversarial attack, black-box, deepfake, substitute model},
	pages = {2596--2608},
}

@inproceedings{ruiz_disrupting_2020,
	address = {Glasgow, UK},
	title = {Disrupting {Deepfakes}: {Adversarial} {Attacks} {Against} {Conditional} {Image} {Translation} {Networks} and {Facial} {Manipulation} {Systems}},
	isbn = {978-3-030-66823-5},
	shorttitle = {Disrupting {Deepfakes}},
	doi = {10.1007/978-3-030-66823-5_14},
	abstract = {Face modification systems using deep learning have become increasingly powerful and accessible. Given images of a person’s face, such systems can generate new images of that same person under different expressions and poses. Some systems can also modify targeted attributes such as hair color or age. This type of manipulated images and video have been coined Deepfakes. In order to prevent a malicious user from generating modified images of a person without their consent we tackle the new problem of generating adversarial attacks against such image translation systems, which disrupt the resulting output image. We call this problem disrupting deepfakes. Most image translation architectures are generative models conditioned on an attribute (e.g. put a smile on this person’s face). We are first to propose and successfully apply (1) class transferable adversarial attacks that generalize to different classes, which means that the attacker does not need to have knowledge about the conditioning class, and (2) adversarial training for generative adversarial networks (GANs) as a first step towards robust image translation networks. Finally, in our scenario, the deepfaker can adaptively blur the image and potentially mount a successful defense against disruption. We present a spread-spectrum adversarial attack, which evades blur defenses. We open-source our code.},
	language = {en},
	booktitle = {European {Conference} on {Computer} {Vision} ({ECCV}) {Workshops}},
	publisher = {Springer International Publishing},
	author = {Ruiz, Nataniel and Bargal, Sarah Adel and Sclaroff, Stan},
	year = {2020},
	keywords = {Adversarial attacks, Deepfake, Face modification, GAN, Generative models, Image translation, Privacy},
	pages = {236--251},
}

@inproceedings{chen_gmail_2019,
	address = {Anchorage AK USA},
	title = {Gmail {Smart} {Compose}: {Real}-{Time} {Assisted} {Writing}},
	isbn = {978-1-4503-6201-6},
	shorttitle = {Gmail {Smart} {Compose}},
	url = {https://dl.acm.org/doi/10.1145/3292500.3330723},
	doi = {10.1145/3292500.3330723},
	language = {en},
	urldate = {2024-04-20},
	booktitle = {25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Chen, Mia Xu and Lee, Benjamin N. and Bansal, Gagan and Cao, Yuan and Zhang, Shuyuan and Lu, Justin and Tsay, Jackie and Wang, Yinan and Dai, Andrew M. and Chen, Zhifeng and Sohn, Timothy and Wu, Yonghui},
	month = jul,
	year = {2019},
	keywords = {assisted writing, language model, large-scale serving, smart compose},
	pages = {2287--2295},
}

@misc{wiggers_deviantart_2022,
	title = {{DeviantArt} provides a way for artists to opt out of {AI} art generators},
	url = {https://techcrunch.com/2022/11/11/deviantart-provides-a-way-for-artists-to-opt-out-of-ai-art-generators/},
	abstract = {DeviantArt is introducing a way for artists to exclude their work from AI training sets. It's also launching its own generative art tool.},
	language = {en-US},
	urldate = {2023-07-22},
	journal = {TechCrunch},
	author = {Wiggers, Kyle},
	month = nov,
	year = {2022},
	note = {https://techcrunch.com/2022/11/11/deviantart-provides-a-way-for-artists-to-opt-out-of-ai-art-generators/ (accessed 2024-01-31)},
	keywords = {blog, opt-out, protection},
}

@misc{wu_how_2023,
	title = {How to {Know} if {Your} {Images} {Trained} an {AI} {Model} (and {How} to {Opt} {Out})},
	url = {https://www.makeuseof.com/how-to-know-images-trained-ai-art-generator/},
	abstract = {Were your images used to train an AI art generator without your permission? Here's how to find out, and opt out!},
	language = {en},
	urldate = {2023-07-21},
	journal = {MUO},
	author = {Wu, Garling},
	month = jan,
	year = {2023},
	note = {https://www.makeuseof.com/how-to-know-images-trained-ai-art-generator/ (accessed 204-01-31)},
	keywords = {blog},
}

@inproceedings{ruta_aladin_2021,
	address = {Montreal, QC, Canada},
	title = {{ALADIN}: {All} {Layer} {Adaptive} {Instance} {Normalization} for {Fine}-grained {Style} {Similarity}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66542-812-5},
	shorttitle = {{ALADIN}},
	url = {https://ieeexplore.ieee.org/document/9711450/},
	doi = {10.1109/ICCV48922.2021.01171},
	urldate = {2024-04-20},
	booktitle = {{IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Ruta, Dan and Motiian, Saeid and Faieta, Baldo and Lin, Zhe and Jin, Hailin and Filipkowski, Alex and Gilbert, Andrew and Collomosse, John},
	month = oct,
	year = {2021},
	keywords = {feature space},
	pages = {11906--11915},
}

@inproceedings{caron_emerging_2021,
	address = {Montreal, QC, Canada},
	title = {Emerging {Properties} in {Self}-{Supervised} {Vision} {Transformers}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9709990/},
	doi = {10.1109/ICCV48922.2021.00951},
	urldate = {2024-04-20},
	booktitle = {{IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jegou, Herve and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	month = oct,
	year = {2021},
	keywords = {feature space},
	pages = {9630--9640},
}

@inproceedings{yu_artificial_2021,
	address = {Montreal, QC, Canada},
	title = {Artificial {Fingerprinting} for {Generative} {Models}: {Rooting} {Deepfake} {Attribution} in {Training} {Data}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66542-812-5},
	shorttitle = {Artificial {Fingerprinting} for {Generative} {Models}},
	url = {https://ieeexplore.ieee.org/document/9711167/},
	doi = {10.1109/ICCV48922.2021.01418},
	urldate = {2024-04-20},
	booktitle = {{IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Yu, Ning and Skripniuk, Vladislav and Abdelnabi, Sahar and Fritz, Mario},
	month = oct,
	year = {2021},
	keywords = {deepfake attribution, potential application for IPinGenAI},
	pages = {14428--14437},
}

@inproceedings{zhu_hidden_2018,
	address = {Munich, Germany},
	title = {{HiDDeN}: {Hiding} {Data} {With} {Deep} {Networks}},
	isbn = {978-3-030-01266-3 978-3-030-01267-0},
	shorttitle = {{HiDDeN}},
	url = {https://link.springer.com/10.1007/978-3-030-01267-0_40},
	language = {en},
	urldate = {2024-04-20},
	booktitle = {European {Conference} on {Computer} {Vision} ({ECCV})},
	publisher = {Springer International Publishing},
	author = {Zhu, Jiren and Kaplan, Russell and Johnson, Justin and Fei-Fei, Li},
	year = {2018},
	pages = {682--697},
}

@inproceedings{navas_dwt-dct-svd_2008,
	address = {Bangalore, India},
	title = {{DWT}-{DCT}-{SVD} based watermarking},
	isbn = {978-1-4244-1796-4},
	url = {http://ieeexplore.ieee.org/document/4554423/},
	doi = {10.1109/COMSWA.2008.4554423},
	urldate = {2024-04-20},
	booktitle = {3rd {International} {Conference} on {Communication} {Systems} {Software} and {Middleware} and {Workshops} ({COMSWARE})},
	publisher = {IEEE},
	author = {Navas, K. A. and Ajay, Mathews Cheriyan and Lekshmi, M. and Archana, Tampy S. and Sasikumar, M.},
	month = jan,
	year = {2008},
	keywords = {DWT, Data mining, Detectors, Discrete cosine transforms, Discrete wavelet transforms, Educational institutions, Frequency domain analysis, Image coding, Robustness, SVD, Transform coding, Watermarking, robust, watermarking},
	pages = {271--274},
}

@inproceedings{lu_quark_2022,
	title = {{QUARK}: {Controllable} {Text} {Generation} with {Reinforced} {Unlearning}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b125999bde7e80910cbdbd323087df8f-Paper-Conference.pdf},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lu, Ximing and Welleck, Sean and Hessel, Jack and Jiang, Liwei and Qin, Lianhui and West, Peter and Ammanabrolu, Prithviraj and Choi, Yejin},
	year = {2022},
	pages = {27591--27609},
}

@inproceedings{kandpal_large_2023,
	title = {Large {Language} {Models} {Struggle} to {Learn} {Long}-{Tail} {Knowledge}},
	url = {https://proceedings.mlr.press/v202/kandpal23a.html},
	abstract = {The Internet contains a wealth of knowledge—from the birthdays of historical figures to tutorials on how to code—all of which may be learned by language models. However, while certain pieces of information are ubiquitous on the web, others appear extremely rarely. In this paper, we study the relationship between the knowledge memorized by large language models and the information in pre-training datasets scraped from the web. In particular, we show that a language model’s ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models are better at learning long-tail knowledge, we estimate that today’s models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data. Finally, we show that retrieval-augmentation can reduce the dependence on relevant pre-training information, presenting a promising approach for capturing the long-tail.},
	booktitle = {40th {International} {Conference} on {Machine} {Learning} ({ICML})},
	publisher = {PMLR},
	author = {Kandpal, Nikhil and Deng, Haikang and Roberts, Adam and Wallace, Eric and Raffel, Colin},
	month = jul,
	year = {2023},
	keywords = {duplication, exploitable properties},
	pages = {15696--15707},
}

@inproceedings{lu_specialist_2023,
	address = {Vancouver, BC, Canada},
	title = {Specialist {Diffusion}: {Plug}-and-{Play} {Sample}-{Efficient} {Fine}-{Tuning} of {Text}-to-{Image} {Diffusion} {Models} to {Learn} {Any} {Unseen} {Style}},
	isbn = {9798350301298},
	shorttitle = {Specialist {Diffusion}},
	url = {https://ieeexplore.ieee.org/document/10204017/},
	doi = {10.1109/CVPR52729.2023.01371},
	abstract = {Diffusion models have demonstrated impressive capability of text-conditioned image synthesis, and broader application horizons are emerging by personalizing those pretrained diffusion models toward generating some specialized target object or style. In this paper, we aim to learn an unseen style by simply fine-tuning a pre-trained diffusion model with a handful of images (e.g., less than 10), so that the fine-tuned model can generate high-quality images of arbitrary objects in this style. Such extremely lowshot fine-tuning is accomplished by a novel toolkit of finetuning techniques, including text-to-image customized data augmentations, a content loss to facilitate content-style disentanglement, and sparse updating that focuses on only a few time steps. Our framework, dubbed Specialist Diffusion, is plug-and-play to existing diffusion model backbones and other personalization techniques. We demonstrate it to outperform the latest few-shot personalization alternatives of diffusion models such as Textual Inversion [7] and DreamBooth [24], in terms of learning highly sophisticated styles with ultra-sample-efficient tuning. We further show that Specialist Diffusion can be integrated on top of textual inversion to boost performance further, even on highly unusual styles. Our codes are available at: https://github.com/Picsart-AI-Research/ Specialist-Diffusion.},
	language = {en},
	urldate = {2024-02-15},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Lu, Haoming and Tunanyan, Hazarapet and Wang, Kai and Navasardyan, Shant and Wang, Zhangyang and Shi, Humphrey},
	month = jun,
	year = {2023},
	pages = {14267--14276},
}

@inproceedings{liu_detecting_2024,
	address = {San Diego, CA, USA},
	title = {Detecting {Voice} {Cloning} {Attacks} via {Timbre} {Watermarking}},
	isbn = {978-1-891562-93-8},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2024-200-paper.pdf},
	doi = {10.14722/ndss.2024.24200},
	abstract = {Nowadays, it is common to release audio content to the public, for social sharing or commercial purposes. However, with the rise of voice cloning technology, attackers have the potential to easily impersonate a specific person by utilizing his publicly released audio without any permission. Therefore, it becomes significant to detect any potential misuse of the released audio content and protect its timbre from being impersonated.},
	language = {en},
	urldate = {2024-02-23},
	booktitle = {Network and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Liu, Chang and Zhang, Jie and Zhang, Tianwei and Yang, Xi and Zhang, Weiming and Yu, NengHai},
	year = {2024},
	keywords = {audio, text-to-speech, training data ownership, watermark, watermarking},
}

@misc{noauthor_openai_2023,
	title = {{OpenAI} {Moderation}},
	url = {https://platform.openai.com/docs/guides/moderation},
	abstract = {Explore developer resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's platform.},
	language = {en},
	urldate = {2024-04-11},
	year = {2023},
	note = {https://platform.openai.com/docs/guides/moderation (accessed 2024-01-31)},
}

@misc{hanu_how_2021,
	title = {How {AI} {Is} {Learning} to {Identify} {Toxic} {Online} {Content}},
	url = {https://www.scientificamerican.com/article/can-ai-identify-toxic-online-content/},
	abstract = {Machine-learning systems could help flag hateful, threatening or offensive language},
	language = {en},
	urldate = {2024-04-19},
	journal = {Scientific American},
	author = {Hanu, Laura and Thewlis, James and Haco, Sasha},
	month = feb,
	year = {2021},
	note = {https://www.scientificamerican.com/article/can-ai-identify-toxic-online-content/ (accessed 2024-01-31)},
}

@misc{hanu_detoxify_2020-1,
	title = {Detoxify},
	copyright = {Apache-2.0},
	url = {https://github.com/unitaryai/detoxify},
	abstract = {Trained models \& code to predict toxic comments on all 3 Jigsaw Toxic Comment Challenges. Built using ⚡ Pytorch Lightning and 🤗 Transformers. For access to our API, please email us at contact@unitary.ai.},
	urldate = {2024-04-11},
	author = {Hanu, Laura and The Unitary Team},
	month = nov,
	year = {2020},
	doi = {10.5281/zenodo.7925667},
}

@inproceedings{gandikota_unified_2024,
	address = {Waikoloa, HI, USA},
	title = {Unified {Concept} {Editing} in {Diffusion} {Models}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350318920},
	url = {https://ieeexplore.ieee.org/document/10484056/},
	doi = {10.1109/WACV57701.2024.00503},
	urldate = {2024-04-19},
	booktitle = {{IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Gandikota, Rohit and Orgad, Hadas and Belinkov, Yonatan and Materzyńska, Joanna and Bau, David},
	month = jan,
	year = {2024},
	keywords = {concept removal},
	pages = {5099--5108},
}

@inproceedings{kong_data_2023,
	address = {Raleigh, NC, USA},
	title = {Data {Redaction} from {Pre}-trained {GANs}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66546-299-0},
	url = {https://ieeexplore.ieee.org/document/10136171/},
	doi = {10.1109/SaTML54575.2023.00048},
	urldate = {2024-04-19},
	booktitle = {{IEEE} {Conference} on {Secure} and {Trustworthy} {Machine} {Learning} ({SaTML})},
	publisher = {IEEE},
	author = {Kong, Zhifeng and Chaudhuri, Kamalika},
	month = feb,
	year = {2023},
	keywords = {Computational modeling, Costs, Data redaction, Deep learning, Machine learning algorithms, Systematics, Training, Training data, concept removal, post-editing, potential application for IPinGenAI, pre-trained GANs, protection},
	pages = {638--677},
}

@inproceedings{wu_uncovering_2023,
	address = {Vancouver, BC, Canada},
	title = {Uncovering the {Disentanglement} {Capability} in {Text}-to-{Image} {Diffusion} {Models}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350301298},
	url = {https://ieeexplore.ieee.org/document/10204566/},
	doi = {10.1109/CVPR52729.2023.00189},
	urldate = {2024-04-19},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wu, Qiucheng and Liu, Yujian and Zhao, Handong and Kale, Ajinkya and Bui, Trung and Yu, Tong and Lin, Zhe and Zhang, Yang and Chang, Shiyu},
	month = jun,
	year = {2023},
	keywords = {exploitable properties},
	pages = {1900--1910},
}

@inproceedings{liu_metacloak_2024,
	address = {Seattle WA, USA},
	title = {{MetaCloak}: {Preventing} {Unauthorized} {Subject}-driven {Text}-to-image {Diffusion}-based {Synthesis} via {Meta}-learning},
	abstract = {Text-to-image diffusion models allow seamless generation of personalized images from scant reference photos. Yet, these tools, in the wrong hands, can fabricate misleading or harmful content, endangering individuals. To address this problem, existing poisoning-based approaches perturb user images in an imperceptible way to render them "unlearnable" from malicious uses. We identify two limitations of these defending approaches: i) sub-optimal due to the hand-crafted heuristics for solving the intractable bilevel optimization and ii) lack of robustness against simple data transformations like Gaussian filtering. To solve these challenges, we propose MetaCloak, which solves the bi-level poisoning problem with a meta-learning framework with an additional transformation sampling process to craft transferable and robust perturbation. Specifically, we employ a pool of surrogate diffusion models to craft transferable and model-agnostic perturbation. Furthermore, by incorporating an additional transformation process, we design a simple denoising-error maximization loss that is sufficient for causing transformation-robust semantic distortion and degradation in a personalized generation. Extensive experiments on the VGGFace2 and CelebA-HQ datasets show that MetaCloak outperforms existing approaches. Notably, MetaCloak can successfully fool online training services like Replicate, in a black-box manner, demonstrating the effectiveness of MetaCloak in real-world scenarios. Our code is available at https://github.com/liuyixin-louis/MetaCloak.},
	urldate = {2023-12-06},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Liu, Yixin and Fan, Chenrui and Dai, Yutong and Chen, Xun and Zhou, Pan and Sun, Lichao},
	month = jul,
	year = {2024},
	keywords = {Artificial Intelligence (cs.AI), Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Vision and Pattern Recognition (cs.CV), Cryptography and Security (cs.CR), FOS: Computer and information sciences, Training data copyright, cloaking, protection, text-to-image},
}

@inproceedings{kotovenko_content_2019,
	address = {Seoul, Korea (South)},
	title = {Content and {Style} {Disentanglement} for {Artistic} {Style} {Transfer}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9008547/},
	doi = {10.1109/ICCV.2019.00452},
	urldate = {2024-04-19},
	booktitle = {{IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Kotovenko, Dmytro and Sanakoyeu, Artsiom and Lang, Sabine and Ommer, Bjorn},
	month = oct,
	year = {2019},
	pages = {4421--4430},
}

@inproceedings{wu_not_2023,
	address = {Thessaloniki Greece},
	title = {Not {Only} {Generative} {Art}: {Stable} {Diffusion} for {Content}-{Style} {Disentanglement} in {Art} {Analysis}},
	isbn = {9798400701788},
	shorttitle = {Not {Only} {Generative} {Art}},
	url = {https://dl.acm.org/doi/10.1145/3591106.3592262},
	doi = {10.1145/3591106.3592262},
	language = {en},
	urldate = {2024-04-08},
	booktitle = {{ACM} {International} {Conference} on {Multimedia} {Retrieval}},
	publisher = {ACM},
	author = {Wu, Yankun and Nakashima, Yuta and Garcia, Noa},
	month = jun,
	year = {2023},
	pages = {199--208},
}

@inproceedings{liu_artsy-gan_2018,
	address = {Beijing, China},
	title = {Artsy-{GAN}: {A} style transfer system with improved quality, diversity and performance},
	isbn = {978-1-5386-3788-3},
	shorttitle = {Artsy-{GAN}},
	url = {https://ieeexplore.ieee.org/document/8546172/},
	doi = {10.1109/ICPR.2018.8546172},
	urldate = {2024-04-19},
	booktitle = {24th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	publisher = {IEEE},
	author = {Liu, Hanwen and Michelini, Pablo Navarrete and Zhu, Dan},
	month = aug,
	year = {2018},
	keywords = {Diversity reception, Feature extraction, Gallium nitride, Generators, Image color analysis, Painting, Training},
	pages = {79--84},
}

@inproceedings{xu_drb-gan_2021,
	address = {Montreal, QC, Canada},
	title = {{DRB}-{GAN}: {A} {Dynamic} {ResBlock} {Generative} {Adversarial} {Network} for {Artistic} {Style} {Transfer}},
	isbn = {978-1-66542-812-5},
	shorttitle = {{DRB}-{GAN}},
	url = {https://ieeexplore.ieee.org/document/9711396/},
	doi = {10.1109/ICCV48922.2021.00632},
	abstract = {The paper proposes a Dynamic ResBlock Generative Adversarial Network (DRB-GAN) for artistic style transfer. The style code is modeled as the shared parameters for Dynamic ResBlocks connecting both the style encoding network and the style transfer network. In the style encoding network, a style class-aware attention mechanism is used to attend the style feature representation for generating the style codes. In the style transfer network, multiple Dynamic ResBlocks are designed to integrate the style code and the extracted CNN semantic feature and then feed into the spatial window Layer-Instance Normalization (SWLIN) decoder, which enables high-quality synthetic images with artistic style transfer. Moreover, the style collection conditional discriminator is designed to equip our DRBGAN model with abilities for both arbitrary style transfer and collection style transfer during the training stage. No matter for arbitrary style transfer or collection style transfer, extensive experiments strongly demonstrate that our proposed DRB-GAN outperforms state-of-the-art methods and exhibits its superior performance in terms of visual quality and efﬁciency. Our source code is available at https://github.com/xuwenju123/DRB-GAN .},
	language = {en},
	urldate = {2024-02-16},
	booktitle = {{IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Xu, Wenju and Long, Chengjiang and Wang, Ruisheng and Wang, Guanghui},
	month = oct,
	year = {2021},
	pages = {6363--6372},
}

@inproceedings{wang_stylediffusion_2023,
	address = {Paris, France},
	title = {{StyleDiffusion}: {Controllable} {Disentangled} {Style} {Transfer} via {Diffusion} {Models}},
	isbn = {9798350307184},
	shorttitle = {{StyleDiffusion}},
	url = {https://ieeexplore.ieee.org/document/10377803/},
	doi = {10.1109/ICCV51070.2023.00706},
	abstract = {Content and style (C-S) disentanglement is a fundamental problem and critical challenge of style transfer. Existing approaches based on explicit deﬁnitions (e.g., Gram matrix) or implicit learning (e.g., GANs) are neither interpretable nor easy to control, resulting in entangled representations and less satisfying results. In this paper, we propose a new C-S disentangled framework for style transfer without using previous assumptions. The key insight is to explicitly extract the content information and implicitly learn the complementary style information, yielding interpretable and controllable C-S disentanglement and style transfer. A simple yet effective CLIP-based style disentanglement loss coordinated with a style reconstruction prior is introduced to disentangle C-S in the CLIP image space. By further leveraging the powerful style removal and generative ability of diffusion models, our framework achieves superior results than state of the art and ﬂexible C-S disentanglement and trade-off control. Our work provides new insights into the C-S disentanglement in style transfer and demonstrates the potential of diffusion models for learning well-disentangled C-S characteristics.},
	language = {en},
	urldate = {2024-02-14},
	booktitle = {{IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Wang, Zhizhong and Zhao, Lei and Xing, Wei},
	month = oct,
	year = {2023},
	pages = {7643--7655},
}

@inproceedings{collins_editing_2020,
	address = {Seattle, WA, USA},
	title = {Editing in {Style}: {Uncovering} the {Local} {Semantics} of {GANs}},
	isbn = {978-1-72817-168-5},
	shorttitle = {Editing in {Style}},
	url = {https://ieeexplore.ieee.org/document/9157140/},
	doi = {10.1109/CVPR42600.2020.00581},
	abstract = {While the quality of GAN image synthesis has improved tremendously in recent years, our ability to control and condition the output is still limited. Focusing on StyleGAN, we introduce a simple and effective method for making local, semantically-aware edits to a target output image. This is accomplished by borrowing elements from a source image, also a GAN output, via a novel manipulation of style vectors. Our method requires neither supervision from an external model, nor involves complex spatial morphing operations. Instead, it relies on the emergent disentanglement of semantic objects that is learned by StyleGAN during its training. Semantic editing is demonstrated on GANs producing human faces, indoor scenes, cats, and cars. We measure the locality and photorealism of the edits produced by our method, and ﬁnd that it accomplishes both.},
	language = {en},
	urldate = {2024-02-15},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Collins, Edo and Bala, Raja and Price, Bob and Susstrunk, Sabine},
	month = jun,
	year = {2020},
	pages = {5770--5779},
}

@inproceedings{yin_dreaming_2020,
	address = {Seattle, WA, USA},
	title = {Dreaming to {Distill}: {Data}-{Free} {Knowledge} {Transfer} via {DeepInversion}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72817-168-5},
	shorttitle = {Dreaming to {Distill}},
	url = {https://ieeexplore.ieee.org/document/9156864/},
	doi = {10.1109/CVPR42600.2020.00874},
	urldate = {2024-04-19},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yin, Hongxu and Molchanov, Pavlo and Alvarez, Jose M. and Li, Zhizhong and Mallya, Arun and Hoiem, Derek and Jha, Niraj K. and Kautz, Jan},
	month = jun,
	year = {2020},
	keywords = {memorisation},
	pages = {8712--8721},
}

@inproceedings{carlini_secret_2019,
	address = {Santa Clara, CA},
	title = {The {Secret} {Sharer}: {Evaluating} and {Testing} {Unintended} {Memorization} in {Neural} {Networks}},
	isbn = {978-1-939133-06-9},
	url = {https://www.usenix.org/conference/usenixsecurity19/presentation/carlini},
	booktitle = {28th {USENIX} {Security} {Symposium}},
	publisher = {USENIX Association},
	author = {Carlini, Nicholas and Liu, Chang and Erlingsson, Ulfar and Kos, Jernej and Song, Dawn},
	month = aug,
	year = {2019},
	keywords = {memorisation},
	pages = {267--284},
}

@inproceedings{pizzi_self-supervised_2022,
	address = {New Orleans, LA, USA},
	title = {A {Self}-{Supervised} {Descriptor} for {Image} {Copy} {Detection}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9880343/},
	doi = {10.1109/CVPR52688.2022.01413},
	urldate = {2024-04-19},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Pizzi, Ed and Roy, Sreya Dutta and Ravindra, Sugosh Nagavara and Goyal, Priya and Douze, Matthijs},
	month = jun,
	year = {2022},
	keywords = {content moderation, detection of similarity, feature spave, image copy detection, paper, potential application for IPinGenAI, protection},
	pages = {14512--14522},
}

@inproceedings{yoon_diffusion_2023,
	address = {Honolulu, HI, USA},
	title = {Diffusion {Probabilistic} {Models} {Generalize} when {They} {Fail} to {Memorize}},
	url = {https://openreview.net/forum?id=shciCbSk9h},
	abstract = {In this work, we study the training of diffusion probabilistic models through a series of hypotheses and carefully designed experiments. We call our key finding the memorization-generalization dichotomy, and it asserts that generalization and memorization are mutually exclusive phenomena. This contrasts with the modern wisdom of supervised learning that deep neural networks exhibit "benign" overfitting and generalize well despite overfitting the data.},
	language = {en},
	urldate = {2023-12-12},
	booktitle = {{ICML} 2023 {Workshop} on {Structured} {Probabilistic} {Inference} \& {Generative} {Modeling}},
	author = {Yoon, TaeHo and Choi, Joo Young and Kwon, Sehyun and Ryu, Ernest K.},
	month = jul,
	year = {2023},
}

@inproceedings{tirumala_memorization_2022,
	title = {Memorization {Without} {Overfitting}: {Analyzing} the {Training} {Dynamics} of {Large} {Language} {Models}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/fa0509f4dab6807e2cb465715bf2d249-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tirumala, Kushal and Markosyan, Aram and Zettlemoyer, Luke and Aghajanyan, Armen},
	year = {2022},
	keywords = {memorisation},
	pages = {38274--38290},
}

@inproceedings{van_den_burg_memorization_2021,
	title = {On {Memorization} in {Probabilistic} {Deep} {Generative} {Models}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {van den Burg, Gerrit and Williams, Chris},
	year = {2021},
	pages = {27916--27928},
}

@inproceedings{arpit_closer_2017,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {A {Closer} {Look} at {Memorization} in {Deep} {Networks}},
	volume = {70},
	url = {https://proceedings.mlr.press/v70/arpit17a.html},
	abstract = {We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.},
	language = {en},
	booktitle = {34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Arpit, Devansh and Jastrzębski, Stanisław and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S. and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and Lacoste-Julien, Simon},
	month = aug,
	year = {2017},
	keywords = {exploitable properties, memorisation, paper},
	pages = {233--242},
}

@inproceedings{feldman_what_2020,
	title = {What {Neural} {Networks} {Memorize} and {Why}: {Discovering} the {Long} {Tail} via {Influence} {Estimation}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/1e14bfe2714193e7af5abc64ecbd6b46-Abstract.html?ref=dl-staging-website.ghost.io},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Feldman, Vitaly and Zhang, Chiyuan},
	year = {2020},
	keywords = {memorisation},
	pages = {2881--2891},
}

@inproceedings{feldman_does_2020,
	address = {Chicago IL USA},
	title = {Does learning require memorization? a short tale about a long tail},
	isbn = {978-1-4503-6979-4},
	shorttitle = {Does learning require memorization?},
	url = {https://dl.acm.org/doi/10.1145/3357713.3384290},
	doi = {10.1145/3357713.3384290},
	language = {en},
	urldate = {2024-04-19},
	booktitle = {52nd {Annual} {ACM} {SIGACT} {Symposium} on {Theory} of {Computing}},
	publisher = {ACM},
	author = {Feldman, Vitaly},
	month = jun,
	year = {2020},
	keywords = {Generalization, Interpolation, Long-tailed Distribution, Overfitting, Privacy-preserving Learning, memorisation},
	pages = {954--959},
}

@misc{vincent_getty_2023,
	title = {Getty {Images} sues {AI} art generator {Stable} {Diffusion} in the {US} for copyright infringement},
	url = {https://www.theverge.com/2023/2/6/23587393/ai-art-copyright-lawsuit-getty-images-stable-diffusion},
	abstract = {Getty says Stability AI stole 12 million images without permission},
	language = {en},
	urldate = {2024-03-13},
	journal = {The Verge},
	author = {Vincent, James},
	month = feb,
	year = {2023},
	note = {https://www.theverge.com/2023/2/6/23587393/ai-art-copyright-lawsuit-getty-images-stable-diffusion (accessed 2024-01-31)},
}

@article{kahveci_attribution_2023,
	title = {Attribution problem of generative {AI}: a view from {US} copyright law},
	volume = {18},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1747-1532, 1747-1540},
	shorttitle = {Attribution problem of generative {AI}},
	url = {https://academic.oup.com/jiplp/article/18/11/796/7271384},
	doi = {10.1093/jiplp/jpad076},
	language = {en},
	number = {11},
	urldate = {2024-04-19},
	journal = {Journal of Intellectual Property Law and Practice},
	author = {Kahveci, Zeynep Ülkü},
	month = nov,
	year = {2023},
	pages = {796--807},
}

@misc{bogle_new_2023,
	title = {New {York} {Times}, {CNN} and {Australia}’s {ABC} block {OpenAI}’s {GPTBot} web crawler from accessing content},
	url = {https://www.theguardian.com/technology/2023/aug/25/new-york-times-cnn-and-abc-block-openais-gptbot-web-crawler-from-scraping-content},
	abstract = {Chicago Tribune and Australian newspapers the Canberra Times and Newcastle Herald also appear to have disallowed web crawler from maker of Chat GPT},
	language = {en-GB},
	urldate = {2024-02-20},
	journal = {The Guardian},
	author = {Bogle, Ariel},
	month = aug,
	year = {2023},
	note = {https://www.theguardian.com/technology/2023/aug/25/new-york-times-cnn-and-abc-block-openais-gptbot-web-crawler-from-scraping-content (accessed 2024-01-31)},
	keywords = {Artificial intelligence (AI), Australian Broadcasting Corporation, CNN, Media, New York Times, Newspapers, OpenAI, Reuters},
}

@inproceedings{kaneko_cyclegan-vc2_2019,
	address = {Brighton, United Kingdom},
	title = {Cyclegan-{VC2}: {Improved} {Cyclegan}-based {Non}-parallel {Voice} {Conversion}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-4799-8131-1},
	shorttitle = {Cyclegan-{VC2}},
	url = {https://ieeexplore.ieee.org/document/8682897/},
	doi = {10.1109/ICASSP.2019.8682897},
	urldate = {2024-04-19},
	booktitle = {{IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Kaneko, Takuhiro and Kameoka, Hirokazu and Tanaka, Kou and Hojo, Nobukatsu},
	month = may,
	year = {2019},
	keywords = {Acoustics, Artificial neural networks, Convolution, CycleGAN, CycleGAN-VC, Generators, Task analysis, Training, Two dimensional displays, Voice conversion (VC), generative adversarial networks (GANs), non-parallel VC},
	pages = {6820--6824},
}

@misc{brewster_fraudsters_2021,
	title = {Fraudsters {Cloned} {Company} {Director}’s {Voice} {In} \$35 {Million} {Heist}, {Police} {Find}},
	url = {https://www.forbes.com/sites/thomasbrewster/2021/10/14/huge-bank-fraud-uses-deep-fake-voice-tech-to-steal-millions/},
	abstract = {AI voice cloning is used in a huge heist in the U.A.E., amidst warnings about cybercriminal use of the new technology.},
	language = {en},
	urldate = {2024-04-05},
	journal = {Forbes},
	author = {Brewster, Thomas},
	month = oct,
	year = {2021},
	note = {https://www.forbes.com/sites/thomasbrewster/2021/10/14/huge-bank-fraud-uses-deep-fake-voice-tech-to-steal-millions/ (accessed 2024-01-31)},
}

@inproceedings{corvi_detection_2023,
	address = {Rhodes Island, Greece},
	title = {On {The} {Detection} of {Synthetic} {Images} {Generated} by {Diffusion} {Models}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-72816-327-7},
	url = {https://ieeexplore.ieee.org/document/10095167/},
	doi = {10.1109/ICASSP49357.2023.10095167},
	urldate = {2024-04-19},
	booktitle = {{IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Corvi, Riccardo and Cozzolino, Davide and Zingarini, Giada and Poggi, Giovanni and Nagano, Koki and Verdoliva, Luisa},
	month = jun,
	year = {2023},
	keywords = {Detectors, Diffusion models, Forensics, GANs, Signal processing, Social networking (online), Synthetic image detection, Text-to-image, Training, Video games, Visualization},
	pages = {1--5},
}

@inproceedings{shan_glaze_2023,
	address = {Anaheim, CA},
	title = {Glaze: {Protecting} {Artists} from {Style} {Mimicry} by {Text}-to-{Image} {Models}},
	isbn = {978-1-939133-37-3},
	url = {https://www.usenix.org/conference/usenixsecurity23/presentation/shan},
	booktitle = {32nd {USENIX} {Security} {Symposium}},
	publisher = {USENIX Association},
	author = {Shan, Shawn and Cryan, Jenna and Wenger, Emily and Zheng, Haitao and Hanocka, Rana and Zhao, Ben Y.},
	month = aug,
	year = {2023},
	keywords = {Computer Science - Cryptography and Security, cloaking, protection},
	pages = {2187--2204},
}

@inproceedings{carlini_extracting_2021,
	title = {Extracting {Training} {Data} from {Large} {Language} {Models}},
	isbn = {978-1-939133-24-3},
	url = {https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting},
	booktitle = {30th {USENIX} {Security} {Symposium}},
	publisher = {USENIX Association},
	author = {Carlini, Nicholas and Tramèr, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and Oprea, Alina and Raffel, Colin},
	month = aug,
	year = {2021},
	keywords = {Training data copyright, data replication, deduplication, memorisation, protection, training data sanitisation, violation},
	pages = {2633--2650},
}

@article{mccoy_how_2023,
	title = {How {Much} {Do} {Language} {Models} {Copy} {From} {Their} {Training} {Data}? {Evaluating} {Linguistic} {Novelty} in {Text} {Generation} {Using} {RAVEN}},
	volume = {11},
	issn = {2307-387X},
	shorttitle = {How {Much} {Do} {Language} {Models} {Copy} {From} {Their} {Training} {Data}?},
	url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00567/116616/How-Much-Do-Language-Models-Copy-From-Their},
	doi = {10.1162/tacl_a_00567},
	abstract = {Abstract
            Current language models can generate high-quality text. Are they simply copying text they have seen before, or have they learned generalizable linguistic abstractions? To tease apart these possibilities, we introduce RAVEN, a suite of analyses for assessing the novelty of generated text, focusing on sequential structure (n-grams) and syntactic structure. We apply these analyses to four neural language models trained on English (an LSTM, a Transformer, Transformer-XL, and GPT-2). For local structure—e.g., individual dependencies—text generated with a standard sampling scheme is substantially less novel than our baseline of human-generated text from each model’s test set. For larger-scale structure—e.g., overall sentence structure—model-generated text is as novel or even more novel than the human-generated baseline, but models still sometimes copy substantially, in some cases duplicating passages over 1,000 words long from the training set. We also perform extensive manual analysis, finding evidence that GPT-2 uses both compositional and analogical generalization mechanisms and showing that GPT-2’s novel text is usually well-formed morphologically and syntactically but has reasonably frequent semantic issues (e.g., being self-contradictory).},
	language = {en},
	urldate = {2024-04-19},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {McCoy, R. Thomas and Smolensky, Paul and Linzen, Tal and Gao, Jianfeng and Celikyilmaz, Asli},
	month = jun,
	year = {2023},
	pages = {652--670},
}

@inproceedings{ruiz_dreambooth_2023,
	address = {Vancouver, BC, Canada},
	title = {{DreamBooth}: {Fine} {Tuning} {Text}-to-{Image} {Diffusion} {Models} for {Subject}-{Driven} {Generation}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350301298},
	shorttitle = {{DreamBooth}},
	url = {https://ieeexplore.ieee.org/document/10204880/},
	doi = {10.1109/CVPR52729.2023.02155},
	urldate = {2024-04-19},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
	month = jun,
	year = {2023},
	pages = {22500--22510},
}

@misc{baio_invasive_2022,
	title = {Invasive {Diffusion}: {How} one unwilling illustrator found herself turned into an {AI} model},
	shorttitle = {Invasive {Diffusion}},
	url = {https://waxy.org/2022/11/invasive-diffusion-how-one-unwilling-illustrator-found-herself-turned-into-an-ai-model/},
	abstract = {How does it feel to be turned into an AI image model? To find out, I opened a door to the multiverse and interviewed the creator and unwilling subject of a controversial DreamBooth model.},
	language = {en-US},
	urldate = {2023-07-20},
	journal = {Waxy.org},
	author = {Baio, Andy},
	month = nov,
	year = {2022},
	note = {https://waxy.org/2022/11/invasive-diffusion-how-one-unwilling-illustrator-found-herself-turned-into-an-ai-model/ (accessed 2024-01-31)},
	keywords = {blog},
}

@inproceedings{qian_autovc_2019,
	title = {{AutoVC}: {Zero}-{Shot} {Voice} {Style} {Transfer} with {Only} {Autoencoder} {Loss}},
	shorttitle = {{AutoVC}},
	url = {https://proceedings.mlr.press/v97/qian19c.html},
	abstract = {Despite the progress in voice conversion, many-to-many voice conversion trained on non-parallel data, as well as zero-shot voice conversion, remains under-explored. Deep style transfer algorithms, generative adversarial networks (GAN) in particular, are being applied as new solutions in this field. However, GAN training is very sophisticated and difficult, and there is no strong evidence that its generated speech is of good perceptual quality. In this paper, we propose a new style transfer scheme that involves only an autoencoder with a carefully designed bottleneck. We formally show that this scheme can achieve distribution-matching style transfer by training only on self-reconstruction loss. Based on this scheme, we proposed AutoVC, which achieves state-of-the-art results in many-to-many voice conversion with non-parallel data, and which is the first to perform zero-shot voice conversion.},
	language = {en},
	urldate = {2024-03-13},
	booktitle = {36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Qian, Kaizhi and Zhang, Yang and Chang, Shiyu and Yang, Xuesong and Hasegawa-Johnson, Mark},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {5210--5219},
}

@article{brittain_judge_2023,
	chapter = {Litigation},
	title = {Judge pares down artists' {AI} copyright lawsuit against {Midjourney}, {Stability} {AI}},
	url = {https://www.reuters.com/legal/litigation/judge-pares-down-artists-ai-copyright-lawsuit-against-midjourney-stability-ai-2023-10-30/},
	abstract = {A judge in California federal court on Monday trimmed a lawsuit by visual artists who accuse Stability AI, Midjourney and DeviantArt of misusing their copyrighted work in connection with the companies' generative artificial intelligence systems.},
	language = {en},
	urldate = {2024-03-13},
	journal = {Reuters},
	author = {Brittain, Blake},
	month = oct,
	year = {2023},
	note = {https://www.reuters.com/legal/litigation/judge-pares-down-artists-ai-copyright-lawsuit-against-midjourney-stability-ai-2023-10-30/ (accessed 2024-01-31)},
}

@misc{grynbaum_new_2023,
	title = {New {York} {Times} {Sues} {OpenAI} and {Microsoft} {Over} {Use} of {Copyrighted} {Work}},
	url = {https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html},
	language = {en-GB},
	urldate = {2024-03-13},
	journal = {New York Times},
	author = {Grynbaum, Michael M. and Mac, Ryan},
	month = dec,
	year = {2023},
	note = {https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html (accessed 2024-01-31)},
	keywords = {Artificial intelligence (AI), Australian Broadcasting Corporation, CNN, Media, New York Times, Newspapers, OpenAI, Reuters},
}

@inproceedings{kawar_imagic_2023,
	address = {Vancouver, BC, Canada},
	title = {Imagic: {Text}-{Based} {Real} {Image} {Editing} with {Diffusion} {Models}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350301298},
	shorttitle = {Imagic},
	url = {https://ieeexplore.ieee.org/document/10203581/},
	doi = {10.1109/CVPR52729.2023.00582},
	urldate = {2024-04-19},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Kawar, Bahjat and Zada, Shiran and Lang, Oran and Tov, Omer and Chang, Huiwen and Dekel, Tali and Mosseri, Inbar and Irani, Michal},
	month = jun,
	year = {2023},
	pages = {6007--6017},
}

@inproceedings{avrahami_blended_2022,
	address = {New Orleans, LA, USA},
	title = {Blended {Diffusion} for {Text}-driven {Editing} of {Natural} {Images}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9879075/},
	doi = {10.1109/CVPR52688.2022.01767},
	urldate = {2024-04-19},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Avrahami, Omri and Lischinski, Dani and Fried, Ohad},
	month = jun,
	year = {2022},
	pages = {18187--18197},
}

@inproceedings{brooks_instructpix2pix_2023,
	address = {Vancouver, BC, Canada},
	title = {{InstructPix2Pix}: {Learning} to {Follow} {Image} {Editing} {Instructions}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350301298},
	shorttitle = {{InstructPix2Pix}},
	url = {https://ieeexplore.ieee.org/document/10204579/},
	doi = {10.1109/CVPR52729.2023.01764},
	urldate = {2024-04-19},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Brooks, Tim and Holynski, Aleksander and Efros, Alexei A.},
	month = jun,
	year = {2023},
	pages = {18392--18402},
}

@misc{losio_first_2022,
	title = {First {Open} {Source} {Copyright} {Lawsuit} {Chal}­lenges {GitHub} {Copi}­lot},
	url = {https://www.infoq.com/news/2022/11/lawsuit-github-copilot/},
	abstract = {A class-action law­suit has been filed in a US fed­eral court chal­leng­ing the legal­ity of GitHub Copi­lot and the related OpenAI Codex. The suit against GitHub, Microsoft, and OpenAI claims violation of open-source licenses and could have a wide impact in the world of artificial intelligence.},
	language = {en},
	urldate = {2024-03-13},
	journal = {InfoQ},
	author = {Losio, Renato},
	month = nov,
	year = {2022},
	note = {https://www.infoq.com/news/2022/11/lawsuit-github-copilot/ (accessed 2021-01-31)},
}

@article{radford_improving_nodate,
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
	url = {https://openai.com/research/language-unsupervised},
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
	language = {en},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	note = {https://openai.com/research/language-unsupervised (accessed 2021-01-31)},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2023-07-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
	year = {2017},
	keywords = {transformer},
}

@inproceedings{radford_learning_2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {https://proceedings.mlr.press/v139/radford21a.html},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
	language = {en},
	urldate = {2023-07-26},
	booktitle = {38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {8748--8763},
}

@inproceedings{dhariwal_diffusion_2021,
	title = {Diffusion {Models} {Beat} {GANs} on {Image} {Synthesis}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Dhariwal, Prafulla and Nichol, Alexander},
	year = {2021},
	keywords = {GANs, diffusion},
	pages = {8780--8794},
}

@misc{vyas_provable_2023,
	title = {Provable {Copyright} {Protection} for {Generative} {Models}},
	url = {http://arxiv.org/abs/2302.10870},
	doi = {10.48550/arXiv.2302.10870},
	abstract = {There is a growing concern that learned conditional generative models may output samples that are substantially similar to some copyrighted data \$C\$ that was in their training set. We give a formal definition of \${\textbackslash}textit\{near access-freeness (NAF)\}\$ and prove bounds on the probability that a model satisfying this definition outputs a sample similar to \$C\$, even if \$C\$ is included in its training set. Roughly speaking, a generative model \$p\$ is \${\textbackslash}textit\{\$k\$-NAF\}\$ if for every potentially copyrighted data \$C\$, the output of \$p\$ diverges by at most \$k\$-bits from the output of a model \$q\$ that \${\textbackslash}textit\{did not access \$C\$ at all\}\$. We also give generative model learning algorithms, which efficiently modify the original generative model learning algorithm in a black box manner, that output generative models with strong bounds on the probability of sampling protected content. Furthermore, we provide promising experiments for both language (transformers) and image (diffusion) generative models, showing minimal degradation in output quality while ensuring strong protections against sampling protected content.},
	urldate = {2023-07-22},
	publisher = {arXiv},
	author = {Vyas, Nikhil and Kakade, Sham and Barak, Boaz},
	month = feb,
	year = {2023},
	note = {arXiv:2302.10870},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Training data copyright, copyright protection by design, ethical GenAI, protection},
}

@misc{wadhera_comprehensive_2022,
	title = {A {Comprehensive} {Review} on {Digital} {Image} {Watermarking}},
	url = {http://arxiv.org/abs/2207.06909},
	doi = {10.48550/arXiv.2207.06909},
	abstract = {The advent of the Internet led to the easy availability of digital data like images, audio, and video. Easy access to multimedia gives rise to the issues such as content authentication, security, copyright protection, and ownership identification. Here, we discuss the concept of digital image watermarking with a focus on the technique used in image watermark embedding and extraction of the watermark. The detailed classification along with the basic characteristics, namely visual imperceptibility, robustness, capacity, security of digital watermarking is also presented in this work. Further, we have also discussed the recent application areas of digital watermarking such as healthcare, remote education, electronic voting systems, and the military. The robustness is evaluated by examining the effect of image processing attacks on the signed content and the watermark recoverability. The authors believe that the comprehensive survey presented in this paper will help the new researchers to gather knowledge in this domain. Further, the comparative analysis can enkindle ideas to improve upon the already mentioned techniques.},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Wadhera, Shweta and Kamra, Deepa and Rajpal, Ankit and Jain, Aruna and Jain, Vishal},
	month = jul,
	year = {2022},
	note = {arXiv:2207.06909},
	keywords = {Computer Science - Multimedia, Electrical Engineering and Systems Science - Signal Processing, survey, watermarking},
}

@misc{ren_fastspeech_2022,
	title = {{FastSpeech} 2: {Fast} and {High}-{Quality} {End}-to-{End} {Text} to {Speech}},
	shorttitle = {{FastSpeech} 2},
	url = {http://arxiv.org/abs/2006.04558},
	doi = {10.48550/arXiv.2006.04558},
	abstract = {Non-autoregressive text to speech (TTS) models such as FastSpeech can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x training speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even surpass autoregressive models. Audio samples are available at https://speechresearch.github.io/fastspeech2/.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Ren, Yi and Hu, Chenxu and Tan, Xu and Qin, Tao and Zhao, Sheng and Zhao, Zhou and Liu, Tie-Yan},
	month = aug,
	year = {2022},
	note = {arXiv:2006.04558},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{oord_representation_2019,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/1807.03748},
	doi = {10.48550/arXiv.1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	urldate = {2024-02-19},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	month = jan,
	year = {2019},
	note = {arXiv:1807.03748},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{nichol_improved_2021,
	title = {Improved {Denoising} {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2102.09672},
	doi = {10.48550/arXiv.2102.09672},
	abstract = {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion},
	urldate = {2024-02-19},
	publisher = {arXiv},
	author = {Nichol, Alex and Dhariwal, Prafulla},
	month = feb,
	year = {2021},
	note = {arXiv:2102.09672},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{neel_descent--delete_2020,
	title = {Descent-to-{Delete}: {Gradient}-{Based} {Methods} for {Machine} {Unlearning}},
	shorttitle = {Descent-to-{Delete}},
	url = {http://arxiv.org/abs/2007.02923},
	doi = {10.48550/arXiv.2007.02923},
	abstract = {We study the data deletion problem for convex models. By leveraging techniques from convex optimization and reservoir sampling, we give the first data deletion algorithms that are able to handle an arbitrarily long sequence of adversarial updates while promising both per-deletion run-time and steady-state error that do not grow with the length of the update sequence. We also introduce several new conceptual distinctions: for example, we can ask that after a deletion, the entire state maintained by the optimization algorithm is statistically indistinguishable from the state that would have resulted had we retrained, or we can ask for the weaker condition that only the observable output is statistically indistinguishable from the observable output that would have resulted from retraining. We are able to give more efficient deletion algorithms under this weaker deletion criterion.},
	urldate = {2023-07-23},
	publisher = {arXiv},
	author = {Neel, Seth and Roth, Aaron and Sharifi-Malvajerdi, Saeed},
	month = jul,
	year = {2020},
	note = {arXiv:2007.02923},
	keywords = {Computer Science - Machine Learning, Machine unlearning, Statistics - Machine Learning},
}

@misc{kwon_diffusion-based_2023,
	title = {Diffusion-based {Image} {Translation} using {Disentangled} {Style} and {Content} {Representation}},
	url = {http://arxiv.org/abs/2209.15264},
	abstract = {Diffusion-based image translation guided by semantic texts or a single target image has enabled flexible style transfer which is not limited to the specific domains. Unfortunately, due to the stochastic nature of diffusion models, it is often difficult to maintain the original content of the image during the reverse diffusion. To address this, here we present a novel diffusion-based unsupervised image translation method using disentangled style and content representation. Specifically, inspired by the splicing Vision Transformer, we extract intermediate keys of multihead self attention layer from ViT model and used them as the content preservation loss. Then, an image guided style transfer is performed by matching the [CLS] classification token from the denoised samples and target image, whereas additional CLIP loss is used for the text-driven style transfer. To further accelerate the semantic change during the reverse diffusion, we also propose a novel semantic divergence loss and resampling strategy. Our experimental results show that the proposed method outperforms state-of-the-art baseline models in both text-guided and image-guided translation tasks.},
	urldate = {2024-02-16},
	publisher = {arXiv},
	author = {Kwon, Gihyun and Ye, Jong Chul},
	month = feb,
	year = {2023},
	note = {arXiv:2209.15264},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{kingma_auto-encoding_2022,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2024-02-19},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2022},
	note = {arXiv:1312.6114},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{karras_style-based_2019,
	title = {A {Style}-{Based} {Generator} {Architecture} for {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1812.04948},
	doi = {10.48550/arXiv.1812.04948},
	abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
	urldate = {2024-02-19},
	publisher = {arXiv},
	author = {Karras, Tero and Laine, Samuli and Aila, Timo},
	month = mar,
	year = {2019},
	note = {arXiv:1812.04948},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{karras_progressive_2018,
	title = {Progressive {Growing} of {GANs} for {Improved} {Quality}, {Stability}, and {Variation}},
	url = {http://arxiv.org/abs/1710.10196},
	doi = {10.48550/arXiv.1710.10196},
	abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024{\textasciicircum}2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
	urldate = {2024-04-08},
	publisher = {arXiv},
	author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	month = feb,
	year = {2018},
	note = {arXiv:1710.10196},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{higgins_towards_2018,
	title = {Towards a {Definition} of {Disentangled} {Representations}},
	url = {http://arxiv.org/abs/1812.02230},
	abstract = {How can intelligent agents solve a diverse set of tasks in a data-efficient manner? The disentangled representation learning approach posits that such an agent would benefit from separating out (disentangling) the underlying structure of the world into disjoint parts of its representation. However, there is no generally agreed-upon definition of disentangling, not least because it is unclear how to formalise the notion of world structure beyond toy datasets with a known ground truth generative process. Here we propose that a principled solution to characterising disentangled representations can be found by focusing on the transformation properties of the world. In particular, we suggest that those transformations that change only some properties of the underlying world state, while leaving all other properties invariant, are what gives exploitable structure to any kind of data. Similar ideas have already been successfully applied in physics, where the study of symmetry transformations has revolutionised the understanding of the world structure. By connecting symmetry transformations to vector representations using the formalism of group and representation theory we arrive at the first formal definition of disentangled representations. Our new definition is in agreement with many of the current intuitions about disentangling, while also providing principled resolutions to a number of previous points of contention. While this work focuses on formally defining disentangling - as opposed to solving the learning problem - we believe that the shift in perspective to studying data transformations can stimulate the development of better representation learning algorithms.},
	urldate = {2024-02-16},
	publisher = {arXiv},
	author = {Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
	month = dec,
	year = {2018},
	note = {arXiv:1812.02230},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{goodfellow_explaining_2015,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1412.6572},
	doi = {10.48550/arXiv.1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	urldate = {2024-02-07},
	publisher = {arXiv},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	month = mar,
	year = {2015},
	note = {arXiv:1412.6572},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{gatys_neural_2015,
	title = {A {Neural} {Algorithm} of {Artistic} {Style}},
	url = {http://arxiv.org/abs/1508.06576},
	doi = {10.48550/arXiv.1508.06576},
	abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
	month = sep,
	year = {2015},
	note = {arXiv:1508.06576},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition, paper, style mimicry, violation},
}

@misc{dai_training_2023,
	title = {Training {Data} {Attribution} for {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2306.02174},
	doi = {10.48550/arXiv.2306.02174},
	abstract = {Diffusion models have become increasingly popular for synthesizing high-quality samples based on training datasets. However, given the oftentimes enormous sizes of the training datasets, it is difficult to assess how training data impact the samples produced by a trained diffusion model. The difficulty of relating diffusion model inputs and outputs poses significant challenges to model explainability and training data attribution. Here we propose a novel solution that reveals how training data influence the output of diffusion models through the use of ensembles. In our approach individual models in an encoded ensemble are trained on carefully engineered splits of the overall training data to permit the identification of influential training examples. The resulting model ensembles enable efficient ablation of training data influence, allowing us to assess the impact of training data on model outputs. We demonstrate the viability of these ensembles as generative models and the validity of our approach to assessing influence.},
	urldate = {2024-02-20},
	publisher = {arXiv},
	author = {Dai, Zheng and Gifford, David K.},
	month = jun,
	year = {2023},
	note = {arXiv:2306.02174},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{brock_large_2019,
	title = {Large {Scale} {GAN} {Training} for {High} {Fidelity} {Natural} {Image} {Synthesis}},
	url = {http://arxiv.org/abs/1809.11096},
	doi = {10.48550/arXiv.1809.11096},
	abstract = {Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple "truncation trick," allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.},
	urldate = {2024-02-19},
	publisher = {arXiv},
	author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
	month = feb,
	year = {2019},
	note = {arXiv:1809.11096},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{baumhauer_machine_2020,
	title = {Machine {Unlearning}: {Linear} {Filtration} for {Logit}-based {Classifiers}},
	shorttitle = {Machine {Unlearning}},
	url = {http://arxiv.org/abs/2002.02730},
	doi = {10.48550/arXiv.2002.02730},
	abstract = {Recently enacted legislation grants individuals certain rights to decide in what fashion their personal data may be used, and in particular a "right to be forgotten". This poses a challenge to machine learning: how to proceed when an individual retracts permission to use data which has been part of the training process of a model? From this question emerges the field of machine unlearning, which could be broadly described as the investigation of how to "delete training data from models". Our work complements this direction of research for the specific setting of class-wide deletion requests for classification models (e.g. deep neural networks). As a first step, we propose linear filtration as a intuitive, computationally efficient sanitization method. Our experiments demonstrate benefits in an adversarial setting over naive deletion schemes.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Baumhauer, Thomas and Schöttle, Pascal and Zeppelzauer, Matthias},
	month = jul,
	year = {2020},
	note = {arXiv:2002.02730},
	keywords = {Computer Science - Machine Learning, DNN, Machine unlearning, Statistics - Machine Learning, image classification},
}

@inproceedings{wohlin_guidelines_2014,
	address = {London, England, United Kingdom},
	title = {Guidelines for snowballing in systematic literature studies and a replication in software engineering},
	isbn = {978-1-4503-2476-2},
	url = {https://dl.acm.org/doi/10.1145/2601248.2601268},
	doi = {10.1145/2601248.2601268},
	language = {en},
	urldate = {2024-04-19},
	booktitle = {18th {International} {Conference} on {Evaluation} and {Assessment} in {Software} {Engineering}},
	publisher = {ACM},
	author = {Wohlin, Claes},
	month = may,
	year = {2014},
	keywords = {replication, snowball search, snowballing, systematic literature review, systematic mapping studies},
	pages = {1--10},
}

@article{sun_adversarial_2023,
	title = {Adversarial {Attacks} {Against} {Deep} {Generative} {Models} on {Data}: {A} {Survey}},
	volume = {35},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {1041-4347, 1558-2191, 2326-3865},
	shorttitle = {Adversarial {Attacks} {Against} {Deep} {Generative} {Models} on {Data}},
	url = {https://ieeexplore.ieee.org/document/9627776/},
	doi = {10.1109/TKDE.2021.3130903},
	number = {4},
	urldate = {2024-04-19},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Sun, Hui and Zhu, Tianqing and Zhang, Zhiqiu and Jin, Dawei and Xiong, Ping and Zhou, Wanlei},
	month = apr,
	year = {2023},
	keywords = {Biological system modeling, Codes, Data models, Deep generative models, Generators, Privacy, Security, Training, deep learning, evasion attack, membership inference attack, model defense, survey},
	pages = {3367--3388},
}

@article{hristov_artificial_2019,
	title = {Artificial {Intelligence} and the {Copyright} {Survey}},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=3490458},
	doi = {10.2139/ssrn.3490458},
	language = {en},
	urldate = {2024-04-19},
	journal = {SSRN Electronic Journal},
	author = {Hristov, Kalin},
	year = {2019},
	keywords = {AI, Artificial Intelligence, Author, Authorship, Copyright, Copyright Act, Employee, Employer, IP, Intellectual Property, Machine Learning, Owner, Programmer, Questionnaire, Survey, United States of America, Work Made for Hire, legal},
}

@article{chesterman_good_2023,
	title = {Good {Models} {Borrow}, {Great} {Models} {Steal}: {Intellectual} {Property} {Rights} and {Generative} {AI}},
	issn = {1556-5068},
	shorttitle = {Good {Models} {Borrow}, {Great} {Models} {Steal}},
	url = {https://www.ssrn.com/abstract=4590006},
	doi = {10.2139/ssrn.4590006},
	language = {en},
	urldate = {2024-04-19},
	journal = {SSRN Electronic Journal},
	author = {Chesterman, Simon},
	year = {2023},
	keywords = {Artificial Intelligence, ChatGPT, Copyright, GPT-4, Generative AI, Intellectual Property, Large Language Models},
}

@incollection{smits_generative_2022,
	address = {The Hague},
	title = {Generative {AI} and {Intellectual} {Property} {Rights}},
	volume = {35},
	isbn = {978-94-6265-522-5 978-94-6265-523-2},
	url = {https://link.springer.com/10.1007/978-94-6265-523-2_17},
	language = {en},
	urldate = {2024-04-19},
	booktitle = {Law and {Artificial} {Intelligence}},
	publisher = {T.M.C. Asser Press},
	author = {Smits, Jan and Borghuis, Tijn},
	year = {2022},
	keywords = {Authorship, Era of Abundance, Generative AI, Human-Al Cooperation, Public Domain, Unauthorized Imitation, Work, survey},
	pages = {323--344},
}

@inproceedings{zhong_copyright_2023,
	address = {Austin TX USA},
	title = {Copyright {Protection} and {Accountability} of {Generative} {AI}: {Attack}, {Watermarking} and {Attribution}},
	isbn = {978-1-4503-9419-2},
	shorttitle = {Copyright {Protection} and {Accountability} of {Generative} {AI}},
	url = {https://dl.acm.org/doi/10.1145/3543873.3587321},
	doi = {10.1145/3543873.3587321},
	language = {en},
	urldate = {2024-04-19},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2023},
	publisher = {ACM},
	author = {Zhong, Haonan and Chang, Jiamin and Yang, Ziyue and Wu, Tingmin and Mahawaga Arachchige, Pathum Chamikara and Pathmabandu, Chehara and Xue, Minhui},
	month = apr,
	year = {2023},
	pages = {94--98},
}

@article{wang_survey_2023,
	title = {A {Survey} on {ChatGPT}: {AI}–{Generated} {Contents}, {Challenges}, and {Solutions}},
	volume = {4},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2644-1268},
	shorttitle = {A {Survey} on {ChatGPT}},
	url = {https://ieeexplore.ieee.org/document/10221755/},
	doi = {10.1109/OJCS.2023.3300321},
	urldate = {2024-04-19},
	journal = {IEEE Open Journal of the Computer Society},
	author = {Wang, Yuntao and Pan, Yanghe and Yan, Miao and Su, Zhou and Luan, Tom H.},
	year = {2023},
	pages = {280--302},
}

@misc{nolan_ai_2023,
	title = {{AI} art generators face separate copyright lawsuits from {Getty} {Images} and a group of artists},
	url = {https://www.businessinsider.com/ai-art-artists-getty-images-lawsuits-stable-diffusion-2023-1},
	abstract = {Three artists have launched a class action against the companies behind Stable Diffusion, Midjourney, and DreamUp.},
	language = {en-US},
	urldate = {2023-07-23},
	journal = {Business Insider},
	author = {Nolan, Beatrice},
	month = jan,
	year = {2023},
	note = {https://www.businessinsider.com/ai-art-artists-getty-images-lawsuits-stable-diffusion-2023-1 (accessed 2024-01-31)},
	keywords = {blog, violation},
}

@misc{marcus_generative_2024,
	title = {Generative {AI} {Has} a {Visual} {Plagiarism} {Problem}},
	url = {https://spectrum.ieee.org/midjourney-copyright},
	abstract = {Experiments with Midjourney and DALL-E 3 show a copyright minefield},
	language = {en},
	urldate = {2024-03-13},
	journal = {IEEE Spectrum},
	author = {Marcus, Gary and Southen, Reid},
	month = jun,
	year = {2024},
	note = {https://spectrum.ieee.org/midjourney-copyright (accessed 2024-01-31)},
}

@misc{heikkila_this_2022,
	title = {This artist is dominating {AI}-generated art. {And} he’s not happy about it.},
	url = {https://www.technologyreview.com/2022/09/16/1059598/this-artist-is-dominating-ai-generated-art-and-hes-not-happy-about-it/},
	abstract = {Greg Rutkowski is a more popular prompt than Picasso.},
	language = {en},
	urldate = {2023-07-21},
	journal = {MIT Technology Review},
	author = {Heikkilä, Melissa},
	month = sep,
	year = {2022},
	note = {https://www.technologyreview.com/2022/09/16/1059598/  (accessed 2024-01-31)},
	keywords = {blog},
}

@inproceedings{rombach_high-resolution_2022,
	address = {New Orleans, LA, USA},
	title = {High-{Resolution} {Image} {Synthesis} with {Latent} {Diffusion} {Models}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9878449/},
	doi = {10.1109/CVPR52688.2022.01042},
	urldate = {2024-04-19},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bjorn},
	month = jun,
	year = {2022},
	pages = {10674--10685},
}

@misc{wu_towards_2023,
	title = {Towards {Prompt}-robust {Face} {Privacy} {Protection} via {Adversarial} {Decoupling} {Augmentation} {Framework}},
	url = {http://arxiv.org/abs/2305.03980},
	doi = {10.48550/arXiv.2305.03980},
	abstract = {Denoising diffusion models have shown remarkable potential in various generation tasks. The open-source large-scale text-to-image model, Stable Diffusion, becomes prevalent as it can generate realistic artistic or facial images with personalization through fine-tuning on a limited number of new samples. However, this has raised privacy concerns as adversaries can acquire facial images online and fine-tune text-to-image models for malicious editing, leading to baseless scandals, defamation, and disruption to victims' lives. Prior research efforts have focused on deriving adversarial loss from conventional training processes for facial privacy protection through adversarial perturbations. However, existing algorithms face two issues: 1) they neglect the image-text fusion module, which is the vital module of text-to-image diffusion models, and 2) their defensive performance is unstable against different attacker prompts. In this paper, we propose the Adversarial Decoupling Augmentation Framework (ADAF), addressing these issues by targeting the image-text fusion module to enhance the defensive performance of facial privacy protection algorithms. ADAF introduces multi-level text-related augmentations for defense stability against various attacker prompts. Concretely, considering the vision, text, and common unit space, we propose Vision-Adversarial Loss, Prompt-Robust Augmentation, and Attention-Decoupling Loss. Extensive experiments on CelebA-HQ and VGGFace2 demonstrate ADAF's promising performance, surpassing existing algorithms.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Wu, Ruijia and Wang, Yuhang and Shi, Huafeng and Yu, Zhipeng and Wu, Yichao and Liang, Ding},
	month = may,
	year = {2023},
	note = {arXiv:2305.03980},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Privacy, potential application for IPinGenAI},
}

@misc{wu_depn_2023,
	title = {{DEPN}: {Detecting} and {Editing} {Privacy} {Neurons} in {Pretrained} {Language} {Models}},
	shorttitle = {{DEPN}},
	url = {http://arxiv.org/abs/2310.20138},
	doi = {10.48550/arXiv.2310.20138},
	abstract = {Large language models pretrained on a huge amount of data capture rich knowledge and information in the training data. The ability of data memorization and regurgitation in pretrained language models, revealed in previous studies, brings the risk of data leakage. In order to effectively reduce these risks, we propose a framework DEPN to Detect and Edit Privacy Neurons in pretrained language models, partially inspired by knowledge neurons and model editing. In DEPN, we introduce a novel method, termed as privacy neuron detector, to locate neurons associated with private information, and then edit these detected privacy neurons by setting their activations to zero. Furthermore, we propose a privacy neuron aggregator dememorize private information in a batch processing manner. Experimental results show that our method can significantly and efficiently reduce the exposure of private data leakage without deteriorating the performance of the model. Additionally, we empirically demonstrate the relationship between model memorization and privacy neurons, from multiple perspectives, including model size, training time, prompts, privacy neuron distribution, illustrating the robustness of our approach.},
	urldate = {2024-02-21},
	publisher = {arXiv},
	author = {Wu, Xinwei and Li, Junzhuo and Xu, Minghui and Dong, Weilong and Wu, Shuangzhi and Bian, Chao and Xiong, Deyi},
	month = dec,
	year = {2023},
	note = {arXiv:2310.20138},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security},
}

@misc{zeng_securing_2023,
	title = {Securing {Deep} {Generative} {Models} with {Universal} {Adversarial} {Signature}},
	url = {http://arxiv.org/abs/2305.16310},
	doi = {10.48550/arXiv.2305.16310},
	abstract = {Recent advances in deep generative models have led to the development of methods capable of synthesizing high-quality, realistic images. These models pose threats to society due to their potential misuse. Prior research attempted to mitigate these threats by detecting generated images, but the varying traces left by different generative models make it challenging to create a universal detector capable of generalizing to new, unseen generative models. In this paper, we propose to inject a universal adversarial signature into an arbitrary pre-trained generative model, in order to make its generated contents more detectable and traceable. First, the imperceptible optimal signature for each image can be found by a signature injector through adversarial training. Subsequently, the signature can be incorporated into an arbitrary generator by fine-tuning it with the images processed by the signature injector. In this way, the detector corresponding to the signature can be reused for any fine-tuned generator for tracking the generator identity. The proposed method is validated on the FFHQ and ImageNet datasets with various state-of-the-art generative models, consistently showing a promising detection rate. Code will be made publicly available at {\textbackslash}url\{https://github.com/zengxianyu/genwm\}.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Zeng, Yu and Zhou, Mo and Xue, Yuan and Patel, Vishal M.},
	month = may,
	year = {2023},
	note = {arXiv:2305.16310},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, detection of generated content, paper, protection},
}

@misc{zhang_editguard_2023,
	title = {{EditGuard}: {Versatile} {Image} {Watermarking} for {Tamper} {Localization} and {Copyright} {Protection}},
	shorttitle = {{EditGuard}},
	url = {http://arxiv.org/abs/2312.08883},
	abstract = {In the era where AI-generated content (AIGC) models can produce stunning and lifelike images, the lingering shadow of unauthorized reproductions and malicious tampering poses imminent threats to copyright integrity and information security. Current image watermarking methods, while widely accepted for safeguarding visual content, can only protect copyright and ensure traceability. They fall short in localizing increasingly realistic image tampering, potentially leading to trust crises, privacy violations, and legal disputes. To solve this challenge, we propose an innovative proactive forensics framework EditGuard, to unify copyright protection and tamper-agnostic localization, especially for AIGC-based editing methods. It can offer a meticulous embedding of imperceptible watermarks and precise decoding of tampered areas and copyright information. Leveraging our observed fragility and locality of image-into-image steganography, the realization of EditGuard can be converted into a united image-bit steganography issue, thus completely decoupling the training process from the tampering types. Extensive experiments demonstrate that our EditGuard balances the tamper localization accuracy, copyright recovery precision, and generalizability to various AIGC-based tampering methods, especially for image forgery that is difficult for the naked eye to detect. The project page is available at https://xuanyuzhang21.github.io/project/editguard/.},
	urldate = {2024-02-16},
	publisher = {arXiv},
	author = {Zhang, Xuanyu and Li, Runyi and Yu, Jiwen and Xu, Youmin and Li, Weiqi and Zhang, Jian},
	month = dec,
	year = {2023},
	note = {arXiv:2312.08883},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, training data ownership, watermarking},
}

@misc{zhao_invisible_2023,
	title = {Invisible {Image} {Watermarks} {Are} {Provably} {Removable} {Using} {Generative} {AI}},
	url = {http://arxiv.org/abs/2306.01953},
	abstract = {Invisible watermarks safeguard images' copyright by embedding hidden messages only detectable by owners. They also prevent people from misusing images, especially those generated by AI models. We propose a family of regeneration attacks to remove these invisible watermarks. The proposed attack method first adds random noise to an image to destroy the watermark and then reconstructs the image. This approach is flexible and can be instantiated with many existing image-denoising algorithms and pre-trained generative models such as diffusion models. Through formal proofs and empirical results, we show that all invisible watermarks are vulnerable to the proposed attack. For a particularly resilient watermark, RivaGAN, regeneration attacks remove 93-99\% of the invisible watermarks while the baseline attacks remove no more than 3\%. However, if we do not require the watermarked image to look the same as the original one, watermarks that keep the image semantically similar can be an alternative defense against our attack. Our finding underscores the need for a shift in research/industry emphasis from invisible watermarks to semantically similar ones. Code is available at https://github.com/XuandongZhao/WatermarkAttacker.},
	urldate = {2024-01-09},
	publisher = {arXiv},
	author = {Zhao, Xuandong and Zhang, Kexun and Su, Zihao and Vasan, Saastha and Grishchenko, Ilya and Kruegel, Christopher and Vigna, Giovanni and Wang, Yu-Xiang and Li, Lei},
	month = aug,
	year = {2023},
	note = {arXiv:2306.01953},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security},
}

@misc{zheng_understanding_2023,
	title = {Understanding and {Improving} {Adversarial} {Attacks} on {Latent} {Diffusion} {Model}},
	url = {http://arxiv.org/abs/2310.04687},
	doi = {10.48550/arXiv.2310.04687},
	abstract = {Latent Diffusion Model (LDM) has emerged as a leading tool in image generation, particularly with its capability in few-shot generation. This capability also presents risks, notably in unauthorized artwork replication and misinformation generation. In response, adversarial attacks have been designed to safeguard personal images from being used as reference data. However, existing adversarial attacks are predominantly empirical, lacking a solid theoretical foundation. In this paper, we introduce a comprehensive theoretical framework for understanding adversarial attacks on LDM. Based on the framework, we propose a novel adversarial attack that exploits a unified target to guide the adversarial attack both in the forward and the reverse process of LDM. We provide empirical evidences that our method overcomes the offset problem of the optimization of adversarial attacks in existing methods. Through rigorous experiments, our findings demonstrate that our method outperforms current attacks and is able to generalize over different state-of-the-art few-shot generation pipelines based on LDM. Our method can serve as a stronger and efficient tool for people exposed to the risk of data privacy and security to protect themselves in the new era of powerful generative models. The code is available on GitHub: https://github.com/CaradryanLiang/ImprovedAdvDM.git.},
	urldate = {2023-12-05},
	publisher = {arXiv},
	author = {Zheng, Boyang and Liang, Chumeng and Wu, Xiaoyu and Liu, Yan},
	month = oct,
	year = {2023},
	note = {arXiv:2310.04687},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Training data copyright, cloaking, diffusion, protection},
}

@misc{ye_duaw_2023,
	title = {{DUAW}: {Data}-free {Universal} {Adversarial} {Watermark} against {Stable} {Diffusion} {Customization}},
	shorttitle = {{DUAW}},
	url = {http://arxiv.org/abs/2308.09889},
	doi = {10.48550/arXiv.2308.09889},
	abstract = {Stable Diffusion (SD) customization approaches enable users to personalize SD model outputs, greatly enhancing the flexibility and diversity of AI art. However, they also allow individuals to plagiarize specific styles or subjects from copyrighted images, which raises significant concerns about potential copyright infringement. To address this issue, we propose an invisible data-free universal adversarial watermark (DUAW), aiming to protect a myriad of copyrighted images from different customization approaches across various versions of SD models. First, DUAW is designed to disrupt the variational autoencoder during SD customization. Second, DUAW operates in a data-free context, where it is trained on synthetic images produced by a Large Language Model (LLM) and a pretrained SD model. This approach circumvents the necessity of directly handling copyrighted images, thereby preserving their confidentiality. Once crafted, DUAW can be imperceptibly integrated into massive copyrighted images, serving as a protective measure by inducing significant distortions in the images generated by customized SD models. Experimental results demonstrate that DUAW can effectively distort the outputs of fine-tuned SD models, rendering them discernible to both human observers and a simple classifier.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Ye, Xiaoyu and Huang, Hao and An, Jiaqi and Wang, Yongtao},
	month = aug,
	year = {2023},
	note = {arXiv:2308.09889},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Training data copyright, Watermark, cloaking, diffusion, paper, protection},
}

@misc{zhang_hive_2023,
	title = {{HIVE}: {Harnessing} {Human} {Feedback} for {Instructional} {Visual} {Editing}},
	shorttitle = {{HIVE}},
	url = {http://arxiv.org/abs/2303.09618},
	doi = {10.48550/arXiv.2303.09618},
	abstract = {Incorporating human feedback has been shown to be crucial to align text generated by large language models to human preferences. We hypothesize that state-of-the-art instructional image editing models, where outputs are generated based on an input image and an editing instruction, could similarly benefit from human feedback, as their outputs may not adhere to the correct instructions and preferences of users. In this paper, we present a novel framework to harness human feedback for instructional visual editing (HIVE). Specifically, we collect human feedback on the edited images and learn a reward function to capture the underlying user preferences. We then introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward. Besides, to mitigate the bias brought by the limitation of data, we contribute a new 1M training dataset, a 3.6K reward dataset for rewards learning, and a 1K evaluation dataset to boost the performance of instructional image editing. We conduct extensive empirical experiments quantitatively and qualitatively, showing that HIVE is favored over previous state-of-the-art instructional image editing approaches by a large margin.},
	urldate = {2024-03-13},
	publisher = {arXiv},
	author = {Zhang, Shu and Yang, Xinyi and Feng, Yihao and Qin, Can and Chen, Chia-Chih and Yu, Ning and Chen, Zeyuan and Wang, Huan and Savarese, Silvio and Ermon, Stefano and Xiong, Caiming and Xu, Ran},
	month = mar,
	year = {2023},
	note = {arXiv:2303.09618},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
}

@misc{xie_mugc_2024,
	title = {{MUGC}: {Machine} {Generated} versus {User} {Generated} {Content} {Detection}},
	shorttitle = {{MUGC}},
	url = {http://arxiv.org/abs/2403.19725},
	abstract = {As advanced modern systems like deep neural networks (DNNs) and generative AI continue to enhance their capabilities in producing convincing and realistic content, the need to distinguish between user-generated and machine generated content is becoming increasingly evident. In this research, we undertake a comparative evaluation of eight traditional machine-learning algorithms to distinguish between machine-generated and human-generated data across three diverse datasets: Poems, Abstracts, and Essays. Our results indicate that traditional methods demonstrate a high level of accuracy in identifying machine-generated data, reflecting the documented effectiveness of popular pre-trained models like RoBERT. We note that machine-generated texts tend to be shorter and exhibit less word variety compared to human-generated content. While specific domain-related keywords commonly utilized by humans, albeit disregarded by current LLMs (Large Language Models), may contribute to this high detection accuracy, we show that deeper word representations like word2vec can capture subtle semantic variances. Furthermore, readability, bias, moral, and affect comparisons reveal a discernible contrast between machine-generated and human generated content. There are variations in expression styles and potentially underlying biases in the data sources (human and machine-generated). This study provides valuable insights into the advancing capacities and challenges associated with machine-generated content across various domains.},
	urldate = {2024-04-03},
	publisher = {arXiv},
	author = {Xie, Yaqi and Rawal, Anjali and Cen, Yujing and Zhao, Dixuan and Narang, Sunil K. and Sushmita, Shanu},
	month = mar,
	year = {2024},
	note = {arXiv:2403.19725},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{xu_shadowcast_2024,
	title = {Shadowcast: {Stealthy} {Data} {Poisoning} {Attacks} {Against} {Vision}-{Language} {Models}},
	shorttitle = {Shadowcast},
	url = {http://arxiv.org/abs/2402.06659},
	doi = {10.48550/arXiv.2402.06659},
	abstract = {Vision-Language Models (VLMs) excel in generating textual responses from visual inputs, yet their versatility raises significant security concerns. This study takes the first step in exposing VLMs' susceptibility to data poisoning attacks that can manipulate responses to innocuous, everyday prompts. We introduce Shadowcast, a stealthy data poisoning attack method where poison samples are visually indistinguishable from benign images with matching texts. Shadowcast demonstrates effectiveness in two attack types. The first is Label Attack, tricking VLMs into misidentifying class labels, such as confusing Donald Trump for Joe Biden. The second is Persuasion Attack, which leverages VLMs' text generation capabilities to craft narratives, such as portraying junk food as health food, through persuasive and seemingly rational descriptions. We show that Shadowcast are highly effective in achieving attacker's intentions using as few as 50 poison samples. Moreover, these poison samples remain effective across various prompts and are transferable across different VLM architectures in the black-box setting. This work reveals how poisoned VLMs can generate convincing yet deceptive misinformation and underscores the importance of data quality for responsible deployments of VLMs. Our code is available at: https://github.com/umd-huang-lab/VLM-Poisoning.},
	urldate = {2024-03-12},
	publisher = {arXiv},
	author = {Xu, Yuancheng and Yao, Jiarui and Shu, Manli and Sun, Yanchao and Wu, Zichu and Yu, Ning and Goldstein, Tom and Huang, Furong},
	month = feb,
	year = {2024},
	note = {arXiv:2402.06659},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{yang_disdiff_2023,
	title = {{DisDiff}: {Unsupervised} {Disentanglement} of {Diffusion} {Probabilistic} {Models}},
	shorttitle = {{DisDiff}},
	url = {http://arxiv.org/abs/2301.13721},
	abstract = {Targeting to understand the underlying explainable factors behind observations and modeling the conditional generation process on these factors, we connect disentangled representation learning to Diffusion Probabilistic Models (DPMs) to take advantage of the remarkable modeling ability of DPMs. We propose a new task, disentanglement of (DPMs): given a pre-trained DPM, without any annotations of the factors, the task is to automatically discover the inherent factors behind the observations and disentangle the gradient fields of DPM into sub-gradient fields, each conditioned on the representation of each discovered factor. With disentangled DPMs, those inherent factors can be automatically discovered, explicitly represented, and clearly injected into the diffusion process via the sub-gradient fields. To tackle this task, we devise an unsupervised approach named DisDiff, achieving disentangled representation learning in the framework of DPMs. Extensive experiments on synthetic and real-world datasets demonstrate the effectiveness of DisDiff.},
	urldate = {2024-02-15},
	publisher = {arXiv},
	author = {Yang, Tao and Wang, Yuwang and Lv, Yan and Zheng, Nanning},
	month = oct,
	year = {2023},
	note = {arXiv:2301.13721},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yang_guardt2i_2024,
	title = {{GuardT2I}: {Defending} {Text}-to-{Image} {Models} from {Adversarial} {Prompts}},
	shorttitle = {{GuardT2I}},
	url = {http://arxiv.org/abs/2403.01446},
	doi = {10.48550/arXiv.2403.01446},
	abstract = {Recent advancements in Text-to-Image (T2I) models have raised significant safety concerns about their potential misuse for generating inappropriate or Not-Safe-For-Work (NSFW) contents, despite existing countermeasures such as NSFW classifiers or model fine-tuning for inappropriate concept removal. Addressing this challenge, our study unveils GuardT2I, a novel moderation framework that adopts a generative approach to enhance T2I models' robustness against adversarial prompts. Instead of making a binary classification, GuardT2I utilizes a Large Language Model (LLM) to conditionally transform text guidance embeddings within the T2I models into natural language for effective adversarial prompt detection, without compromising the models' inherent performance. Our extensive experiments reveal that GuardT2I outperforms leading commercial solutions like OpenAI-Moderation and Microsoft Azure Moderator by a significant margin across diverse adversarial scenarios.},
	urldate = {2024-04-11},
	publisher = {arXiv},
	author = {Yang, Yijun and Gao, Ruiyuan and Yang, Xiao and Zhong, Jianyuan and Xu, Qiang},
	month = mar,
	year = {2024},
	note = {arXiv:2403.01446},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yu_lsun_2016,
	title = {{LSUN}: {Construction} of a {Large}-scale {Image} {Dataset} using {Deep} {Learning} with {Humans} in the {Loop}},
	shorttitle = {{LSUN}},
	url = {http://arxiv.org/abs/1506.03365},
	doi = {10.48550/arXiv.1506.03365},
	abstract = {While there has been remarkable progress in the performance of visual recognition algorithms, the state-of-the-art models tend to be exceptionally data-hungry. Large labeled training datasets, expensive and tedious to produce, are required to optimize millions of parameters in deep network models. Lagging behind the growth in model capacity, the available datasets are quickly becoming outdated in terms of size and density. To circumvent this bottleneck, we propose to amplify human effort through a partially automated labeling scheme, leveraging deep learning with humans in the loop. Starting from a large set of candidate images for each category, we iteratively sample a subset, ask people to label them, classify the others with a trained model, split the set into positives, negatives, and unlabeled based on the classification confidence, and then iterate with the unlabeled set. To assess the effectiveness of this cascading procedure and enable further progress in visual recognition research, we construct a new image dataset, LSUN. It contains around one million labeled images for each of 10 scene categories and 20 object categories. We experiment with training popular convolutional networks and find that they achieve substantial performance gains when trained on this dataset.},
	urldate = {2023-12-11},
	publisher = {arXiv},
	author = {Yu, Fisher and Seff, Ari and Zhang, Yinda and Song, Shuran and Funkhouser, Thomas and Xiao, Jianxiong},
	month = jun,
	year = {2016},
	note = {arXiv:1506.03365},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhang_forget-me-not_2023,
	title = {Forget-{Me}-{Not}: {Learning} to {Forget} in {Text}-to-{Image} {Diffusion} {Models}},
	shorttitle = {Forget-{Me}-{Not}},
	url = {http://arxiv.org/abs/2303.17591},
	doi = {10.48550/arXiv.2303.17591},
	abstract = {The unlearning problem of deep learning models, once primarily an academic concern, has become a prevalent issue in the industry. The significant advances in text-to-image generation techniques have prompted global discussions on privacy, copyright, and safety, as numerous unauthorized personal IDs, content, artistic creations, and potentially harmful materials have been learned by these models and later utilized to generate and distribute uncontrolled content. To address this challenge, we propose {\textbackslash}textbf\{Forget-Me-Not\}, an efficient and low-cost solution designed to safely remove specified IDs, objects, or styles from a well-configured text-to-image model in as little as 30 seconds, without impairing its ability to generate other content. Alongside our method, we introduce the {\textbackslash}textbf\{Memorization Score (M-Score)\} and {\textbackslash}textbf\{ConceptBench\} to measure the models' capacity to generate general concepts, grouped into three primary categories: ID, object, and style. Using M-Score and ConceptBench, we demonstrate that Forget-Me-Not can effectively eliminate targeted concepts while maintaining the model's performance on other concepts. Furthermore, Forget-Me-Not offers two practical extensions: a) removal of potentially harmful or NSFW content, and b) enhancement of model accuracy, inclusion and diversity through {\textbackslash}textbf\{concept correction and disentanglement\}. It can also be adapted as a lightweight model patch for Stable Diffusion, allowing for concept manipulation and convenient distribution. To encourage future research in this critical area and promote the development of safe and inclusive generative models, we will open-source our code and ConceptBench at {\textbackslash}href\{https://github.com/SHI-Labs/Forget-Me-Not\}\{https://github.com/SHI-Labs/Forget-Me-Not\}.},
	urldate = {2023-07-22},
	publisher = {arXiv},
	author = {Zhang, Eric and Wang, Kai and Xu, Xingqian and Wang, Zhangyang and Shi, Humphrey},
	month = mar,
	year = {2023},
	note = {arXiv:2303.17591},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Machine unlearning, concept removal, potential application for IPinGenAI, protection},
}

@misc{zhao_unlearnable_2023,
	title = {Unlearnable {Examples} for {Diffusion} {Models}: {Protect} {Data} from {Unauthorized} {Exploitation}},
	shorttitle = {Unlearnable {Examples} for {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2306.01902},
	doi = {10.48550/arXiv.2306.01902},
	abstract = {Diffusion models have demonstrated remarkable performance in image generation tasks, paving the way for powerful AIGC applications. However, these widely-used generative models can also raise security and privacy concerns, such as copyright infringement, and sensitive data leakage. To tackle these issues, we propose a method, Unlearnable Diffusion Perturbation, to safeguard images from unauthorized exploitation. Our approach involves designing an algorithm to generate sample-wise perturbation noise for each image to be protected. This imperceptible protective noise makes the data almost unlearnable for diffusion models, i.e., diffusion models trained or fine-tuned on the protected data cannot generate high-quality and diverse images related to the protected training data. Theoretically, we frame this as a max-min optimization problem and introduce EUDP, a noise scheduler-based method to enhance the effectiveness of the protective noise. We evaluate our methods on both Denoising Diffusion Probabilistic Model and Latent Diffusion Models, demonstrating that training diffusion models on the protected data lead to a significant reduction in the quality of the generated images. Especially, the experimental results on Stable Diffusion demonstrate that our method effectively safeguards images from being used to train Diffusion Models in various tasks, such as training specific objects and styles. This achievement holds significant importance in real-world scenarios, as it contributes to the protection of privacy and copyright against AI-generated content.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Zhao, Zhengyue and Duan, Jinhao and Hu, Xing and Xu, Kaidi and Wang, Chenan and Zhang, Rui and Du, Zidong and Guo, Qi and Chen, Yunji},
	month = jun,
	year = {2023},
	note = {arXiv:2306.01902},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Training data copyright, cloaking, diffusion, paper, protection},
}

@misc{zhu_unpaired_2020,
	title = {Unpaired {Image}-to-{Image} {Translation} using {Cycle}-{Consistent} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1703.10593},
	doi = {10.48550/arXiv.1703.10593},
	abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X {\textbackslash}rightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y {\textbackslash}rightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) {\textbackslash}approx X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
	urldate = {2024-02-19},
	publisher = {arXiv},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	month = aug,
	year = {2020},
	note = {arXiv:1703.10593},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wu_erasediff_2024,
	title = {{EraseDiff}: {Erasing} {Data} {Influence} in {Diffusion} {Models}},
	shorttitle = {{EraseDiff}},
	url = {http://arxiv.org/abs/2401.05779},
	doi = {10.48550/arXiv.2401.05779},
	abstract = {In this work, we introduce an unlearning algorithm for diffusion models. Our algorithm equips a diffusion model with a mechanism to mitigate the concerns related to data memorization. To achieve this, we formulate the unlearning problem as a constraint optimization problem, aiming to preserve the utility of the diffusion model on the remaining data and scrub the information associated with forgetting data by deviating the learnable generative process from the ground-truth denoising procedure. To solve the resulting problem, we adopt a first-order method, having superior practical performance while being vigilant about the diffusion process. Empirically, we demonstrate that our algorithm can preserve the model utility, effectiveness, and efficiency while removing across the widely-used diffusion models and in both conditional and unconditional image generation scenarios.},
	urldate = {2024-02-08},
	publisher = {arXiv},
	author = {Wu, Jing and Le, Trung and Hayat, Munawar and Harandi, Mehrtash},
	month = feb,
	year = {2024},
	note = {arXiv:2401.05779},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, concept removal, potential application for IPinGenAI, protection},
}

@misc{webster_this_2021,
	title = {This {Person} ({Probably}) {Exists}. {Identity} {Membership} {Attacks} {Against} {GAN} {Generated} {Faces}},
	url = {http://arxiv.org/abs/2107.06018},
	doi = {10.48550/arXiv.2107.06018},
	abstract = {Recently, generative adversarial networks (GANs) have achieved stunning realism, fooling even human observers. Indeed, the popular tongue-in-cheek website \{{\textbackslash}small {\textbackslash}url\{ http://thispersondoesnotexist.com\}\}, taunts users with GAN generated images that seem too real to believe. On the other hand, GANs do leak information about their training data, as evidenced by membership attacks recently demonstrated in the literature. In this work, we challenge the assumption that GAN faces really are novel creations, by constructing a successful membership attack of a new kind. Unlike previous works, our attack can accurately discern samples sharing the same identity as training samples without being the same samples. We demonstrate the interest of our attack across several popular face datasets and GAN training procedures. Notably, we show that even in the presence of significant dataset diversity, an over represented person can pose a privacy concern.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Webster, Ryan and Rabin, Julien and Simon, Loic and Jurie, Frederic},
	month = jul,
	year = {2021},
	note = {arXiv:2107.06018},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, GANs, Privacy, membership inference, potential application for IPinGenAI, protection},
}

@misc{wen_tree-ring_2023,
	title = {Tree-{Ring} {Watermarks}: {Fingerprints} for {Diffusion} {Images} that are {Invisible} and {Robust}},
	shorttitle = {Tree-{Ring} {Watermarks}},
	url = {http://arxiv.org/abs/2305.20030},
	doi = {10.48550/arXiv.2305.20030},
	abstract = {Watermarking the outputs of generative models is a crucial technique for tracing copyright and preventing potential harm from AI-generated content. In this paper, we introduce a novel technique called Tree-Ring Watermarking that robustly fingerprints diffusion model outputs. Unlike existing methods that perform post-hoc modifications to images after sampling, Tree-Ring Watermarking subtly influences the entire sampling process, resulting in a model fingerprint that is invisible to humans. The watermark embeds a pattern into the initial noise vector used for sampling. These patterns are structured in Fourier space so that they are invariant to convolutions, crops, dilations, flips, and rotations. After image generation, the watermark signal is detected by inverting the diffusion process to retrieve the noise vector, which is then checked for the embedded signal. We demonstrate that this technique can be easily applied to arbitrary diffusion models, including text-conditioned Stable Diffusion, as a plug-in with negligible loss in FID. Our watermark is semantically hidden in the image space and is far more robust than watermarking alternatives that are currently deployed. Code is available at https://github.com/YuxinWenRick/tree-ring-watermark.},
	urldate = {2024-02-16},
	publisher = {arXiv},
	author = {Wen, Yuxin and Kirchenbauer, John and Geiping, Jonas and Goldstein, Tom},
	month = jul,
	year = {2023},
	note = {arXiv:2305.20030},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, detection of generated content, output, watermarking},
}

@misc{wen_canary_2023,
	title = {Canary in a {Coalmine}: {Better} {Membership} {Inference} with {Ensembled} {Adversarial} {Queries}},
	shorttitle = {Canary in a {Coalmine}},
	url = {http://arxiv.org/abs/2210.10750},
	doi = {10.48550/arXiv.2210.10750},
	abstract = {As industrial applications are increasingly automated by machine learning models, enforcing personal data ownership and intellectual property rights requires tracing training data back to their rightful owners. Membership inference algorithms approach this problem by using statistical techniques to discern whether a target sample was included in a model's training set. However, existing methods only utilize the unaltered target sample or simple augmentations of the target to compute statistics. Such a sparse sampling of the model's behavior carries little information, leading to poor inference capabilities. In this work, we use adversarial tools to directly optimize for queries that are discriminative and diverse. Our improvements achieve significantly more accurate membership inference than existing methods, especially in offline scenarios and in the low false-positive regime which is critical in legal settings. Code is available at https://github.com/YuxinWenRick/canary-in-a-coalmine.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Wen, Yuxin and Bansal, Arpit and Kazemi, Hamid and Borgnia, Eitan and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
	month = jun,
	year = {2023},
	note = {arXiv:2210.10750},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Training data copyright, detection of participation, discriminative models, membership inference, paper, protection, training data ownership},
}

@misc{webster_-duplication_2023,
	title = {On the {De}-duplication of {LAION}-{2B}},
	url = {http://arxiv.org/abs/2303.12733},
	doi = {10.48550/arXiv.2303.12733},
	abstract = {Generative models, such as DALL-E, Midjourney, and Stable Diffusion, have societal implications that extend beyond the field of computer science. These models require large image databases like LAION-2B, which contain two billion images. At this scale, manual inspection is difficult and automated analysis is challenging. In addition, recent studies show that duplicated images pose copyright problems for models trained on LAION2B, which hinders its usability. This paper proposes an algorithmic chain that runs with modest compute, that compresses CLIP features to enable efficient duplicate detection, even for vast image volumes. Our approach demonstrates that roughly 700 million images, or about 30{\textbackslash}\%, of LAION-2B's images are likely duplicated. Our method also provides the histograms of duplication on this dataset, which we use to reveal more examples of verbatim copies by Stable Diffusion and further justify the approach. The current version of the de-duplicated set will be distributed online.},
	urldate = {2024-04-08},
	publisher = {arXiv},
	author = {Webster, Ryan and Rabin, Julien and Simon, Loic and Jurie, Frederic},
	month = mar,
	year = {2023},
	note = {arXiv:2303.12733},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_how_2023,
	title = {How to {Detect} {Unauthorized} {Data} {Usages} in {Text}-to-image {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2307.03108},
	doi = {10.48550/arXiv.2307.03108},
	abstract = {Recent text-to-image diffusion models have shown surprising performance in generating high-quality images. However, concerns have arisen regarding the unauthorized usage of data during the training process. One example is when a model trainer collects a set of images created by a particular artist and attempts to train a model capable of generating similar images without obtaining permission from the artist. To address this issue, it becomes crucial to detect unauthorized data usage. In this paper, we propose a method for detecting such unauthorized data usage by planting injected memorization into the text-to-image diffusion models trained on the protected dataset. Specifically, we modify the protected image dataset by adding unique contents on the images such as stealthy image wrapping functions that are imperceptible to human vision but can be captured and memorized by diffusion models. By analyzing whether the model has memorization for the injected content (i.e., whether the generated images are processed by the chosen post-processing function), we can detect models that had illegally utilized the unauthorized data. Our experiments conducted on Stable Diffusion and LoRA model demonstrate the effectiveness of the proposed method in detecting unauthorized data usages.},
	urldate = {2024-01-09},
	publisher = {arXiv},
	author = {Wang, Zhenting and Chen, Chen and Liu, Yuchen and Lyu, Lingjuan and Metaxas, Dimitris and Ma, Shiqing},
	month = jul,
	year = {2023},
	note = {arXiv:2307.03108},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Training data copyright, detection, protection},
}

@misc{wang_tacotron_2017,
	title = {Tacotron: {Towards} {End}-to-{End} {Speech} {Synthesis}},
	shorttitle = {Tacotron},
	url = {http://arxiv.org/abs/1703.10135},
	doi = {10.48550/arXiv.1703.10135},
	abstract = {A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given {\textless}text, audio{\textgreater} pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Wang, Yuxuan and Skerry-Ryan, R. J. and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J. and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and Le, Quoc and Agiomyrgiannakis, Yannis and Clark, Rob and Saurous, Rif A.},
	month = apr,
	year = {2017},
	note = {arXiv:1703.10135},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound},
}

@misc{wang_security_2023,
	title = {Security and {Privacy} on {Generative} {Data} in {AIGC}: {A} {Survey}},
	shorttitle = {Security and {Privacy} on {Generative} {Data} in {AIGC}},
	url = {http://arxiv.org/abs/2309.09435},
	doi = {10.48550/arXiv.2309.09435},
	abstract = {The advent of artificial intelligence-generated content (AIGC) represents a pivotal moment in the evolution of information technology. With AIGC, it can be effortless to generate high-quality data that is challenging for the public to distinguish. Nevertheless, the proliferation of generative data across cyberspace brings security and privacy issues, including privacy leakages of individuals and media forgery for fraudulent purposes. Consequently, both academia and industry begin to emphasize the trustworthiness of generative data, successively providing a series of countermeasures for security and privacy. In this survey, we systematically review the security and privacy on generative data in AIGC, particularly for the first time analyzing them from the perspective of information security properties. Specifically, we reveal the successful experiences of state-of-the-art countermeasures in terms of the foundational properties of privacy, controllability, authenticity, and compliance, respectively. Finally, we summarize the open challenges and potential exploration directions from each of theses properties.},
	urldate = {2023-12-05},
	publisher = {arXiv},
	author = {Wang, Tao and Zhang, Yushu and Qi, Shuren and Zhao, Ruoyu and Xia, Zhihua and Weng, Jian},
	month = sep,
	year = {2023},
	note = {arXiv:2309.09435},
	keywords = {Computer Science - Cryptography and Security, Survey, generative AI},
}

@misc{wang_evaluating_2023,
	title = {Evaluating {Data} {Attribution} for {Text}-to-{Image} {Models}},
	url = {http://arxiv.org/abs/2306.09345},
	doi = {10.48550/arXiv.2306.09345},
	abstract = {While large text-to-image models are able to synthesize "novel" images, these images are necessarily a reflection of the training data. The problem of data attribution in such models -- which of the images in the training set are most responsible for the appearance of a given generated image -- is a difficult yet important one. As an initial step toward this problem, we evaluate attribution through "customization" methods, which tune an existing large-scale model toward a given exemplar object or style. Our key insight is that this allows us to efficiently create synthetic images that are computationally influenced by the exemplar by construction. With our new dataset of such exemplar-influenced images, we are able to evaluate various data attribution algorithms and different possible feature spaces. Furthermore, by training on our dataset, we can tune standard models, such as DINO, CLIP, and ViT, toward the attribution problem. Even though the procedure is tuned towards small exemplar sets, we show generalization to larger sets. Finally, by taking into account the inherent uncertainty of the problem, we can assign soft attribution scores over a set of training images.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Wang, Sheng-Yu and Efros, Alexei A. and Zhu, Jun-Yan and Zhang, Richard},
	month = aug,
	year = {2023},
	note = {arXiv:2306.09345},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Training data copyright, attribution, paper, protection},
}

@misc{van_le_anti-dreambooth_2023,
	title = {Anti-{DreamBooth}: {Protecting} users from personalized text-to-image synthesis},
	shorttitle = {Anti-{DreamBooth}},
	url = {http://arxiv.org/abs/2303.15433},
	doi = {10.48550/arXiv.2303.15433},
	abstract = {Text-to-image diffusion models are nothing but a revolution, allowing anyone, even without design skills, to create realistic images from simple text inputs. With powerful personalization tools like DreamBooth, they can generate images of a specific person just by learning from his/her few reference images. However, when misused, such a powerful and convenient tool can produce fake news or disturbing content targeting any individual victim, posing a severe negative social impact. In this paper, we explore a defense system called Anti-DreamBooth against such malicious use of DreamBooth. The system aims to add subtle noise perturbation to each user's image before publishing in order to disrupt the generation quality of any DreamBooth model trained on these perturbed images. We investigate a wide range of algorithms for perturbation optimization and extensively evaluate them on two facial datasets over various text-to-image model versions. Despite the complicated formulation of DreamBooth and Diffusion-based text-to-image models, our methods effectively defend users from the malicious use of those models. Their effectiveness withstands even adverse conditions, such as model or prompt/term mismatching between training and testing. Our code will be available at https://github.com/VinAIResearch/Anti-DreamBooth.git.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Van Le, Thanh and Phung, Hao and Nguyen, Thuan Hoang and Dao, Quan and Tran, Ngoc and Tran, Anh},
	month = oct,
	year = {2023},
	note = {arXiv:2303.15433},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Privacy, cloaking, paper, protection},
}

@misc{tsai_ring--bell_2023,
	title = {Ring-{A}-{Bell}! {How} {Reliable} are {Concept} {Removal} {Methods} for {Diffusion} {Models}?},
	url = {http://arxiv.org/abs/2310.10012},
	doi = {10.48550/arXiv.2310.10012},
	abstract = {Diffusion models for text-to-image (T2I) synthesis, such as Stable Diffusion (SD), have recently demonstrated exceptional capabilities for generating high-quality content. However, this progress has raised several concerns of potential misuse, particularly in creating copyrighted, prohibited, and restricted content, or NSFW (not safe for work) images. While efforts have been made to mitigate such problems, either by implementing a safety filter at the evaluation stage or by fine-tuning models to eliminate undesirable concepts or styles, the effectiveness of these safety measures in dealing with a wide range of prompts remains largely unexplored. In this work, we aim to investigate these safety mechanisms by proposing one novel concept retrieval algorithm for evaluation. We introduce Ring-A-Bell, a model-agnostic red-teaming tool for T2I diffusion models, where the whole evaluation can be prepared in advance without prior knowledge of the target model. Specifically, Ring-A-Bell first performs concept extraction to obtain holistic representations for sensitive and inappropriate concepts. Subsequently, by leveraging the extracted concept, Ring-A-Bell automatically identifies problematic prompts for diffusion models with the corresponding generation of inappropriate content, allowing the user to assess the reliability of deployed safety mechanisms. Finally, we empirically validate our method by testing online services such as Midjourney and various methods of concept removal. Our results show that Ring-A-Bell, by manipulating safe prompting benchmarks, can transform prompts that were originally regarded as safe to evade existing safety mechanisms, thus revealing the defects of the so-called safety mechanisms which could practically lead to the generation of harmful contents.},
	urldate = {2024-01-25},
	publisher = {arXiv},
	author = {Tsai, Yu-Lin and Hsu, Chia-Yi and Xie, Chulin and Lin, Chih-Hsun and Chen, Jia-You and Li, Bo and Chen, Pin-Yu and Yu, Chia-Mu and Huang, Chun-Ying},
	month = oct,
	year = {2023},
	note = {arXiv:2310.10012},
	keywords = {Computer Science - Machine Learning, Privacy, Sensitive content, concept removal, potential application for IPinGenAI},
}

@misc{touvron_llama_2023,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
	shorttitle = {{LLaMA}},
	url = {http://arxiv.org/abs/2302.13971},
	doi = {10.48550/arXiv.2302.13971},
	abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
	urldate = {2024-02-19},
	publisher = {arXiv},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	month = feb,
	year = {2023},
	note = {arXiv:2302.13971},
	keywords = {Computer Science - Computation and Language},
}

@misc{salman_raising_2023,
	title = {Raising the {Cost} of {Malicious} {AI}-{Powered} {Image} {Editing}},
	url = {http://arxiv.org/abs/2302.06588},
	doi = {10.48550/arXiv.2302.06588},
	abstract = {We present an approach to mitigating the risks of malicious image editing posed by large diffusion models. The key idea is to immunize images so as to make them resistant to manipulation by these models. This immunization relies on injection of imperceptible adversarial perturbations designed to disrupt the operation of the targeted diffusion models, forcing them to generate unrealistic images. We provide two methods for crafting such perturbations, and then demonstrate their efficacy. Finally, we discuss a policy component necessary to make our approach fully effective and practical -- one that involves the organizations developing diffusion models, rather than individual users, to implement (and support) the immunization process.},
	urldate = {2023-06-23},
	publisher = {arXiv},
	author = {Salman, Hadi and Khaddaj, Alaa and Leclerc, Guillaume and Ilyas, Andrew and Madry, Aleksander},
	month = feb,
	year = {2023},
	note = {arXiv:2302.06588},
	keywords = {Computer Science - Machine Learning, adversarial perturbations, data misuse, editing, image-to-image, photoguard, protection},
}

@misc{tan_somewhat_2023,
	title = {A {Somewhat} {Robust} {Image} {Watermark} against {Diffusion}-based {Editing} {Models}},
	url = {http://arxiv.org/abs/2311.13713},
	doi = {10.48550/arXiv.2311.13713},
	abstract = {Recently, diffusion models (DMs) have become the state-of-the-art method for image synthesis. Editing models based on DMs, known for their high fidelity and precision, have inadvertently introduced new challenges related to image copyright infringement and malicious editing. Our work is the first to formalize and address this issue. After assessing and attempting to enhance traditional image watermarking techniques, we recognize their limitations in this emerging context. In response, we develop a novel technique, RIW (Robust Invisible Watermarking), to embed invisible watermarks leveraging adversarial example techniques. Our technique ensures a high extraction accuracy of \$96{\textbackslash}\%\$ for the invisible watermark after editing, compared to the \$0{\textbackslash}\%\$ offered by conventional methods. We provide access to our code at https://github.com/BennyTMT/RIW.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Tan, Mingtian and Wang, Tianhao and Jha, Somesh},
	month = nov,
	year = {2023},
	note = {arXiv:2311.13713},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Watermark, editing, image-to-image, paper, prompt input copyright, protection, training data ownership, watermarking},
}

@misc{sun_generative_2023,
	title = {Generative {Adversarial} {Networks} {Unlearning}},
	url = {http://arxiv.org/abs/2308.09881},
	abstract = {As machine learning continues to develop, and data misuse scandals become more prevalent, individuals are becoming increasingly concerned about their personal information and are advocating for the right to remove their data. Machine unlearning has emerged as a solution to erase training data from trained machine learning models. Despite its success in classifiers, research on Generative Adversarial Networks (GANs) is limited due to their unique architecture, including a generator and a discriminator. One challenge pertains to generator unlearning, as the process could potentially disrupt the continuity and completeness of the latent space. This disruption might consequently diminish the model's effectiveness after unlearning. Another challenge is how to define a criterion that the discriminator should perform for the unlearning images. In this paper, we introduce a substitution mechanism and define a fake label to effectively mitigate these challenges. Based on the substitution mechanism and fake label, we propose a cascaded unlearning approach for both item and class unlearning within GAN models, in which the unlearning and learning processes run in a cascaded manner. We conducted a comprehensive evaluation of the cascaded unlearning technique using the MNIST and CIFAR-10 datasets. Experimental results demonstrate that this approach achieves significantly improved item and class unlearning efficiency, reducing the required time by up to 185x and 284x for the MNIST and CIFAR-10 datasets, respectively, in comparison to retraining from scratch. Notably, although the model's performance experiences minor degradation after unlearning, this reduction is negligible when dealing with a minimal number of images (e.g., 64) and has no adverse effects on downstream tasks such as classification.},
	urldate = {2024-01-09},
	publisher = {arXiv},
	author = {Sun, Hui and Zhu, Tianqing and Chang, Wenhan and Zhou, Wanlei},
	month = aug,
	year = {2023},
	note = {arXiv:2308.09881},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, machine unlearning},
}

@misc{somepalli_understanding_2023,
	title = {Understanding and {Mitigating} {Copying} in {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2305.20086},
	doi = {10.48550/arXiv.2305.20086},
	abstract = {Images generated by diffusion models like Stable Diffusion are increasingly widespread. Recent works and even lawsuits have shown that these models are prone to replicating their training data, unbeknownst to the user. In this paper, we first analyze this memorization problem in text-to-image diffusion models. While it is widely believed that duplicated images in the training set are responsible for content replication at inference time, we observe that the text conditioning of the model plays a similarly important role. In fact, we see in our experiments that data replication often does not happen for unconditional models, while it is common in the text-conditional case. Motivated by our findings, we then propose several techniques for reducing data replication at both training and inference time by randomizing and augmenting image captions in the training set.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Somepalli, Gowthami and Singla, Vasu and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
	month = may,
	year = {2023},
	note = {arXiv:2305.20086},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, caption modification, data replication, diffusion, protection, training data sanitisation, violation},
}

@misc{somepalli_diffusion_2022,
	title = {Diffusion {Art} or {Digital} {Forgery}? {Investigating} {Data} {Replication} in {Diffusion} {Models}},
	shorttitle = {Diffusion {Art} or {Digital} {Forgery}?},
	url = {http://arxiv.org/abs/2212.03860},
	abstract = {Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Somepalli, Gowthami and Singla, Vasu and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
	month = dec,
	year = {2022},
	note = {arXiv:2212.03860},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Machine Learning, Training data copyright, detection of participation, detection of similarity, diffusion, paper, protection},
}

@misc{shan_prompt-specific_2023,
	title = {Prompt-{Specific} {Poisoning} {Attacks} on {Text}-to-{Image} {Generative} {Models}},
	url = {http://arxiv.org/abs/2310.13828},
	doi = {10.48550/arXiv.2310.13828},
	abstract = {Data poisoning attacks manipulate training data to introduce unexpected behaviors into machine learning models at training time. For text-to-image generative models with massive training datasets, current understanding of poisoning attacks suggests that a successful attack would require injecting millions of poison samples into their training pipeline. In this paper, we show that poisoning attacks can be successful on generative models. We observe that training data per concept can be quite limited in these models, making them vulnerable to prompt-specific poisoning attacks, which target a model's ability to respond to individual prompts. We introduce Nightshade, an optimized prompt-specific poisoning attack where poison samples look visually identical to benign images with matching text prompts. Nightshade poison samples are also optimized for potency and can corrupt an Stable Diffusion SDXL prompt in {\textless}100 poison samples. Nightshade poison effects "bleed through" to related concepts, and multiple attacks can composed together in a single prompt. Surprisingly, we show that a moderate number of Nightshade attacks can destabilize general features in a text-to-image generative model, effectively disabling its ability to generate meaningful images. Finally, we propose the use of Nightshade` and similar tools as a last defense for content creators against web scrapers that ignore opt-out/do-not-crawl directives, and discuss possible implications for model trainers and content creators.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Shan, Shawn and Ding, Wenxin and Passananti, Josephine and Zheng, Haitao and Zhao, Ben Y.},
	month = oct,
	year = {2023},
	note = {arXiv:2310.13828},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Training data copyright, cloaking, data poisoning, protection},
}

@misc{schramowski_safe_2023,
	title = {Safe {Latent} {Diffusion}: {Mitigating} {Inappropriate} {Degeneration} in {Diffusion} {Models}},
	shorttitle = {Safe {Latent} {Diffusion}},
	url = {http://arxiv.org/abs/2211.05105},
	doi = {10.48550/arXiv.2211.05105},
	abstract = {Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer, as we demonstrate, from degenerated and biased human behavior. In turn, they may even reinforce such biases. To help combat these undesired side effects, we present safe latent diffusion (SLD). Specifically, to measure the inappropriate degeneration due to unfiltered and imbalanced training sets, we establish a novel image generation test bed-inappropriate image prompts (I2P)-containing dedicated, real-world image-to-text prompts covering concepts such as nudity and violence. As our exhaustive empirical evaluation demonstrates, the introduced SLD removes and suppresses inappropriate image parts during the diffusion process, with no additional training required and no adverse effect on overall image quality or text alignment.},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Schramowski, Patrick and Brack, Manuel and Deiseroth, Björn and Kersting, Kristian},
	month = apr,
	year = {2023},
	note = {arXiv:2211.05105},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Training data copyright, concept removal, potential application for IPinGenAI, protection},
}

@misc{sauer_stylegan-t_2023,
	title = {{StyleGAN}-{T}: {Unlocking} the {Power} of {GANs} for {Fast} {Large}-{Scale} {Text}-to-{Image} {Synthesis}},
	shorttitle = {{StyleGAN}-{T}},
	url = {http://arxiv.org/abs/2301.09515},
	doi = {10.48550/arXiv.2301.09515},
	abstract = {Text-to-image synthesis has recently seen significant progress thanks to large pretrained language models, large-scale training data, and the introduction of scalable model families such as diffusion and autoregressive models. However, the best-performing models require iterative evaluation to generate a single sample. In contrast, generative adversarial networks (GANs) only need a single forward pass. They are thus much faster, but they currently remain far behind the state-of-the-art in large-scale text-to-image synthesis. This paper aims to identify the necessary steps to regain competitiveness. Our proposed model, StyleGAN-T, addresses the specific requirements of large-scale text-to-image synthesis, such as large capacity, stable training on diverse datasets, strong text alignment, and controllable variation vs. text alignment tradeoff. StyleGAN-T significantly improves over previous GANs and outperforms distilled diffusion models - the previous state-of-the-art in fast text-to-image synthesis - in terms of sample quality and speed.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Sauer, Axel and Karras, Tero and Laine, Samuli and Geiger, Andreas and Aila, Timo},
	month = jan,
	year = {2023},
	note = {arXiv:2301.09515},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, GANs, exploitable properties, style transfer GANs, violation},
}

@misc{sadasivan_can_2024,
	title = {Can {AI}-{Generated} {Text} be {Reliably} {Detected}?},
	url = {http://arxiv.org/abs/2303.11156},
	doi = {10.48550/arXiv.2303.11156},
	abstract = {The unregulated use of LLMs can potentially lead to malicious consequences such as plagiarism, generating fake news, spamming, etc. Therefore, reliable detection of AI-generated text can be critical to ensure the responsible use of LLMs. Recent works attempt to tackle this problem either using certain model signatures present in the generated text outputs or by applying watermarking techniques that imprint specific patterns onto them. In this paper, we show that these detectors are not reliable in practical scenarios. In particular, we develop a recursive paraphrasing attack to apply on AI text, which can break a whole range of detectors, including the ones using the watermarking schemes as well as neural network-based detectors, zero-shot classifiers, and retrieval-based detectors. Our experiments include passages around 300 tokens in length, showing the sensitivity of the detectors even in the case of relatively long passages. We also observe that our recursive paraphrasing only degrades text quality slightly, measured via human studies, and metrics such as perplexity scores and accuracy on text benchmarks. Additionally, we show that even LLMs protected by watermarking schemes can be vulnerable against spoofing attacks aimed to mislead detectors to classify human-written text as AI-generated, potentially causing reputational damages to the developers. In particular, we show that an adversary can infer hidden AI text signatures of the LLM outputs without having white-box access to the detection method. Finally, we provide a theoretical connection between the AUROC of the best possible detector and the Total Variation distance between human and AI text distributions that can be used to study the fundamental hardness of the reliable detection problem for advanced language models. Our code is publicly available at https://github.com/vinusankars/Reliability-of-AI-text-detectors.},
	urldate = {2024-03-17},
	publisher = {arXiv},
	author = {Sadasivan, Vinu Sankar and Kumar, Aounon and Balasubramanian, Sriram and Wang, Wenxiao and Feizi, Soheil},
	month = feb,
	year = {2024},
	note = {arXiv:2303.11156},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{ren_copyright_2024,
	title = {Copyright {Protection} in {Generative} {AI}: {A} {Technical} {Perspective}},
	shorttitle = {Copyright {Protection} in {Generative} {AI}},
	url = {http://arxiv.org/abs/2402.02333},
	doi = {10.48550/arXiv.2402.02333},
	abstract = {Generative AI has witnessed rapid advancement in recent years, expanding their capabilities to create synthesized content such as text, images, audio, and code. The high fidelity and authenticity of contents generated by these Deep Generative Models (DGMs) have sparked significant copyright concerns. There have been various legal debates on how to effectively safeguard copyrights in DGMs. This work delves into this issue by providing a comprehensive overview of copyright protection from a technical perspective. We examine from two distinct viewpoints: the copyrights pertaining to the source data held by the data owners and those of the generative models maintained by the model builders. For data copyright, we delve into methods data owners can protect their content and DGMs can be utilized without infringing upon these rights. For model copyright, our discussion extends to strategies for preventing model theft and identifying outputs generated by specific models. Finally, we highlight the limitations of existing techniques and identify areas that remain unexplored. Furthermore, we discuss prospective directions for the future of copyright protection, underscoring its importance for the sustainable and ethical development of Generative AI.},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Ren, Jie and Xu, Han and He, Pengfei and Cui, Yingqian and Zeng, Shenglai and Zhang, Jiankun and Wen, Hongzhi and Ding, Jiayuan and Liu, Hui and Chang, Yi and Tang, Jiliang},
	month = feb,
	year = {2024},
	note = {arXiv:2402.02333},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, survey},
}

@misc{rando_red-teaming_2022,
	title = {Red-{Teaming} the {Stable} {Diffusion} {Safety} {Filter}},
	url = {http://arxiv.org/abs/2210.04610},
	doi = {10.48550/arXiv.2210.04610},
	abstract = {Stable Diffusion is a recent open-source image generation model comparable to proprietary models such as DALLE, Imagen, or Parti. Stable Diffusion comes with a safety filter that aims to prevent generating explicit images. Unfortunately, the filter is obfuscated and poorly documented. This makes it hard for users to prevent misuse in their applications, and to understand the filter's limitations and improve it. We first show that it is easy to generate disturbing content that bypasses the safety filter. We then reverse-engineer the filter and find that while it aims to prevent sexual content, it ignores violence, gore, and other similarly disturbing content. Based on our analysis, we argue safety measures in future model releases should strive to be fully open and properly documented to stimulate security contributions from the community.},
	urldate = {2024-02-22},
	publisher = {arXiv},
	author = {Rando, Javier and Paleka, Daniel and Lindner, David and Heim, Lennart and Tramèr, Florian},
	month = nov,
	year = {2022},
	note = {arXiv:2210.04610},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{ramesh_hierarchical_2022,
	title = {Hierarchical {Text}-{Conditional} {Image} {Generation} with {CLIP} {Latents}},
	url = {http://arxiv.org/abs/2204.06125},
	doi = {10.48550/arXiv.2204.06125},
	abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
	urldate = {2023-07-26},
	publisher = {arXiv},
	author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
	month = apr,
	year = {2022},
	note = {arXiv:2204.06125},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, text-to-image},
}

@misc{pham_circumventing_2023,
	title = {Circumventing {Concept} {Erasure} {Methods} {For} {Text}-to-{Image} {Generative} {Models}},
	url = {http://arxiv.org/abs/2308.01508},
	doi = {10.48550/arXiv.2308.01508},
	abstract = {Text-to-image generative models can produce photo-realistic images for an extremely broad range of concepts, and their usage has proliferated widely among the general public. On the flip side, these models have numerous drawbacks, including their potential to generate images featuring sexually explicit content, mirror artistic styles without permission, or even hallucinate (or deepfake) the likenesses of celebrities. Consequently, various methods have been proposed in order to "erase" sensitive concepts from text-to-image models. In this work, we examine five recently proposed concept erasure methods, and show that targeted concepts are not fully excised from any of these methods. Specifically, we leverage the existence of special learned word embeddings that can retrieve "erased" concepts from the sanitized models with no alterations to their weights. Our results highlight the brittleness of post hoc concept erasure methods, and call into question their use in the algorithmic toolkit for AI safety.},
	urldate = {2024-02-09},
	publisher = {arXiv},
	author = {Pham, Minh and Marshall, Kelly O. and Cohen, Niv and Mittal, Govind and Hegde, Chinmay},
	month = oct,
	year = {2023},
	note = {arXiv:2308.01508},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, attacks, violations},
}

@misc{peng_are_2023,
	title = {Are {You} {Copying} {My} {Model}? {Protecting} the {Copyright} of {Large} {Language} {Models} for {EaaS} via {Backdoor} {Watermark}},
	shorttitle = {Are {You} {Copying} {My} {Model}?},
	url = {http://arxiv.org/abs/2305.10036},
	doi = {10.48550/arXiv.2305.10036},
	abstract = {Large language models (LLMs) have demonstrated powerful capabilities in both text understanding and generation. Companies have begun to offer Embedding as a Service (EaaS) based on these LLMs, which can benefit various natural language processing (NLP) tasks for customers. However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive. To protect the copyright of LLMs for EaaS, we propose an Embedding Watermark method called EmbMarker that implants backdoors on embeddings. Our method selects a group of moderate-frequency words from a general text corpus to form a trigger set, then selects a target embedding as the watermark, and inserts it into the embeddings of texts containing trigger words as the backdoor. The weight of insertion is proportional to the number of trigger words included in the text. This allows the watermark backdoor to be effectively transferred to EaaS-stealer's model for copyright verification while minimizing the adverse impact on the original embeddings' utility. Our extensive experiments on various datasets show that our method can effectively protect the copyright of EaaS models without compromising service quality.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Peng, Wenjun and Yi, Jingwei and Wu, Fangzhao and Wu, Shangxi and Zhu, Bin and Lyu, Lingjuan and Jiao, Binxing and Xu, Tong and Sun, Guangzhong and Xie, Xing},
	month = jun,
	year = {2023},
	note = {arXiv:2305.10036},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society, Model copyright, Watermark, model copyright, paper, protection},
}

@misc{pawelczyk_-context_2023,
	title = {In-{Context} {Unlearning}: {Language} {Models} as {Few} {Shot} {Unlearners}},
	shorttitle = {In-{Context} {Unlearning}},
	url = {http://arxiv.org/abs/2310.07579},
	doi = {10.48550/arXiv.2310.07579},
	abstract = {Machine unlearning, the study of efficiently removing the impact of specific training points on the trained model, has garnered increased attention of late, driven by the need to comply with privacy regulations like the Right to be Forgotten. Although unlearning is particularly relevant for LLMs in light of the copyright issues they raise, achieving precise unlearning is computationally infeasible for very large models. To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or when the LLM is accessed via API. In this work, we propose a new class of unlearning methods for LLMs we call ''In-Context Unlearning'', providing inputs in context and without having to update model parameters. To unlearn a particular training instance, we provide the instance alongside a flipped label and additional correctly labelled instances which are prepended as inputs to the LLM at inference time. Our experimental results demonstrate that these contexts effectively remove specific information from the training set while maintaining performance levels that are competitive with (or in some cases exceed) state-of-the-art unlearning methods that require access to the LLM parameters.},
	urldate = {2024-02-21},
	publisher = {arXiv},
	author = {Pawelczyk, Martin and Neel, Seth and Lakkaraju, Himabindu},
	month = oct,
	year = {2023},
	note = {arXiv:2310.07579},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning, LLMs, Machine unlearning, Training data copyright, concept removal, protection, text},
}

@misc{pandey_diffusevae_2022,
	title = {{DiffuseVAE}: {Efficient}, {Controllable} and {High}-{Fidelity} {Generation} from {Low}-{Dimensional} {Latents}},
	shorttitle = {{DiffuseVAE}},
	url = {http://arxiv.org/abs/2201.00308},
	doi = {10.48550/arXiv.2201.00308},
	abstract = {Diffusion probabilistic models have been shown to generate state-of-the-art results on several competitive image synthesis benchmarks but lack a low-dimensional, interpretable latent space, and are slow at generation. On the other hand, standard Variational Autoencoders (VAEs) typically have access to a low-dimensional latent space but exhibit poor sample quality. We present DiffuseVAE, a novel generative framework that integrates VAE within a diffusion model framework, and leverage this to design novel conditional parameterizations for diffusion models. We show that the resulting model equips diffusion models with a low-dimensional VAE inferred latent code which can be used for downstream tasks like controllable synthesis. The proposed method also improves upon the speed vs quality tradeoff exhibited in standard unconditional DDPM/DDIM models (for instance, FID of 16.47 vs 34.36 using a standard DDIM on the CelebA-HQ-128 benchmark using T=10 reverse process steps) without having explicitly trained for such an objective. Furthermore, the proposed model exhibits synthesis quality comparable to state-of-the-art models on standard image synthesis benchmarks like CIFAR-10 and CelebA-64 while outperforming most existing VAE-based methods. Lastly, we show that the proposed method exhibits inherent generalization to different types of noise in the conditioning signal. For reproducibility, our source code is publicly available at https://github.com/kpandey008/DiffuseVAE.},
	urldate = {2024-02-20},
	publisher = {arXiv},
	author = {Pandey, Kushagra and Mukherjee, Avideep and Rai, Piyush and Kumar, Abhishek},
	month = nov,
	year = {2022},
	note = {arXiv:2201.00308},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{openai_gpt-4_2023,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2023-07-26},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08774},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{novelli_generative_2024,
	title = {Generative {AI} in {EU} {Law}: {Liability}, {Privacy}, {Intellectual} {Property}, and {Cybersecurity}},
	shorttitle = {Generative {AI} in {EU} {Law}},
	url = {http://arxiv.org/abs/2401.07348},
	doi = {10.48550/arXiv.2401.07348},
	abstract = {The advent of Generative AI, particularly through Large Language Models (LLMs) like ChatGPT and its successors, marks a paradigm shift in the AI landscape. Advanced LLMs exhibit multimodality, handling diverse data formats, thereby broadening their application scope. However, the complexity and emergent autonomy of these models introduce challenges in predictability and legal compliance. This paper delves into the legal and regulatory implications of Generative AI and LLMs in the European Union context, analyzing aspects of liability, privacy, intellectual property, and cybersecurity. It critically examines the adequacy of the existing and proposed EU legislation, including the Artificial Intelligence Act (AIA) draft, in addressing the unique challenges posed by Generative AI in general and LLMs in particular. The paper identifies potential gaps and shortcomings in the legislative framework and proposes recommendations to ensure the safe and compliant deployment of generative models, ensuring they align with the EU's evolving digital landscape and legal standards.},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Novelli, Claudio and Casolari, Federico and Hacker, Philipp and Spedicato, Giorgio and Floridi, Luciano},
	month = mar,
	year = {2024},
	note = {arXiv:2401.07348},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, legal, survey},
}

@misc{moon_feature_2023,
	title = {Feature {Unlearning} for {Pre}-trained {GANs} and {VAEs}},
	url = {http://arxiv.org/abs/2303.05699},
	doi = {10.48550/arXiv.2303.05699},
	abstract = {We tackle the problem of feature unlearning from a pre-trained image generative model: GANs and VAEs. Unlike a common unlearning task where an unlearning target is a subset of the training set, we aim to unlearn a specific feature, such as hairstyle from facial images, from the pre-trained generative models. As the target feature is only presented in a local region of an image, unlearning the entire image from the pre-trained model may result in losing other details in the remaining region of the image. To specify which features to unlearn, we collect randomly generated images that contain the target features. We then identify a latent representation corresponding to the target feature and then use the representation to fine-tune the pre-trained model. Through experiments on MNIST and CelebA datasets, we show that target features are successfully removed while keeping the fidelity of the original models. Further experiments with an adversarial attack show that the unlearned model is more robust under the presence of malicious parties.},
	urldate = {2024-01-09},
	publisher = {arXiv},
	author = {Moon, Saemi and Cho, Seunghyuk and Kim, Dongwoo},
	month = aug,
	year = {2023},
	note = {arXiv:2303.05699},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, machine unlearning},
}

@misc{matsumoto_membership_2023,
	title = {Membership {Inference} {Attacks} against {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2302.03262},
	doi = {10.48550/arXiv.2302.03262},
	abstract = {Diffusion models have attracted attention in recent years as innovative generative models. In this paper, we investigate whether a diffusion model is resistant to a membership inference attack, which evaluates the privacy leakage of a machine learning model. We primarily discuss the diffusion model from the standpoints of comparison with a generative adversarial network (GAN) as conventional models and hyperparameters unique to the diffusion model, i.e., time steps, sampling steps, and sampling variances. We conduct extensive experiments with DDIM as a diffusion model and DCGAN as a GAN on the CelebA and CIFAR-10 datasets in both white-box and black-box settings and then confirm if the diffusion model is comparably resistant to a membership inference attack as GAN. Next, we demonstrate that the impact of time steps is significant and intermediate steps in a noise schedule are the most vulnerable to the attack. We also found two key insights through further analysis. First, we identify that DDIM is vulnerable to the attack for small sample sizes instead of achieving a lower FID. Second, sampling steps in hyperparameters are important for resistance to the attack, whereas the impact of sampling variances is quite limited.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Matsumoto, Tomoya and Miura, Takayuki and Yanai, Naoto},
	month = mar,
	year = {2023},
	note = {arXiv:2302.03262},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, detection of participation, membership inference, potential application for IPinGenAI, protection},
}

@misc{ma_generative_2023,
	title = {Generative {Watermarking} {Against} {Unauthorized} {Subject}-{Driven} {Image} {Synthesis}},
	url = {http://arxiv.org/abs/2306.07754},
	doi = {10.48550/arXiv.2306.07754},
	abstract = {Large text-to-image models have shown remarkable performance in synthesizing high-quality images. In particular, the subject-driven model makes it possible to personalize the image synthesis for a specific subject, e.g., a human face or an artistic style, by fine-tuning the generic text-to-image model with a few images from that subject. Nevertheless, misuse of subject-driven image synthesis may violate the authority of subject owners. For example, malicious users may use subject-driven synthesis to mimic specific artistic styles or to create fake facial images without authorization. To protect subject owners against such misuse, recent attempts have commonly relied on adversarial examples to indiscriminately disrupt subject-driven image synthesis. However, this essentially prevents any benign use of subject-driven synthesis based on protected images. In this paper, we take a different angle and aim at protection without sacrificing the utility of protected images for general synthesis purposes. Specifically, we propose GenWatermark, a novel watermark system based on jointly learning a watermark generator and a detector. In particular, to help the watermark survive the subject-driven synthesis, we incorporate the synthesis process in learning GenWatermark by fine-tuning the detector with synthesized images for a specific subject. This operation is shown to largely improve the watermark detection accuracy and also ensure the uniqueness of the watermark for each individual subject. Extensive experiments validate the effectiveness of GenWatermark, especially in practical scenarios with unknown models and text prompts (74\% Acc.), as well as partial data watermarking (80\% Acc. for 1/4 watermarking). We also demonstrate the robustness of GenWatermark to two potential countermeasures that substantially degrade the synthesis quality.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Ma, Yihan and Zhao, Zhengyu and He, Xinlei and Li, Zheng and Backes, Michael and Zhang, Yang},
	month = jun,
	year = {2023},
	note = {arXiv:2306.07754},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Training data copyright, protection, watermark},
}

@misc{lovato_foregrounding_2024,
	title = {Foregrounding {Artist} {Opinions}: {A} {Survey} {Study} on {Transparency}, {Ownership}, and {Fairness} in {AI} {Generative} {Art}},
	shorttitle = {Foregrounding {Artist} {Opinions}},
	url = {http://arxiv.org/abs/2401.15497},
	doi = {10.48550/arXiv.2401.15497},
	abstract = {Generative Artificial Intelligence (AI) tools are used to create art-like outputs and aid in the creative process. While these tools have potential benefits for artists, they also have the potential to harm the art workforce and infringe upon artistic and intellectual property rights. Without explicit consent from artists, Generative AI creators scrape artists' digital work to train Generative AI models and produce art-like model outputs at scale. These outputs are now being used to compete with human artists in the marketplace as well as being used by some artists in their generative processes to create art. We surveyed 459 artists to investigate the tension between artists' opinions on Generative AI art's potential utility and harm. This study surveys artists' opinions on the utility and threat of Generative AI art models, fair practices in the disclosure of artistic works in AI art training models, ownership and rights of AI art derivatives, and fair compensation. We find that artists, by and large, think that model creators should be required to disclose in detail what art and images they use to train their AI models. We also find that artists' opinions vary by professional status and practice, demographics, whether they have purchased art, and familiarity with and use of Generative AI. We hope the results of this work will further more meaningful collaboration and alignment between the art community and Generative AI researchers and developers.},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Lovato, Juniper and Zimmerman, Julia and Smith, Isabelle and Dodds, Peter and Karson, Jennifer},
	month = feb,
	year = {2024},
	note = {arXiv:2401.15497},
	keywords = {Computer Science - Computers and Society, survey},
}

@misc{liu_geom-erasing_2023,
	title = {Geom-{Erasing}: {Geometry}-{Driven} {Removal} of {Implicit} {Concept} in {Diffusion} {Models}},
	shorttitle = {Geom-{Erasing}},
	url = {http://arxiv.org/abs/2310.05873},
	doi = {10.48550/arXiv.2310.05873},
	abstract = {Fine-tuning diffusion models through personalized datasets is an acknowledged method for improving generation quality across downstream tasks, which, however, often inadvertently generates unintended concepts such as watermarks and QR codes, attributed to the limitations in image sources and collecting methods within specific downstream tasks. Existing solutions suffer from eliminating these unintentionally learned implicit concepts, primarily due to the dependency on the model's ability to recognize concepts that it actually cannot discern. In this work, we introduce Geom-Erasing, a novel approach that successfully removes the implicit concepts with either an additional accessible classifier or detector model to encode geometric information of these concepts into text domain. Moreover, we propose Implicit Concept, a novel image-text dataset imbued with three implicit concepts (i.e., watermarks, QR codes, and text) for training and evaluation. Experimental results demonstrate that Geom-Erasing not only identifies but also proficiently eradicates implicit concepts, revealing a significant improvement over the existing methods. The integration of geometric information marks a substantial progression in the precise removal of implicit concepts in diffusion models.},
	urldate = {2024-01-25},
	publisher = {arXiv},
	author = {Liu, Zhili and Chen, Kai and Zhang, Yifan and Han, Jianhua and Hong, Lanqing and Xu, Hang and Li, Zhenguo and Yeung, Dit-Yan and Kwok, James},
	month = oct,
	year = {2023},
	note = {arXiv:2310.05873},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{liu_watermarking_2023,
	title = {Watermarking {Diffusion} {Model}},
	url = {http://arxiv.org/abs/2305.12502},
	doi = {10.48550/arXiv.2305.12502},
	abstract = {The availability and accessibility of diffusion models (DMs) have significantly increased in recent years, making them a popular tool for analyzing and predicting the spread of information, behaviors, or phenomena through a population. Particularly, text-to-image diffusion models (e.g., DALLE 2 and Latent Diffusion Models (LDMs) have gained significant attention in recent years for their ability to generate high-quality images and perform various image synthesis tasks. Despite their widespread adoption in many fields, DMs are often susceptible to various intellectual property violations. These can include not only copyright infringement but also more subtle forms of misappropriation, such as unauthorized use or modification of the model. Therefore, DM owners must be aware of these potential risks and take appropriate steps to protect their models. In this work, we are the first to protect the intellectual property of DMs. We propose a simple but effective watermarking scheme that injects the watermark into the DMs and can be verified by the pre-defined prompts. In particular, we propose two different watermarking methods, namely NAIVEWM and FIXEDWM. The NAIVEWM method injects the watermark into the LDMs and activates it using a prompt containing the watermark. On the other hand, the FIXEDWM is considered more advanced and stealthy compared to the NAIVEWM, as it can only activate the watermark when using a prompt containing a trigger in a fixed position. We conducted a rigorous evaluation of both approaches, demonstrating their effectiveness in watermark injection and verification with minimal impact on the LDM's functionality.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Liu, Yugeng and Li, Zheng and Backes, Michael and Shen, Yun and Zhang, Yang},
	month = may,
	year = {2023},
	note = {arXiv:2305.12502},
	keywords = {Computer Science - Cryptography and Security, Model copyright, protection, watermark},
}

@misc{liu_rethinking_2024,
	title = {Rethinking {Machine} {Unlearning} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2402.08787},
	doi = {10.48550/arXiv.2402.08787},
	abstract = {We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We also draw connections between LLM unlearning and related areas such as model editing, influence functions, model explanation, adversarial training, and reinforcement learning. Furthermore, we outline an effective assessment framework for LLM unlearning and explore its applications in copyright and privacy safeguards and sociotechnical harm reduction.},
	urldate = {2024-02-21},
	publisher = {arXiv},
	author = {Liu, Sijia and Yao, Yuanshun and Jia, Jinghan and Casper, Stephen and Baracaldo, Nathalie and Hase, Peter and Xu, Xiaojun and Yao, Yuguang and Li, Hang and Varshney, Kush R. and Bansal, Mohit and Koyejo, Sanmi and Liu, Yang},
	month = feb,
	year = {2024},
	note = {arXiv:2402.08787},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, LLMs, Machine unlearning, Training data copyright, concept removal, protection, text},
}

@misc{liu_diffprotect_2023,
	title = {{DiffProtect}: {Generate} {Adversarial} {Examples} with {Diffusion} {Models} for {Facial} {Privacy} {Protection}},
	shorttitle = {{DiffProtect}},
	url = {http://arxiv.org/abs/2305.13625},
	doi = {10.48550/arXiv.2305.13625},
	abstract = {The increasingly pervasive facial recognition (FR) systems raise serious concerns about personal privacy, especially for billions of users who have publicly shared their photos on social media. Several attempts have been made to protect individuals from being identified by unauthorized FR systems utilizing adversarial attacks to generate encrypted face images. However, existing methods suffer from poor visual quality or low attack success rates, which limit their utility. Recently, diffusion models have achieved tremendous success in image generation. In this work, we ask: can diffusion models be used to generate adversarial examples to improve both visual quality and attack performance? We propose DiffProtect, which utilizes a diffusion autoencoder to generate semantically meaningful perturbations on FR systems. Extensive experiments demonstrate that DiffProtect produces more natural-looking encrypted images than state-of-the-art methods while achieving significantly higher attack success rates, e.g., 24.5\% and 25.1\% absolute improvements on the CelebA-HQ and FFHQ datasets.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Liu, Jiang and Lau, Chun Pong and Chellappa, Rama},
	month = may,
	year = {2023},
	note = {arXiv:2305.13625},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Privacy},
}

@misc{liang_adversarial_2023,
	title = {Adversarial {Example} {Does} {Good}: {Preventing} {Painting} {Imitation} from {Diffusion} {Models} via {Adversarial} {Examples}},
	shorttitle = {Adversarial {Example} {Does} {Good}},
	url = {http://arxiv.org/abs/2302.04578},
	doi = {10.48550/arXiv.2302.04578},
	abstract = {Recently, Diffusion Models (DMs) boost a wave in AI for Art yet raise new copyright concerns, where infringers benefit from using unauthorized paintings to train DMs to generate novel paintings in a similar style. To address these emerging copyright violations, in this paper, we are the first to explore and propose to utilize adversarial examples for DMs to protect human-created artworks. Specifically, we first build a theoretical framework to define and evaluate the adversarial examples for DMs. Then, based on this framework, we design a novel algorithm, named AdvDM, which exploits a Monte-Carlo estimation of adversarial examples for DMs by optimizing upon different latent variables sampled from the reverse process of DMs. Extensive experiments show that the generated adversarial examples can effectively hinder DMs from extracting their features. Therefore, our method can be a powerful tool for human artists to protect their copyright against infringers equipped with DM-based AI-for-Art applications. The code of our method is available on GitHub: https://github.com/mist-project/mist.git.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Liang, Chumeng and Wu, Xiaoyu and Hua, Yang and Zhang, Jiaru and Xue, Yiming and Song, Tao and Xue, Zhengui and Ma, Ruhui and Guan, Haibing},
	month = jun,
	year = {2023},
	note = {arXiv:2302.04578},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Training data copyright, adversarial perturbations, diffusion, paper, protection},
}

@misc{liang_mist_2023,
	title = {Mist: {Towards} {Improved} {Adversarial} {Examples} for {Diffusion} {Models}},
	shorttitle = {Mist},
	url = {http://arxiv.org/abs/2305.12683},
	doi = {10.48550/arXiv.2305.12683},
	abstract = {Diffusion Models (DMs) have empowered great success in artificial-intelligence-generated content, especially in artwork creation, yet raising new concerns in intellectual properties and copyright. For example, infringers can make profits by imitating non-authorized human-created paintings with DMs. Recent researches suggest that various adversarial examples for diffusion models can be effective tools against these copyright infringements. However, current adversarial examples show weakness in transferability over different painting-imitating methods and robustness under straightforward adversarial defense, for example, noise purification. We surprisingly find that the transferability of adversarial examples can be significantly enhanced by exploiting a fused and modified adversarial loss term under consistent parameters. In this work, we comprehensively evaluate the cross-method transferability of adversarial examples. The experimental observation shows that our method generates more transferable adversarial examples with even stronger robustness against the simple adversarial defense.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Liang, Chumeng and Wu, Xiaoyu},
	month = may,
	year = {2023},
	note = {arXiv:2305.12683},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Training data copyright, cloaking, diffusion, protection},
}

@misc{li_blip_2022,
	title = {{BLIP}: {Bootstrapping} {Language}-{Image} {Pre}-training for {Unified} {Vision}-{Language} {Understanding} and {Generation}},
	shorttitle = {{BLIP}},
	url = {http://arxiv.org/abs/2201.12086},
	doi = {10.48550/arXiv.2201.12086},
	abstract = {Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7\% in average recall@1), image captioning (+2.8\% in CIDEr), and VQA (+1.6\% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at https://github.com/salesforce/BLIP.},
	urldate = {2023-07-26},
	publisher = {arXiv},
	author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
	month = feb,
	year = {2022},
	note = {arXiv:2201.12086},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{li_prime_2024,
	title = {{PRIME}: {Protect} {Your} {Videos} {From} {Malicious} {Editing}},
	shorttitle = {{PRIME}},
	url = {http://arxiv.org/abs/2402.01239},
	doi = {10.48550/arXiv.2402.01239},
	abstract = {With the development of generative models, the quality of generated content keeps increasing. Recently, open-source models have made it surprisingly easy to manipulate and edit photos and videos, with just a few simple prompts. While these cutting-edge technologies have gained popularity, they have also given rise to concerns regarding the privacy and portrait rights of individuals. Malicious users can exploit these tools for deceptive or illegal purposes. Although some previous works focus on protecting photos against generative models, we find there are still gaps between protecting videos and images in the aspects of efficiency and effectiveness. Therefore, we introduce our protection method, PRIME, to significantly reduce the time cost and improve the protection performance. Moreover, to evaluate our proposed protection method, we consider both objective metrics and human subjective metrics. Our evaluation results indicate that PRIME only costs 8.3\% GPU hours of the cost of the previous state-of-the-art method and achieves better protection results on both human evaluation and objective metrics. Code can be found in https://github.com/GuanlinLee/prime.},
	urldate = {2024-04-10},
	publisher = {arXiv},
	author = {Li, Guanlin and Yang, Shuai and Zhang, Jie and Zhang, Tianwei},
	month = feb,
	year = {2024},
	note = {arXiv:2402.01239},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, todo},
}

@misc{li_survey_2023,
	title = {A {Survey} of {Large} {Language} {Models} {Attribution}},
	url = {http://arxiv.org/abs/2311.03731},
	doi = {10.48550/arXiv.2311.03731},
	abstract = {Open-domain generative systems have gained significant attention in the field of conversational AI (e.g., generative search engines). This paper presents a comprehensive review of the attribution mechanisms employed by these systems, particularly large language models. Though attribution or citation improve the factuality and verifiability, issues like ambiguous knowledge reservoirs, inherent biases, and the drawbacks of excessive attribution can hinder the effectiveness of these systems. The aim of this survey is to provide valuable insights for researchers, aiding in the refinement of attribution methodologies to enhance the reliability and veracity of responses generated by open-domain generative systems. We believe that this field is still in its early stages; hence, we maintain a repository to keep track of ongoing studies at https://github.com/HITsz-TMG/awesome-llm-attributions.},
	urldate = {2024-02-21},
	publisher = {arXiv},
	author = {Li, Dongfang and Sun, Zetian and Hu, Xinshuo and Liu, Zhenyu and Chen, Ziyang and Hu, Baotian and Wu, Aiguo and Zhang, Min},
	month = dec,
	year = {2023},
	note = {arXiv:2311.03731},
	keywords = {Computer Science - Computation and Language, LLMs, attribution, survey},
}

@misc{li_mitigate_2024,
	title = {Mitigate {Replication} and {Copying} in {Diffusion} {Models} with {Generalized} {Caption} and {Dual} {Fusion} {Enhancement}},
	url = {http://arxiv.org/abs/2309.07254},
	abstract = {While diffusion models demonstrate a remarkable capability for generating high-quality images, their tendency to `replicate' training data raises privacy concerns. Although recent research suggests that this replication may stem from the insufficient generalization of training data captions and duplication of training images, effective mitigation strategies remain elusive. To address this gap, our paper first introduces a generality score that measures the caption generality and employ large language model (LLM) to generalize training captions. Subsequently, we leverage generalized captions and propose a novel dual fusion enhancement approach to mitigate the replication of diffusion models. Our empirical results demonstrate that our proposed methods can significantly reduce replication by 43.5\% compared to the original diffusion model while maintaining the diversity and quality of generations. Code is available at https://github.com/HowardLi0816/dual-fusion-diffusion.},
	urldate = {2024-02-21},
	publisher = {arXiv},
	author = {Li, Chenghao and Chen, Dake and Zhang, Yuke and Beerel, Peter A.},
	month = jan,
	year = {2024},
	note = {arXiv:2309.07254},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, training data sanitisation},
}

@misc{lee_deduplicating_2022,
	title = {Deduplicating {Training} {Data} {Makes} {Language} {Models} {Better}},
	url = {http://arxiv.org/abs/2107.06499},
	doi = {10.48550/arXiv.2107.06499},
	abstract = {We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings. As a result, over 1\% of the unprompted output of language models trained on these datasets is copied verbatim from the training data. We develop two tools that allow us to deduplicate training datasets -- for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times. Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer train steps to achieve the same or better accuracy. We can also reduce train-test overlap, which affects over 4\% of the validation set of standard datasets, thus allowing for more accurate evaluation. We release code for reproducing our work and performing dataset deduplication at https://github.com/google-research/deduplicate-text-datasets.},
	urldate = {2024-04-10},
	publisher = {arXiv},
	author = {Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
	month = mar,
	year = {2022},
	note = {arXiv:2107.06499},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{kumari_ablating_2023,
	title = {Ablating {Concepts} in {Text}-to-{Image} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2303.13516},
	doi = {10.48550/arXiv.2303.13516},
	abstract = {Large-scale text-to-image diffusion models can generate high-fidelity images with powerful compositional ability. However, these models are typically trained on an enormous amount of Internet data, often containing copyrighted material, licensed images, and personal photos. Furthermore, they have been found to replicate the style of various living artists or memorize exact training samples. How can we remove such copyrighted concepts or images without retraining the model from scratch? To achieve this goal, we propose an efficient method of ablating concepts in the pretrained model, i.e., preventing the generation of a target concept. Our algorithm learns to match the image distribution for a target style, instance, or text prompt we wish to ablate to the distribution corresponding to an anchor concept. This prevents the model from generating target concepts given its text condition. Extensive experiments show that our method can successfully prevent the generation of the ablated concept while preserving closely related concepts in the model.},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Kumari, Nupur and Zhang, Bingliang and Wang, Sheng-Yu and Shechtman, Eli and Zhang, Richard and Zhu, Jun-Yan},
	month = may,
	year = {2023},
	note = {arXiv:2303.13516},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning, Training data copyright, concept removal, paper, protection},
}

@misc{kazemi_style_2018,
	title = {Style and {Content} {Disentanglement} in {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1811.05621},
	abstract = {Disentangling factors of variation within data has become a very challenging problem for image generation tasks. Current frameworks for training a Generative Adversarial Network (GAN), learn to disentangle the representations of the data in an unsupervised fashion and capture the most significant factors of the data variations. However, these approaches ignore the principle of content and style disentanglement in image generation, which means their learned latent code may alter the content and style of the generated images at the same time. This paper describes the Style and Content Disentangled GAN (SC-GAN), a new unsupervised algorithm for training GANs that learns disentangled style and content representations of the data. We assume that the representation of an image can be decomposed into a content code that represents the geometrical information of the data, and a style code that captures textural properties. Consequently, by fixing the style portion of the latent representation, we can generate diverse images in a particular style. Reversely, we can set the content code and generate a specific scene in a variety of styles. The proposed SC-GAN has two components: a content code which is the input to the generator, and a style code which modifies the scene style through modification of the Adaptive Instance Normalization (AdaIN) layers' parameters. We evaluate the proposed SC-GAN framework on a set of baseline datasets.},
	urldate = {2024-02-16},
	publisher = {arXiv},
	author = {Kazemi, Hadi and Iranmanesh, Seyed Mehdi and Nasrabadi, Nasser M.},
	month = nov,
	year = {2018},
	note = {arXiv:1811.05621},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{jiang_evading_2023,
	title = {Evading {Watermark} based {Detection} of {AI}-{Generated} {Content}},
	url = {http://arxiv.org/abs/2305.03807},
	abstract = {A generative AI model can generate extremely realistic-looking content, posing growing challenges to the authenticity of information. To address the challenges, watermark has been leveraged to detect AI-generated content. Specifically, a watermark is embedded into an AI-generated content before it is released. A content is detected as AI-generated if a similar watermark can be decoded from it. In this work, we perform a systematic study on the robustness of such watermark-based AI-generated content detection. We focus on AI-generated images. Our work shows that an attacker can post-process a watermarked image via adding a small, human-imperceptible perturbation to it, such that the post-processed image evades detection while maintaining its visual quality. We show the effectiveness of our attack both theoretically and empirically. Moreover, to evade detection, our adversarial post-processing method adds much smaller perturbations to AI-generated images and thus better maintain their visual quality than existing popular post-processing methods such as JPEG compression, Gaussian blur, and Brightness/Contrast. Our work shows the insufficiency of existing watermark-based detection of AI-generated content, highlighting the urgent needs of new methods. Our code is publicly available: https://github.com/zhengyuan-jiang/WEvade.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Jiang, Zhengyuan and Zhang, Jinghuai and Gong, Neil Zhenqiang},
	month = nov,
	year = {2023},
	note = {arXiv:2305.03807},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{lee_language_2023,
	address = {Austin TX USA},
	title = {Do {Language} {Models} {Plagiarize}?},
	isbn = {978-1-4503-9416-1},
	url = {https://dl.acm.org/doi/10.1145/3543507.3583199},
	doi = {10.1145/3543507.3583199},
	language = {en},
	urldate = {2024-04-19},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2023},
	publisher = {ACM},
	author = {Lee, Jooyoung and Le, Thai and Chen, Jinghui and Lee, Dongwon},
	month = apr,
	year = {2023},
	pages = {3637--3647},
}

@misc{kwon_diagonal_2021,
	title = {Diagonal {Attention} and {Style}-based {GAN} for {Content}-{Style} {Disentanglement} in {Image} {Generation} and {Translation}},
	url = {http://arxiv.org/abs/2103.16146},
	doi = {10.48550/arXiv.2103.16146},
	abstract = {One of the important research topics in image generative models is to disentangle the spatial contents and styles for their separate control. Although StyleGAN can generate content feature vectors from random noises, the resulting spatial content control is primarily intended for minor spatial variations, and the disentanglement of global content and styles is by no means complete. Inspired by a mathematical understanding of normalization and attention, here we present a novel hierarchical adaptive Diagonal spatial ATtention (DAT) layers to separately manipulate the spatial contents from styles in a hierarchical manner. Using DAT and AdaIN, our method enables coarse-to-fine level disentanglement of spatial contents and styles. In addition, our generator can be easily integrated into the GAN inversion framework so that the content and style of translated images from multi-domain image translation tasks can be flexibly controlled. By using various datasets, we confirm that the proposed method not only outperforms the existing models in disentanglement scores, but also provides more flexible control over spatial features in the generated images.},
	urldate = {2024-04-08},
	publisher = {arXiv},
	author = {Kwon, Gihyun and Ye, Jong Chul},
	month = jul,
	year = {2021},
	note = {arXiv:2103.16146},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{karayev_recognizing_2014,
	title = {Recognizing {Image} {Style}},
	url = {http://arxiv.org/abs/1311.3715},
	doi = {10.5244/C.28.122},
	abstract = {The style of an image plays a significant role in how it is viewed, but style has received little attention in computer vision research. We describe an approach to predicting style of images, and perform a thorough evaluation of different image features for these tasks. We find that features learned in a multi-layer network generally perform best -- even when trained with object class (not style) labels. Our large-scale learning methods results in the best published performance on an existing dataset of aesthetic ratings and photographic style annotations. We present two novel datasets: 80K Flickr photographs annotated with 20 curated style labels, and 85K paintings annotated with 25 style/genre labels. Our approach shows excellent classification performance on both datasets. We use the learned classifiers to extend traditional tag-based image search to consider stylistic constraints, and demonstrate cross-dataset understanding of style.},
	urldate = {2024-01-31},
	booktitle = {Proceedings of the {British} {Machine} {Vision} {Conference} 2014},
	author = {Karayev, Sergey and Trentacoste, Matthew and Han, Helen and Agarwala, Aseem and Darrell, Trevor and Hertzmann, Aaron and Winnemoeller, Holger},
	year = {2014},
	note = {arXiv:1311.3715},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, detection},
	pages = {122.1--122.11},
}

@misc{jang_knowledge_2022,
	title = {Knowledge {Unlearning} for {Mitigating} {Privacy} {Risks} in {Language} {Models}},
	url = {http://arxiv.org/abs/2210.01504},
	doi = {10.48550/arXiv.2210.01504},
	abstract = {Pretrained Language Models (LMs) memorize a vast amount of knowledge during initial pretraining, including information that may violate the privacy of personal lives and identities. Previous work addressing privacy issues for language models has mostly focused on data preprocessing and differential privacy methods, both requiring re-training the underlying LM. We propose knowledge unlearning as an alternative method to reduce privacy risks for LMs post hoc. We show that simply performing gradient ascent on target token sequences is effective at forgetting them with little to no degradation of general language modeling performances for larger LMs; it sometimes even substantially improves the underlying LM with just a few iterations. We also find that sequential unlearning is better than trying to unlearn all the data at once and that unlearning is highly dependent on which kind of data (domain) is forgotten. By showing comparisons with a previous data preprocessing method and a decoding method known to mitigate privacy risks for LMs, we show that unlearning can give a stronger empirical privacy guarantee in scenarios where the data vulnerable to extraction attacks are known a priori while being much more efficient and robust. We release the code and dataset needed to replicate our results at https://github.com/joeljang/knowledge-unlearning.},
	urldate = {2024-02-21},
	publisher = {arXiv},
	author = {Jang, Joel and Yoon, Dongkeun and Yang, Sohee and Cha, Sungmin and Lee, Moontae and Logeswaran, Lajanugen and Seo, Minjoon},
	month = dec,
	year = {2022},
	note = {arXiv:2210.01504},
	keywords = {Computer Science - Computation and Language, llms, machine unlearning},
}

@misc{jagielski_measuring_2023,
	title = {Measuring {Forgetting} of {Memorized} {Training} {Examples}},
	url = {http://arxiv.org/abs/2207.00099},
	doi = {10.48550/arXiv.2207.00099},
	abstract = {Machine learning models exhibit two seemingly contradictory phenomena: training data memorization, and various forms of forgetting. In memorization, models overfit specific training examples and become susceptible to privacy attacks. In forgetting, examples which appeared early in training are forgotten by the end. In this work, we connect these phenomena. We propose a technique to measure to what extent models "forget" the specifics of training examples, becoming less susceptible to privacy attacks on examples they have not seen recently. We show that, while non-convex models can memorize data forever in the worst-case, standard image, speech, and language models empirically do forget examples over time. We identify nondeterminism as a potential explanation, showing that deterministically trained models do not forget. Our results suggest that examples seen early when training with extremely large datasets - for instance those examples used to pre-train a model - may observe privacy benefits at the expense of examples seen later.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Jagielski, Matthew and Thakkar, Om and Tramèr, Florian and Ippolito, Daphne and Lee, Katherine and Carlini, Nicholas and Wallace, Eric and Song, Shuang and Thakurta, Abhradeep and Papernot, Nicolas and Zhang, Chiyuan},
	month = may,
	year = {2023},
	note = {arXiv:2207.00099},
	keywords = {Computer Science - Machine Learning, memorisation},
}

@misc{huang_arbitrary_2017,
	title = {Arbitrary {Style} {Transfer} in {Real}-time with {Adaptive} {Instance} {Normalization}},
	url = {http://arxiv.org/abs/1703.06868},
	abstract = {Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition, our approach allows flexible user controls such as content-style trade-off, style interpolation, color \& spatial controls, all using a single feed-forward neural network.},
	urldate = {2024-04-08},
	publisher = {arXiv},
	author = {Huang, Xun and Belongie, Serge},
	month = jul,
	year = {2017},
	note = {arXiv:1703.06868},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	doi = {10.48550/arXiv.2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	urldate = {2024-02-08},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, violation},
}

@misc{hu_membership_2023,
	title = {Membership {Inference} of {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2301.09956},
	doi = {10.48550/arXiv.2301.09956},
	abstract = {Recent years have witnessed the tremendous success of diffusion models in data synthesis. However, when diffusion models are applied to sensitive data, they also give rise to severe privacy concerns. In this paper, we systematically present the first study about membership inference attacks against diffusion models, which aims to infer whether a sample was used to train the model. Two attack methods are proposed, namely loss-based and likelihood-based attacks. Our attack methods are evaluated on several state-of-the-art diffusion models, over different datasets in relation to privacy-sensitive data. Extensive experimental evaluations show that our attacks can achieve remarkable performance. Furthermore, we exhaustively investigate various factors which can affect attack performance. Finally, we also evaluate the performance of our attack methods on diffusion models trained with differential privacy.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Hu, Hailong and Pang, Jun},
	month = jan,
	year = {2023},
	note = {arXiv:2301.09956},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, detection of participation, membership inference, potential application for IPinGenAI, protection},
}

@misc{ho_classifier-free_2022,
	title = {Classifier-{Free} {Diffusion} {Guidance}},
	url = {http://arxiv.org/abs/2207.12598},
	doi = {10.48550/arXiv.2207.12598},
	abstract = {Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.},
	urldate = {2024-02-22},
	publisher = {arXiv},
	author = {Ho, Jonathan and Salimans, Tim},
	month = jul,
	year = {2022},
	note = {arXiv:2207.12598},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{hayes_logan_2018,
	title = {{LOGAN}: {Membership} {Inference} {Attacks} {Against} {Generative} {Models}},
	shorttitle = {{LOGAN}},
	url = {http://arxiv.org/abs/1705.07663},
	doi = {10.48550/arXiv.1705.07663},
	abstract = {Generative models estimate the underlying distribution of a dataset to generate realistic samples according to that distribution. In this paper, we present the first membership inference attacks against generative models: given a data point, the adversary determines whether or not it was used to train the model. Our attacks leverage Generative Adversarial Networks (GANs), which combine a discriminative and a generative model, to detect overfitting and recognize inputs that were part of training datasets, using the discriminator's capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, against several state-of-the-art generative models, over datasets of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). We also discuss the sensitivity of the attacks to different training parameters, and their robustness against mitigation strategies, finding that defenses are either ineffective or lead to significantly worse performances of the generative models in terms of training stability and/or sample quality.},
	urldate = {2023-12-07},
	publisher = {arXiv},
	author = {Hayes, Jamie and Melis, Luca and Danezis, George and De Cristofaro, Emiliano},
	month = aug,
	year = {2018},
	note = {arXiv:1705.07663},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Privacy, detection of participation, membership inference, potential application for IPinGenAI},
}

@misc{hartmann_sok_2023,
	title = {{SoK}: {Memorization} in {General}-{Purpose} {Large} {Language} {Models}},
	shorttitle = {{SoK}},
	url = {http://arxiv.org/abs/2310.18362},
	abstract = {Large Language Models (LLMs) are advancing at a remarkable pace, with myriad applications under development. Unlike most earlier machine learning models, they are no longer built for one specific application but are designed to excel in a wide range of tasks. A major part of this success is due to their huge training datasets and the unprecedented number of model parameters, which allow them to memorize large amounts of information contained in the training data. This memorization goes beyond mere language, and encompasses information only present in a few documents. This is often desirable since it is necessary for performing tasks such as question answering, and therefore an important part of learning, but also brings a whole array of issues, from privacy and security to copyright and beyond. LLMs can memorize short secrets in the training data, but can also memorize concepts like facts or writing styles that can be expressed in text in many different ways. We propose a taxonomy for memorization in LLMs that covers verbatim text, facts, ideas and algorithms, writing styles, distributional properties, and alignment goals. We describe the implications of each type of memorization - both positive and negative - for model performance, privacy, security and confidentiality, copyright, and auditing, and ways to detect and prevent memorization. We further highlight the challenges that arise from the predominant way of defining memorization with respect to model behavior instead of model weights, due to LLM-specific phenomena such as reasoning capabilities or differences between decoding algorithms. Throughout the paper, we describe potential risks and opportunities arising from memorization in LLMs that we hope will motivate new research directions.},
	urldate = {2024-02-20},
	publisher = {arXiv},
	author = {Hartmann, Valentin and Suri, Anshuman and Bindschaedler, Vincent and Evans, David and Tople, Shruti and West, Robert},
	month = oct,
	year = {2023},
	note = {arXiv:2310.18362},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{harkonen_ganspace_2020,
	title = {{GANSpace}: {Discovering} {Interpretable} {GAN} {Controls}},
	shorttitle = {{GANSpace}},
	url = {http://arxiv.org/abs/2004.02546},
	abstract = {This paper describes a simple technique to analyze Generative Adversarial Networks (GANs) and create interpretable controls for image synthesis, such as change of viewpoint, aging, lighting, and time of day. We identify important latent directions based on Principal Components Analysis (PCA) applied either in latent space or feature space. Then, we show that a large number of interpretable controls can be defined by layer-wise perturbation along the principal directions. Moreover, we show that BigGAN can be controlled with layer-wise inputs in a StyleGAN-like manner. We show results on different GANs trained on various datasets, and demonstrate good qualitative matches to edit directions found through earlier supervised approaches.},
	urldate = {2024-02-15},
	publisher = {arXiv},
	author = {Härkönen, Erik and Hertzmann, Aaron and Lehtinen, Jaakko and Paris, Sylvain},
	month = dec,
	year = {2020},
	note = {arXiv:2004.02546},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{gudibande_false_2023,
	title = {The {False} {Promise} of {Imitating} {Proprietary} {LLMs}},
	url = {http://arxiv.org/abs/2305.15717},
	doi = {10.48550/arXiv.2305.15717},
	abstract = {An emerging method to cheaply improve a weaker language model is to finetune it on outputs from a stronger model, such as a proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply imitate the proprietary model's capabilities using a weaker open-source model. In this work, we critically analyze this approach. We first finetune a series of LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the models using crowd raters and canonical NLP benchmarks. Initially, we were surprised by the output quality of our imitation models -- they appear far better at following instructions, and crowd workers rate their outputs as competitive with ChatGPT. However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data. We show that these performance discrepancies may slip past human raters because imitation models are adept at mimicking ChatGPT's style but not its factuality. Overall, we conclude that model imitation is a false promise: there exists a substantial capabilities gap between open and closed LMs that, with current methods, can only be bridged using an unwieldy amount of imitation data or by using more capable base LMs. In turn, we argue that the highest leverage action for improving open-source models is to tackle the difficult challenge of developing better base LMs, rather than taking the shortcut of imitating proprietary systems.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Gudibande, Arnav and Wallace, Eric and Snell, Charlie and Geng, Xinyang and Liu, Hao and Abbeel, Pieter and Levine, Sergey and Song, Dawn},
	month = may,
	year = {2023},
	note = {arXiv:2305.15717},
	keywords = {Computer Science - Computation and Language},
}

@misc{gu_memorization_2023,
	title = {On {Memorization} in {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2310.02664},
	doi = {10.48550/arXiv.2310.02664},
	abstract = {Due to their capacity to generate novel and high-quality samples, diffusion models have attracted significant research interest in recent years. Notably, the typical training objective of diffusion models, i.e., denoising score matching, has a closed-form optimal solution that can only generate training data replicating samples. This indicates that a memorization behavior is theoretically expected, which contradicts the common generalization ability of state-of-the-art diffusion models, and thus calls for a deeper understanding. Looking into this, we first observe that memorization behaviors tend to occur on smaller-sized datasets, which motivates our definition of effective model memorization (EMM), a metric measuring the maximum size of training data at which a learned diffusion model approximates its theoretical optimum. Then, we quantify the impact of the influential factors on these memorization behaviors in terms of EMM, focusing primarily on data distribution, model configuration, and training procedure. Besides comprehensive empirical results identifying the influential factors, we surprisingly find that conditioning training data on uninformative random labels can significantly trigger the memorization in diffusion models. Our study holds practical significance for diffusion model users and offers clues to theoretical research in deep generative models. Code is available at https://github.com/sail-sg/DiffMemorize.},
	urldate = {2023-12-12},
	publisher = {arXiv},
	author = {Gu, Xiangming and Du, Chao and Pang, Tianyu and Li, Chongxuan and Lin, Min and Wang, Ye},
	month = oct,
	year = {2023},
	note = {arXiv:2310.02664},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{gozalo-brizuela_chatgpt_2023,
	title = {{ChatGPT} is not all you need. {A} {State} of the {Art} {Review} of large {Generative} {AI} models},
	url = {http://arxiv.org/abs/2301.04655},
	doi = {10.48550/arXiv.2301.04655},
	abstract = {During the last two years there has been a plethora of large generative models such as ChatGPT or Stable Diffusion that have been published. Concretely, these models are able to perform tasks such as being a general question and answering system or automatically creating artistic images that are revolutionizing several sectors. Consequently, the implications that these generative models have in the industry and society are enormous, as several job positions may be transformed. For example, Generative AI is capable of transforming effectively and creatively texts to images, like the DALLE-2 model; text to 3D images, like the Dreamfusion model; images to text, like the Flamingo model; texts to video, like the Phenaki model; texts to audio, like the AudioLM model; texts to other texts, like ChatGPT; texts to code, like the Codex model; texts to scientific texts, like the Galactica model or even create algorithms like AlphaTensor. This work consists on an attempt to describe in a concise way the main models are sectors that are affected by generative AI and to provide a taxonomy of the main generative models published recently.},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Gozalo-Brizuela, Roberto and Garrido-Merchan, Eduardo C.},
	month = jan,
	year = {2023},
	note = {arXiv:2301.04655},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, survey, taxonomy},
}

@misc{ghiasi_plug-inversion_2022,
	title = {Plug-{In} {Inversion}: {Model}-{Agnostic} {Inversion} for {Vision} with {Data} {Augmentations}},
	shorttitle = {Plug-{In} {Inversion}},
	url = {http://arxiv.org/abs/2201.12961},
	doi = {10.48550/arXiv.2201.12961},
	abstract = {Existing techniques for model inversion typically rely on hard-to-tune regularizers, such as total variation or feature regularization, which must be individually calibrated for each network in order to produce adequate images. In this work, we introduce Plug-In Inversion, which relies on a simple set of augmentations and does not require excessive hyper-parameter tuning. Under our proposed augmentation-based scheme, the same set of augmentation hyper-parameters can be used for inverting a wide range of image classification models, regardless of input dimensions or the architecture. We illustrate the practicality of our approach by inverting Vision Transformers (ViTs) and Multi-Layer Perceptrons (MLPs) trained on the ImageNet dataset, tasks which to the best of our knowledge have not been successfully accomplished by any previous works.},
	urldate = {2023-07-26},
	publisher = {arXiv},
	author = {Ghiasi, Amin and Kazemi, Hamid and Reich, Steven and Zhu, Chen and Goldblum, Micah and Goldstein, Tom},
	month = jan,
	year = {2022},
	note = {arXiv:2201.12961},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{gemini_team_gemini_2023,
	title = {Gemini: {A} {Family} of {Highly} {Capable} {Multimodal} {Models}},
	shorttitle = {Gemini},
	url = {http://arxiv.org/abs/2312.11805},
	abstract = {This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.},
	urldate = {2024-02-19},
	publisher = {arXiv},
	author = {Gemini Team},
	month = dec,
	year = {2023},
	note = {arXiv:2312.11805},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{gemini_team_long_2023,
	title = {[{Long} {Author} {Version}] {Gemini}: {A} {Family} of {Highly} {Capable} {Multimodal} {Models}},
	shorttitle = {Gemini},
	url = {http://arxiv.org/abs/2312.11805},
	abstract = {This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.},
	urldate = {2024-02-19},
	publisher = {arXiv},
	author = {Gemini Team and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M. and Hauth, Anja and Millican, Katie and Silver, David and Petrov, Slav and Johnson, Melvin and Antonoglou, Ioannis and Schrittwieser, Julian and Glaese, Amelia and Chen, Jilin and Pitler, Emily and Lillicrap, Timothy and Lazaridou, Angeliki and Firat, Orhan and Molloy, James and Isard, Michael and Barham, Paul R. and Hennigan, Tom and Lee, Benjamin and Viola, Fabio and Reynolds, Malcolm and Xu, Yuanzhong and Doherty, Ryan and Collins, Eli and Meyer, Clemens and Rutherford, Eliza and Moreira, Erica and Ayoub, Kareem and Goel, Megha and Tucker, George and Piqueras, Enrique and Krikun, Maxim and Barr, Iain and Savinov, Nikolay and Danihelka, Ivo and Roelofs, Becca and White, Anaïs and Andreassen, Anders and von Glehn, Tamara and Yagati, Lakshman and Kazemi, Mehran and Gonzalez, Lucas and Khalman, Misha and Sygnowski, Jakub and Frechette, Alexandre and Smith, Charlotte and Culp, Laura and Proleev, Lev and Luan, Yi and Chen, Xi and Lottes, James and Schucher, Nathan and Lebron, Federico and Rrustemi, Alban and Clay, Natalie and Crone, Phil and Kocisky, Tomas and Zhao, Jeffrey and Perz, Bartek and Yu, Dian and Howard, Heidi and Bloniarz, Adam and Rae, Jack W. and Lu, Han and Sifre, Laurent and Maggioni, Marcello and Alcober, Fred and Garrette, Dan and Barnes, Megan and Thakoor, Shantanu and Austin, Jacob and Barth-Maron, Gabriel and Wong, William and Joshi, Rishabh and Chaabouni, Rahma and Fatiha, Deeni and Ahuja, Arun and Liu, Ruibo and Li, Yunxuan and Cogan, Sarah and Chen, Jeremy and Jia, Chao and Gu, Chenjie and Zhang, Qiao and Grimstad, Jordan and Hartman, Ale Jakse and Chadwick, Martin and Tomar, Gaurav Singh and Garcia, Xavier and Senter, Evan and Taropa, Emanuel and Pillai, Thanumalayan Sankaranarayana and Devlin, Jacob and Laskin, Michael and Casas, Diego de Las and Valter, Dasha and Tao, Connie and Blanco, Lorenzo and Badia, Adrià Puigdomènech and Reitter, David and Chen, Mianna and Brennan, Jenny and Rivera, Clara and Brin, Sergey and Iqbal, Shariq and Surita, Gabriela and Labanowski, Jane and Rao, Abhi and Winkler, Stephanie and Parisotto, Emilio and Gu, Yiming and Olszewska, Kate and Zhang, Yujing and Addanki, Ravi and Miech, Antoine and Louis, Annie and Shafey, Laurent El and Teplyashin, Denis and Brown, Geoff and Catt, Elliot and Attaluri, Nithya and Balaguer, Jan and Xiang, Jackie and Wang, Pidong and Ashwood, Zoe and Briukhov, Anton and Webson, Albert and Ganapathy, Sanjay and Sanghavi, Smit and Kannan, Ajay and Chang, Ming-Wei and Stjerngren, Axel and Djolonga, Josip and Sun, Yuting and Bapna, Ankur and Aitchison, Matthew and Pejman, Pedram and Michalewski, Henryk and Yu, Tianhe and Wang, Cindy and Love, Juliette and Ahn, Junwhan and Bloxwich, Dawn and Han, Kehang and Humphreys, Peter and Sellam, Thibault and Bradbury, James and Godbole, Varun and Samangooei, Sina and Damoc, Bogdan and Kaskasoli, Alex and Arnold, Sébastien M. R. and Vasudevan, Vijay and Agrawal, Shubham and Riesa, Jason and Lepikhin, Dmitry and Tanburn, Richard and Srinivasan, Srivatsan and Lim, Hyeontaek and Hodkinson, Sarah and Shyam, Pranav and Ferret, Johan and Hand, Steven and Garg, Ankush and Paine, Tom Le and Li, Jian and Li, Yujia and Giang, Minh and Neitz, Alexander and Abbas, Zaheer and York, Sarah and Reid, Machel and Cole, Elizabeth and Chowdhery, Aakanksha and Das, Dipanjan and Rogozińska, Dominika and Nikolaev, Vitaly and Sprechmann, Pablo and Nado, Zachary and Zilka, Lukas and Prost, Flavien and He, Luheng and Monteiro, Marianne and Mishra, Gaurav and Welty, Chris and Newlan, Josh and Jia, Dawei and Allamanis, Miltiadis and Hu, Clara Huiyi and de Liedekerke, Raoul and Gilmer, Justin and Saroufim, Carl and Rijhwani, Shruti and Hou, Shaobo and Shrivastava, Disha and Baddepudi, Anirudh and Goldin, Alex and Ozturel, Adnan and Cassirer, Albin and Xu, Yunhan and Sohn, Daniel and Sachan, Devendra and Amplayo, Reinald Kim and Swanson, Craig and Petrova, Dessie and Narayan, Shashi and Guez, Arthur and Brahma, Siddhartha and Landon, Jessica and Patel, Miteyan and Zhao, Ruizhe and Villela, Kevin and Wang, Luyu and Jia, Wenhao and Rahtz, Matthew and Giménez, Mai and Yeung, Legg and Lin, Hanzhao and Keeling, James and Georgiev, Petko and Mincu, Diana and Wu, Boxi and Haykal, Salem and Saputro, Rachel and Vodrahalli, Kiran and Qin, James and Cankara, Zeynep and Sharma, Abhanshu and Fernando, Nick and Hawkins, Will and Neyshabur, Behnam and Kim, Solomon and Hutter, Adrian and Agrawal, Priyanka and Castro-Ros, Alex and Driessche, George van den and Wang, Tao and Yang, Fan and Chang, Shuo-yiin and Komarek, Paul and McIlroy, Ross and Lučić, Mario and Zhang, Guodong and Farhan, Wael and Sharman, Michael and Natsev, Paul and Michel, Paul and Cheng, Yong and Bansal, Yamini and Qiao, Siyuan and Cao, Kris and Shakeri, Siamak and Butterfield, Christina and Chung, Justin and Rubenstein, Paul Kishan and Agrawal, Shivani and Mensch, Arthur and Soparkar, Kedar and Lenc, Karel and Chung, Timothy and Pope, Aedan and Maggiore, Loren and Kay, Jackie and Jhakra, Priya and Wang, Shibo and Maynez, Joshua and Phuong, Mary and Tobin, Taylor and Tacchetti, Andrea and Trebacz, Maja and Robinson, Kevin and Katariya, Yash and Riedel, Sebastian and Bailey, Paige and Xiao, Kefan and Ghelani, Nimesh and Aroyo, Lora and Slone, Ambrose and Houlsby, Neil and Xiong, Xuehan and Yang, Zhen and Gribovskaya, Elena and Adler, Jonas and Wirth, Mateo and Lee, Lisa and Li, Music and Kagohara, Thais and Pavagadhi, Jay and Bridgers, Sophie and Bortsova, Anna and Ghemawat, Sanjay and Ahmed, Zafarali and Liu, Tianqi and Powell, Richard and Bolina, Vijay and Iinuma, Mariko and Zablotskaia, Polina and Besley, James and Chung, Da-Woon and Dozat, Timothy and Comanescu, Ramona and Si, Xiance and Greer, Jeremy and Su, Guolong and Polacek, Martin and Kaufman, Raphaël Lopez and Tokumine, Simon and Hu, Hexiang and Buchatskaya, Elena and Miao, Yingjie and Elhawaty, Mohamed and Siddhant, Aditya and Tomasev, Nenad and Xing, Jinwei and Greer, Christina and Miller, Helen and Ashraf, Shereen and Roy, Aurko and Zhang, Zizhao and Ma, Ada and Filos, Angelos and Besta, Milos and Blevins, Rory and Klimenko, Ted and Yeh, Chih-Kuan and Changpinyo, Soravit and Mu, Jiaqi and Chang, Oscar and Pajarskas, Mantas and Muir, Carrie and Cohen, Vered and Lan, Charline Le and Haridasan, Krishna and Marathe, Amit and Hansen, Steven and Douglas, Sholto and Samuel, Rajkumar and Wang, Mingqiu and Austin, Sophia and Lan, Chang and Jiang, Jiepu and Chiu, Justin and Lorenzo, Jaime Alonso and Sjösund, Lars Lowe and Cevey, Sébastien and Gleicher, Zach and Avrahami, Thi and Boral, Anudhyan and Srinivasan, Hansa and Selo, Vittorio and May, Rhys and Aisopos, Konstantinos and Hussenot, Léonard and Soares, Livio Baldini and Baumli, Kate and Chang, Michael B. and Recasens, Adrià and Caine, Ben and Pritzel, Alexander and Pavetic, Filip and Pardo, Fabio and Gergely, Anita and Frye, Justin and Ramasesh, Vinay and Horgan, Dan and Badola, Kartikeya and Kassner, Nora and Roy, Subhrajit and Dyer, Ethan and Campos, Víctor and Tomala, Alex and Tang, Yunhao and Badawy, Dalia El and White, Elspeth and Mustafa, Basil and Lang, Oran and Jindal, Abhishek and Vikram, Sharad and Gong, Zhitao and Caelles, Sergi and Hemsley, Ross and Thornton, Gregory and Feng, Fangxiaoyu and Stokowiec, Wojciech and Zheng, Ce and Thacker, Phoebe and Ünlü, Çağlar and Zhang, Zhishuai and Saleh, Mohammad and Svensson, James and Bileschi, Max and Patil, Piyush and Anand, Ankesh and Ring, Roman and Tsihlas, Katerina and Vezer, Arpi and Selvi, Marco and Shevlane, Toby and Rodriguez, Mikel and Kwiatkowski, Tom and Daruki, Samira and Rong, Keran and Dafoe, Allan and FitzGerald, Nicholas and Gu-Lemberg, Keren and Khan, Mina and Hendricks, Lisa Anne and Pellat, Marie and Feinberg, Vladimir and Cobon-Kerr, James and Sainath, Tara and Rauh, Maribeth and Hashemi, Sayed Hadi and Ives, Richard and Hasson, Yana and Li, YaGuang and Noland, Eric and Cao, Yuan and Byrd, Nathan and Hou, Le and Wang, Qingze and Sottiaux, Thibault and Paganini, Michela and Lespiau, Jean-Baptiste and Moufarek, Alexandre and Hassan, Samer and Shivakumar, Kaushik and van Amersfoort, Joost and Mandhane, Amol and Joshi, Pratik and Goyal, Anirudh and Tung, Matthew and Brock, Andrew and Sheahan, Hannah and Misra, Vedant and Li, Cheng and Rakićević, Nemanja and Dehghani, Mostafa and Liu, Fangyu and Mittal, Sid and Oh, Junhyuk and Noury, Seb and Sezener, Eren and Huot, Fantine and Lamm, Matthew and De Cao, Nicola and Chen, Charlie and Elsayed, Gamaleldin and Chi, Ed and Mahdieh, Mahdis and Tenney, Ian and Hua, Nan and Petrychenko, Ivan and Kane, Patrick and Scandinaro, Dylan and Jain, Rishub and Uesato, Jonathan and Datta, Romina and Sadovsky, Adam and Bunyan, Oskar and Rabiej, Dominik and Wu, Shimu and Zhang, John and Vasudevan, Gautam and Leurent, Edouard and Alnahlawi, Mahmoud and Georgescu, Ionut and Wei, Nan and Zheng, Ivy and Chan, Betty and Rabinovitch, Pam G. and Stanczyk, Piotr and Zhang, Ye and Steiner, David and Naskar, Subhajit and Azzam, Michael and Johnson, Matthew and Paszke, Adam and Chiu, Chung-Cheng and Elias, Jaume Sanchez and Mohiuddin, Afroz and Muhammad, Faizan and Miao, Jin and Lee, Andrew and Vieillard, Nino and Potluri, Sahitya and Park, Jane and Davoodi, Elnaz and Zhang, Jiageng and Stanway, Jeff and Garmon, Drew and Karmarkar, Abhijit and Dong, Zhe and Lee, Jong and Kumar, Aviral and Zhou, Luowei and Evens, Jonathan and Isaac, William and Chen, Zhe and Jia, Johnson and Levskaya, Anselm and Zhu, Zhenkai and Gorgolewski, Chris and Grabowski, Peter and Mao, Yu and Magni, Alberto and Yao, Kaisheng and Snaider, Javier and Casagrande, Norman and Suganthan, Paul and Palmer, Evan and Irving, Geoffrey and Loper, Edward and Faruqui, Manaal and Arkatkar, Isha and Chen, Nanxin and Shafran, Izhak and Fink, Michael and Castaño, Alfonso and Giannoumis, Irene and Kim, Wooyeol and Rybiński, Mikołaj and Sreevatsa, Ashwin and Prendki, Jennifer and Soergel, David and Goedeckemeyer, Adrian and Gierke, Willi and Jafari, Mohsen and Gaba, Meenu and Wiesner, Jeremy and Wright, Diana Gage and Wei, Yawen and Vashisht, Harsha and Kulizhskaya, Yana and Hoover, Jay and Le, Maigo and Li, Lu and Iwuanyanwu, Chimezie and Liu, Lu and Ramirez, Kevin and Khorlin, Andrey and Cui, Albert and LIN, Tian and Georgiev, Marin and Wu, Marcus and Aguilar, Ricardo and Pallo, Keith and Chakladar, Abhishek and Repina, Alena and Wu, Xihui and van der Weide, Tom and Ponnapalli, Priya and Kaplan, Caroline and Simsa, Jiri and Li, Shuangfeng and Dousse, Olivier and Yang, Fan and Piper, Jeff and Ie, Nathan and Lui, Minnie and Pasumarthi, Rama and Lintz, Nathan and Vijayakumar, Anitha and Thiet, Lam Nguyen and Andor, Daniel and Valenzuela, Pedro and Paduraru, Cosmin and Peng, Daiyi and Lee, Katherine and Zhang, Shuyuan and Greene, Somer and Nguyen, Duc Dung and Kurylowicz, Paula and Velury, Sarmishta and Krause, Sebastian and Hardin, Cassidy and Dixon, Lucas and Janzer, Lili and Choo, Kiam and Feng, Ziqiang and Zhang, Biao and Singhal, Achintya and Latkar, Tejasi and Zhang, Mingyang and Le, Quoc and Abellan, Elena Allica and Du, Dayou and McKinnon, Dan and Antropova, Natasha and Bolukbasi, Tolga and Keller, Orgad and Reid, David and Finchelstein, Daniel and Raad, Maria Abi and Crocker, Remi and Hawkins, Peter and Dadashi, Robert and Gaffney, Colin and Lall, Sid and Franko, Ken and Filonov, Egor and Bulanova, Anna and Leblond, Rémi and Yadav, Vikas and Chung, Shirley and Askham, Harry and Cobo, Luis C. and Xu, Kelvin and Fischer, Felix and Xu, Jun and Sorokin, Christina and Alberti, Chris and Lin, Chu-Cheng and Evans, Colin and Zhou, Hao and Dimitriev, Alek and Forbes, Hannah and Banarse, Dylan and Tung, Zora and Liu, Jeremiah and Omernick, Mark and Bishop, Colton and Kumar, Chintu and Sterneck, Rachel and Foley, Ryan and Jain, Rohan and Mishra, Swaroop and Xia, Jiawei and Bos, Taylor and Cideron, Geoffrey and Amid, Ehsan and Piccinno, Francesco and Wang, Xingyu and Banzal, Praseem and Gurita, Petru and Noga, Hila and Shah, Premal and Mankowitz, Daniel J. and Polozov, Alex and Kushman, Nate and Krakovna, Victoria and Brown, Sasha and Bateni, MohammadHossein and Duan, Dennis and Firoiu, Vlad and Thotakuri, Meghana and Natan, Tom and Mohananey, Anhad and Geist, Matthieu and Mudgal, Sidharth and Girgin, Sertan and Li, Hui and Ye, Jiayu and Roval, Ofir and Tojo, Reiko and Kwong, Michael and Lee-Thorp, James and Yew, Christopher and Yuan, Quan and Bagri, Sumit and Sinopalnikov, Danila and Ramos, Sabela and Mellor, John and Sharma, Abhishek and Severyn, Aliaksei and Lai, Jonathan and Wu, Kathy and Cheng, Heng-Tze and Miller, David and Sonnerat, Nicolas and Vnukov, Denis and Greig, Rory and Beattie, Jennifer and Caveness, Emily and Bai, Libin and Eisenschlos, Julian and Korchemniy, Alex and Tsai, Tomy and Jasarevic, Mimi and Kong, Weize and Dao, Phuong and Zheng, Zeyu and Liu, Frederick and Yang, Fan and Zhu, Rui and Geller, Mark and Teh, Tian Huey and Sanmiya, Jason and Gladchenko, Evgeny and Trdin, Nejc and Sozanschi, Andrei and Toyama, Daniel and Rosen, Evan and Tavakkol, Sasan and Xue, Linting and Elkind, Chen and Woodman, Oliver and Carpenter, John and Papamakarios, George and Kemp, Rupert and Kafle, Sushant and Grunina, Tanya and Sinha, Rishika and Talbert, Alice and Goyal, Abhimanyu and Wu, Diane and Owusu-Afriyie, Denese and Du, Cosmo and Thornton, Chloe and Pont-Tuset, Jordi and Narayana, Pradyumna and Li, Jing and Fatehi, Sabaer and Wieting, John and Ajmeri, Omar and Uria, Benigno and Zhu, Tao and Ko, Yeongil and Knight, Laura and Héliou, Amélie and Niu, Ning and Gu, Shane and Pang, Chenxi and Tran, Dustin and Li, Yeqing and Levine, Nir and Stolovich, Ariel and Kalb, Norbert and Santamaria-Fernandez, Rebeca and Goenka, Sonam and Yustalim, Wenny and Strudel, Robin and Elqursh, Ali and Lakshminarayanan, Balaji and Deck, Charlie and Upadhyay, Shyam and Lee, Hyo and Dusenberry, Mike and Li, Zonglin and Wang, Xuezhi and Levin, Kyle and Hoffmann, Raphael and Holtmann-Rice, Dan and Bachem, Olivier and Yue, Summer and Arora, Sho and Malmi, Eric and Mirylenka, Daniil and Tan, Qijun and Koh, Christy and Yeganeh, Soheil Hassas and Põder, Siim and Zheng, Steven and Pongetti, Francesco and Tariq, Mukarram and Sun, Yanhua and Ionita, Lucian and Seyedhosseini, Mojtaba and Tafti, Pouya and Kotikalapudi, Ragha and Liu, Zhiyu and Gulati, Anmol and Liu, Jasmine and Ye, Xinyu and Chrzaszcz, Bart and Wang, Lily and Sethi, Nikhil and Li, Tianrun and Brown, Ben and Singh, Shreya and Fan, Wei and Parisi, Aaron and Stanton, Joe and Kuang, Chenkai and Koverkathu, Vinod and Choquette-Choo, Christopher A. and Li, Yunjie and Lu, T. J. and Ittycheriah, Abe and Shroff, Prakash and Sun, Pei and Varadarajan, Mani and Bahargam, Sanaz and Willoughby, Rob and Gaddy, David and Dasgupta, Ishita and Desjardins, Guillaume and Cornero, Marco and Robenek, Brona and Mittal, Bhavishya and Albrecht, Ben and Shenoy, Ashish and Moiseev, Fedor and Jacobsson, Henrik and Ghaffarkhah, Alireza and Rivière, Morgane and Walton, Alanna and Crepy, Clément and Parrish, Alicia and Liu, Yuan and Zhou, Zongwei and Farabet, Clement and Radebaugh, Carey and Srinivasan, Praveen and van der Salm, Claudia and Fidjeland, Andreas and Scellato, Salvatore and Latorre-Chimoto, Eri and Klimczak-Plucińska, Hanna and Bridson, David and de Cesare, Dario and Hudson, Tom and Mendolicchio, Piermaria and Walker, Lexi and Morris, Alex and Penchev, Ivo and Mauger, Matthew and Guseynov, Alexey and Reid, Alison and Odoom, Seth and Loher, Lucia and Cotruta, Victor and Yenugula, Madhavi and Grewe, Dominik and Petrushkina, Anastasia and Duerig, Tom and Sanchez, Antonio and Yadlowsky, Steve and Shen, Amy and Globerson, Amir and Kurzrok, Adam and Webb, Lynette and Dua, Sahil and Li, Dong and Lahoti, Preethi and Bhupatiraju, Surya and Hurt, Dan and Qureshi, Haroon and Agarwal, Ananth and Shani, Tomer and Eyal, Matan and Khare, Anuj and Belle, Shreyas Rammohan and Wang, Lei and Tekur, Chetan and Kale, Mihir Sanjay and Wei, Jinliang and Sang, Ruoxin and Saeta, Brennan and Liechty, Tyler and Sun, Yi and Zhao, Yao and Lee, Stephan and Nayak, Pandu and Fritz, Doug and Vuyyuru, Manish Reddy and Aslanides, John and Vyas, Nidhi and Wicke, Martin and Ma, Xiao and Bilal, Taylan and Eltyshev, Evgenii and Balle, Daniel and Martin, Nina and Cate, Hardie and Manyika, James and Amiri, Keyvan and Kim, Yelin and Xiong, Xi and Kang, Kai and Luisier, Florian and Tripuraneni, Nilesh and Madras, David and Guo, Mandy and Waters, Austin and Wang, Oliver and Ainslie, Joshua and Baldridge, Jason and Zhang, Han and Pruthi, Garima and Bauer, Jakob and Yang, Feng and Mansour, Riham and Gelman, Jason and Xu, Yang and Polovets, George and Liu, Ji and Cai, Honglong and Chen, Warren and Sheng, XiangHai and Xue, Emily and Ozair, Sherjil and Yu, Adams and Angermueller, Christof and Li, Xiaowei and Wang, Weiren and Wiesinger, Julia and Koukoumidis, Emmanouil and Tian, Yuan and Iyer, Anand and Gurumurthy, Madhu and Goldenson, Mark and Shah, Parashar and Blake, M. K. and Yu, Hongkun and Urbanowicz, Anthony and Palomaki, Jennimaria and Fernando, Chrisantha and Brooks, Kevin and Durden, Ken and Mehta, Harsh and Momchev, Nikola and Rahimtoroghi, Elahe and Georgaki, Maria and Raul, Amit and Ruder, Sebastian and Redshaw, Morgan and Lee, Jinhyuk and Jalan, Komal and Li, Dinghua and Perng, Ginger and Hechtman, Blake and Schuh, Parker and Nasr, Milad and Chen, Mia and Milan, Kieran and Mikulik, Vladimir and Strohman, Trevor and Franco, Juliana and Green, Tim and Hassabis, Demis and Kavukcuoglu, Koray and Dean, Jeffrey and Vinyals, Oriol},
	month = dec,
	year = {2023},
	note = {arXiv:2312.11805},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{gandikota_concept_2023,
	title = {Concept {Sliders}: {LoRA} {Adaptors} for {Precise} {Control} in {Diffusion} {Models}},
	shorttitle = {Concept {Sliders}},
	url = {http://arxiv.org/abs/2311.12092},
	doi = {10.48550/arXiv.2311.12092},
	abstract = {We present a method to create interpretable concept sliders that enable precise control over attributes in image generations from diffusion models. Our approach identifies a low-rank parameter direction corresponding to one concept while minimizing interference with other attributes. A slider is created using a small set of prompts or sample images; thus slider directions can be created for either textual or visual concepts. Concept Sliders are plug-and-play: they can be composed efficiently and continuously modulated, enabling precise control over image generation. In quantitative experiments comparing to previous editing techniques, our sliders exhibit stronger targeted edits with lower interference. We showcase sliders for weather, age, styles, and expressions, as well as slider compositions. We show how sliders can transfer latents from StyleGAN for intuitive editing of visual concepts for which textual description is difficult. We also find that our method can help address persistent quality issues in Stable Diffusion XL including repair of object deformations and fixing distorted hands. Our code, data, and trained sliders are available at https://sliders.baulab.info/},
	urldate = {2024-01-25},
	publisher = {arXiv},
	author = {Gandikota, Rohit and Materzynska, Joanna and Zhou, Tingrui and Torralba, Antonio and Bau, David},
	month = nov,
	year = {2023},
	note = {arXiv:2311.12092},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{gandikota_erasing_2023,
	title = {Erasing {Concepts} from {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2303.07345},
	doi = {10.48550/arXiv.2303.07345},
	abstract = {Motivated by recent advancements in text-to-image diffusion, we study erasure of specific concepts from the model's weights. While Stable Diffusion has shown promise in producing explicit or realistic artwork, it has raised concerns regarding its potential for misuse. We propose a fine-tuning method that can erase a visual concept from a pre-trained diffusion model, given only the name of the style and using negative guidance as a teacher. We benchmark our method against previous approaches that remove sexually explicit content and demonstrate its effectiveness, performing on par with Safe Latent Diffusion and censored training. To evaluate artistic style removal, we conduct experiments erasing five modern artists from the network and conduct a user study to assess the human perception of the removed styles. Unlike previous methods, our approach can remove concepts from a diffusion model permanently rather than modifying the output at the inference time, so it cannot be circumvented even if a user has access to model weights. Our code, data, and results are available at https://erasing.baulab.info/},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Gandikota, Rohit and Materzynska, Joanna and Fiotto-Kaufman, Jaden and Bau, David},
	month = jun,
	year = {2023},
	note = {arXiv:2303.07345},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Training data copyright, concept removal, diffusion, protection},
}

@misc{akbar_beware_2023,
	title = {Beware of diffusion models for synthesizing medical images -- {A} comparison with {GANs} in terms of memorizing brain {MRI} and chest x-ray images},
	url = {http://arxiv.org/abs/2305.07644},
	doi = {10.48550/arXiv.2305.07644},
	abstract = {Diffusion models were initially developed for text-to-image generation and are now being utilized to generate high-quality synthetic images. Preceded by GANs, diffusion models have shown impressive results using various evaluation metrics. However, commonly used metrics such as FID and IS are not suitable for determining whether diffusion models are simply reproducing the training images. Here we train StyleGAN and diffusion models, using BRATS20, BRATS21 and a chest x-ray pneumonia dataset, to synthesize brain MRI and chest x-ray images, and measure the correlation between the synthe4c images and all training images. Our results show that diffusion models are more likely to memorize the training images, compared to StyleGAN, especially for small datasets and when using 2D slices from 3D volumes. Researchers should be careful when using diffusion models for medical imaging, if the final goal is to share the synthe4c images},
	urldate = {2024-02-20},
	publisher = {arXiv},
	author = {Akbar, Muhammad Usman and Wang, Wuhao and Eklund, Anders},
	month = oct,
	year = {2023},
	note = {arXiv:2305.07644},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{gal_image_2022,
	title = {An {Image} is {Worth} {One} {Word}: {Personalizing} {Text}-to-{Image} {Generation} using {Textual} {Inversion}},
	shorttitle = {An {Image} is {Worth} {One} {Word}},
	url = {http://arxiv.org/abs/2208.01618},
	doi = {10.48550/arXiv.2208.01618},
	abstract = {Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new "words" in the embedding space of a frozen text-to-image model. These "words" can be composed into natural language sentences, guiding personalized creation in an intuitive way. Notably, we find evidence that a single word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks. Our code, data and new words will be available at: https://textual-inversion.github.io},
	urldate = {2024-03-13},
	publisher = {arXiv},
	author = {Gal, Rinon and Alaluf, Yuval and Atzmon, Yuval and Patashnik, Or and Bermano, Amit H. and Chechik, Gal and Cohen-Or, Daniel},
	month = aug,
	year = {2022},
	note = {arXiv:2208.01618},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning, fine-tuning, paper, violation},
}

@misc{cui_diffusionshield_2023,
	title = {{DiffusionShield}: {A} {Watermark} for {Copyright} {Protection} against {Generative} {Diffusion} {Models}},
	shorttitle = {{DiffusionShield}},
	url = {http://arxiv.org/abs/2306.04642},
	doi = {10.48550/arXiv.2306.04642},
	abstract = {Recently, Generative Diffusion Models (GDMs) have showcased their remarkable capabilities in learning and generating images. A large community of GDMs has naturally emerged, further promoting the diversified applications of GDMs in various fields. However, this unrestricted proliferation has raised serious concerns about copyright protection. For example, artists including painters and photographers are becoming increasingly concerned that GDMs could effortlessly replicate their unique creative works without authorization. In response to these challenges, we introduce a novel watermarking scheme, DiffusionShield, tailored for GDMs. DiffusionShield protects images from copyright infringement by GDMs through encoding the ownership information into an imperceptible watermark and injecting it into the images. Its watermark can be easily learned by GDMs and will be reproduced in their generated images. By detecting the watermark from generated images, copyright infringement can be exposed with evidence. Benefiting from the uniformity of the watermarks and the joint optimization method, DiffusionShield ensures low distortion of the original image, high watermark detection performance, and the ability to embed lengthy messages. We conduct rigorous and comprehensive experiments to show the effectiveness of DiffusionShield in defending against infringement by GDMs and its superiority over traditional watermarking methods.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Cui, Yingqian and Ren, Jie and Xu, Han and He, Pengfei and Liu, Hui and Sun, Lichao and Xing, Yue and Tang, Jiliang},
	month = oct,
	year = {2023},
	note = {arXiv:2306.04642},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Training data copyright, diffusion, protection, training data ownership, watermark, watermarking},
}

@misc{chou_villandiffusion_2023,
	title = {{VillanDiffusion}: {A} {Unified} {Backdoor} {Attack} {Framework} for {Diffusion} {Models}},
	shorttitle = {{VillanDiffusion}},
	url = {http://arxiv.org/abs/2306.06874},
	doi = {10.48550/arXiv.2306.06874},
	abstract = {Diffusion Models (DMs) are state-of-the-art generative models that learn a reversible corruption process from iterative noise addition and denoising. They are the backbone of many generative AI applications, such as text-to-image conditional generation. However, recent studies have shown that basic unconditional DMs (e.g., DDPM and DDIM) are vulnerable to backdoor injection, a type of output manipulation attack triggered by a maliciously embedded pattern at model input. This paper presents a unified backdoor attack framework (VillanDiffusion) to expand the current scope of backdoor analysis for DMs. Our framework covers mainstream unconditional and conditional DMs (denoising-based and score-based) and various training-free samplers for holistic evaluations. Experiments show that our unified framework facilitates the backdoor analysis of different DM configurations and provides new insights into caption-based backdoor attacks on DMs. Our code is available on GitHub: {\textbackslash}url\{https://github.com/IBM/villandiffusion\}},
	urldate = {2023-12-05},
	publisher = {arXiv},
	author = {Chou, Sheng-Yen and Chen, Pin-Yu and Ho, Tsung-Yi},
	month = nov,
	year = {2023},
	note = {arXiv:2306.06874},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, backdoor attacks, diffusion, potential application for IPinGenAI},
}

@misc{fernandez_stable_2023,
	title = {The {Stable} {Signature}: {Rooting} {Watermarks} in {Latent} {Diffusion} {Models}},
	shorttitle = {The {Stable} {Signature}},
	url = {http://arxiv.org/abs/2303.15435},
	doi = {10.48550/arXiv.2303.15435},
	abstract = {Generative image modeling enables a wide range of applications but raises ethical concerns about responsible deployment. This paper introduces an active strategy combining image watermarking and Latent Diffusion Models. The goal is for all generated images to conceal an invisible watermark allowing for future detection and/or identification. The method quickly fine-tunes the latent decoder of the image generator, conditioned on a binary signature. A pre-trained watermark extractor recovers the hidden signature from any generated image and a statistical test then determines whether it comes from the generative model. We evaluate the invisibility and robustness of the watermarks on a variety of generation tasks, showing that Stable Signature works even after the images are modified. For instance, it detects the origin of an image generated from a text prompt, then cropped to keep \$10{\textbackslash}\%\$ of the content, with \$90\$+\${\textbackslash}\%\$ accuracy at a false positive rate below 10\${\textasciicircum}\{-6\}\$.},
	urldate = {2024-02-16},
	publisher = {arXiv},
	author = {Fernandez, Pierre and Couairon, Guillaume and Jégou, Hervé and Douze, Matthijs and Furon, Teddy},
	month = jul,
	year = {2023},
	note = {arXiv:2303.15435},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, detection of generated content, output, watermarking},
}

@misc{feng_catch_2023,
	title = {Catch {You} {Everything} {Everywhere}: {Guarding} {Textual} {Inversion} via {Concept} {Watermarking}},
	shorttitle = {Catch {You} {Everything} {Everywhere}},
	url = {http://arxiv.org/abs/2309.05940},
	doi = {10.48550/arXiv.2309.05940},
	abstract = {AIGC (AI-Generated Content) has achieved tremendous success in many applications such as text-to-image tasks, where the model can generate high-quality images with diverse prompts, namely, different descriptions in natural languages. More surprisingly, the emerging personalization techniques even succeed in describing unseen concepts with only a few personal images as references, and there have been some commercial platforms for sharing the valuable personalized concept. However, such an advanced technique also introduces a severe threat, where malicious users can misuse the target concept to generate highly-realistic illegal images. Therefore, it becomes necessary for the platform to trace malicious users and hold them accountable. In this paper, we focus on guarding the most popular lightweight personalization model, ie, Textual Inversion (TI). To achieve it, we propose the novel concept watermarking, where watermark information is embedded into the target concept and then extracted from generated images based on the watermarked concept. Specifically, we jointly train a watermark encoder and a watermark decoder with the sampler in the loop. It shows great resilience to different diffusion sampling processes possibly chosen by malicious users, meanwhile preserving utility for normal use. In practice, the concept owner can upload his concept with different watermarks (ie, serial numbers) to the platform, and the platform allocates different users with different serial numbers for subsequent tracing and forensics.},
	urldate = {2024-02-16},
	publisher = {arXiv},
	author = {Feng, Weitao and He, Jiyan and Zhang, Jie and Zhang, Tianwei and Zhou, Wenbo and Zhang, Weiming and Yu, Nenghai},
	month = sep,
	year = {2023},
	note = {arXiv:2309.05940},
	keywords = {Computer Science - Cryptography and Security, training data ownership, watermarking},
}

@misc{eldan_whos_2023,
	title = {Who's {Harry} {Potter}? {Approximate} {Unlearning} in {LLMs}},
	shorttitle = {Who's {Harry} {Potter}?},
	url = {http://arxiv.org/abs/2310.02238},
	doi = {10.48550/arXiv.2310.02238},
	abstract = {Large language models (LLMs) are trained on massive internet corpora that often contain copyrighted content. This poses legal and ethical challenges for the developers and users of these models, as well as the original authors and publishers. In this paper, we propose a novel technique for unlearning a subset of the training data from a LLM, without having to retrain it from scratch. We evaluate our technique on the task of unlearning the Harry Potter books from the Llama2-7b model (a generative language model recently open-sourced by Meta). While the model took over 184K GPU-hours to pretrain, we show that in about 1 GPU hour of finetuning, we effectively erase the model's ability to generate or recall Harry Potter-related content, while its performance on common benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remains almost unaffected. We make our fine-tuned model publicly available on HuggingFace for community evaluation. To the best of our knowledge, this is the first paper to present an effective technique for unlearning in generative language models. Our technique consists of three main components: First, we use a reinforced model that is further trained on the target data to identify the tokens that are most related to the unlearning target, by comparing its logits with those of a baseline model. Second, we replace idiosyncratic expressions in the target data with generic counterparts, and leverage the model's own predictions to generate alternative labels for every token. These labels aim to approximate the next-token predictions of a model that has not been trained on the target data. Third, we finetune the model on these alternative labels, which effectively erases the original text from the model's memory whenever it is prompted with its context.},
	urldate = {2024-02-21},
	publisher = {arXiv},
	author = {Eldan, Ronen and Russinovich, Mark},
	month = oct,
	year = {2023},
	note = {arXiv:2310.02238},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, concept removal, machine unlearning, protection},
}

@misc{dzuong_uncertain_2024,
	title = {Uncertain {Boundaries}: {Multidisciplinary} {Approaches} to {Copyright} {Issues} in {Generative} {AI}},
	shorttitle = {Uncertain {Boundaries}},
	url = {http://arxiv.org/abs/2404.08221},
	doi = {10.48550/arXiv.2404.08221},
	abstract = {In the rapidly evolving landscape of generative artificial intelligence (AI), the increasingly pertinent issue of copyright infringement arises as AI advances to generate content from scraped copyrighted data, prompting questions about ownership and protection that impact professionals across various careers. With this in mind, this survey provides an extensive examination of copyright infringement as it pertains to generative AI, aiming to stay abreast of the latest developments and open problems. Specifically, it will first outline methods of detecting copyright infringement in mediums such as text, image, and video. Next, it will delve an exploration of existing techniques aimed at safeguarding copyrighted works from generative models. Furthermore, this survey will discuss resources and tools for users to evaluate copyright violations. Finally, insights into ongoing regulations and proposals for AI will be explored and compared. Through combining these disciplines, the implications of AI-driven content and copyright are thoroughly illustrated and brought into question.},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Dzuong, Jocelyn and Wang, Zichong and Zhang, Wenbin},
	month = mar,
	year = {2024},
	note = {arXiv:2404.08221},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning, survey},
}

@misc{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2024-02-19},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, feature space},
}

@misc{dinzinger_survey_2024,
	title = {A {Survey} of {Web} {Content} {Control} for {Generative} {AI}},
	url = {http://arxiv.org/abs/2404.02309},
	doi = {10.48550/arXiv.2404.02309},
	abstract = {The groundbreaking advancements around generative AI have recently caused a wave of concern culminating in a row of lawsuits, including high-profile actions against Stability AI and OpenAI. This situation of legal uncertainty has sparked a broad discussion on the rights of content creators and publishers to protect their intellectual property on the web. European as well as US law already provides rough guidelines, setting a direction for technical solutions to regulate web data use. In this course, researchers and practitioners have worked on numerous web standards and opt-out formats that empower publishers to keep their data out of the development of generative AI models. The emerging AI/ML opt-out protocols are valuable in regards to data sovereignty, but again, it creates an adverse situation for a site owners who are overwhelmed by the multitude of recent ad hoc standards to consider. In our work, we want to survey the different proposals, ideas and initiatives, and provide a comprehensive legal and technical background in the context of the current discussion on web publishers control.},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Dinzinger, Michael and Heß, Florian and Granitzer, Michael},
	month = apr,
	year = {2024},
	note = {arXiv:2404.02309},
	keywords = {Computer Science - Information Retrieval, survey},
}

@misc{cui_ft-shield_2023,
	title = {{FT}-{Shield}: {A} {Watermark} {Against} {Unauthorized} {Fine}-tuning in {Text}-to-{Image} {Diffusion} {Models}},
	shorttitle = {{FT}-{Shield}},
	url = {http://arxiv.org/abs/2310.02401},
	doi = {10.48550/arXiv.2310.02401},
	abstract = {Text-to-image generative models based on latent diffusion models (LDM) have demonstrated their outstanding ability in generating high-quality and high-resolution images according to language prompt. Based on these powerful latent diffusion models, various fine-tuning methods have been proposed to achieve the personalization of text-to-image diffusion models such as artistic style adaptation and human face transfer. However, the unauthorized usage of data for model personalization has emerged as a prevalent concern in relation to copyright violations. For example, a malicious user may use the fine-tuning technique to generate images which mimic the style of a painter without his/her permission. In light of this concern, we have proposed FT-Shield, a watermarking approach specifically designed for the fine-tuning of text-to-image diffusion models to aid in detecting instances of infringement. We develop a novel algorithm for the generation of the watermark to ensure that the watermark on the training images can be quickly and accurately transferred to the generated images of text-to-image diffusion models. A watermark will be detected on an image by a binary watermark detector if the image is generated by a model that has been fine-tuned using the protected watermarked images. Comprehensive experiments were conducted to validate the effectiveness of FT-Shield.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Cui, Yingqian and Ren, Jie and Lin, Yuping and Xu, Han and He, Pengfei and Xing, Yue and Fan, Wenqi and Liu, Hui and Tang, Jiliang},
	month = oct,
	year = {2023},
	note = {arXiv:2310.02401},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Model copyright, Training data copyright, Watermark, diffusion, protection, text-to-image, training data ownership, watermarking},
}

@misc{chen_pathway_2023,
	title = {A {Pathway} {Towards} {Responsible} {AI} {Generated} {Content}},
	url = {http://arxiv.org/abs/2303.01325},
	doi = {10.48550/arXiv.2303.01325},
	abstract = {AI Generated Content (AIGC) has received tremendous attention within the past few years, with content generated in the format of image, text, audio, video, etc. Meanwhile, AIGC has become a double-edged sword and recently received much criticism regarding its responsible usage. In this article, we focus on 8 main concerns that may hinder the healthy development and deployment of AIGC in practice, including risks from (1) privacy; (2) bias, toxicity, misinformation; (3) intellectual property (IP); (4) robustness; (5) open source and explanation; (6) technology abuse; (7) consent, credit, and compensation; (8) environment. Additionally, we provide insights into the promising directions for tackling these risks while constructing generative models, enabling AIGC to be used more responsibly to truly benefit society.},
	urldate = {2024-02-23},
	publisher = {arXiv},
	author = {Chen, Chen and Fu, Jie and Lyu, Lingjuan},
	month = dec,
	year = {2023},
	note = {arXiv:2303.01325},
	keywords = {Computer Science - Artificial Intelligence, survey},
}

@misc{choi_stargan_2018,
	title = {{StarGAN}: {Unified} {Generative} {Adversarial} {Networks} for {Multi}-{Domain} {Image}-to-{Image} {Translation}},
	shorttitle = {{StarGAN}},
	url = {http://arxiv.org/abs/1711.09020},
	doi = {10.48550/arXiv.1711.09020},
	abstract = {Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.},
	urldate = {2024-04-08},
	publisher = {arXiv},
	author = {Choi, Yunjey and Choi, Minje and Kim, Munyoung and Ha, Jung-Woo and Kim, Sunghun and Choo, Jaegul},
	month = sep,
	year = {2018},
	note = {arXiv:1711.09020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{chen_editshield_2023,
	title = {{EditShield}: {Protecting} {Unauthorized} {Image} {Editing} by {Instruction}-guided {Diffusion} {Models}},
	shorttitle = {{EditShield}},
	url = {http://arxiv.org/abs/2311.12066},
	doi = {10.48550/arXiv.2311.12066},
	abstract = {Text-to-image diffusion models have emerged as an evolutionary for producing creative content in image synthesis. Based on the impressive generation abilities of these models, instruction-guided diffusion models can edit images with simple instructions and input images. While they empower users to obtain their desired edited images with ease, they have raised concerns about unauthorized image manipulation. Prior research has delved into the unauthorized use of personalized diffusion models; however, this problem of instruction-guided diffusion models remains largely unexplored. In this paper, we first propose a protection method EditShield against unauthorized modifications from such models. Specifically, EditShield works by adding imperceptible perturbations that can shift the latent representation used in the diffusion process, forcing models to generate unrealistic images with mismatched subjects. Our extensive experiments demonstrate EditShield's effectiveness among synthetic and real-world datasets. Besides, EditShield also maintains robustness against various editing types and synonymous instruction phrases.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Chen, Ruoxi and Jin, Haibo and Chen, Jinyin and Sun, Lichao},
	month = nov,
	year = {2023},
	note = {arXiv:2311.12066},
	keywords = {Computer Science - Cryptography and Security, adversarial perturbations, diffusion, editing, protection},
}

@misc{chen_challenges_2023,
	title = {Challenges and {Remedies} to {Privacy} and {Security} in {AIGC}: {Exploring} the {Potential} of {Privacy} {Computing}, {Blockchain}, and {Beyond}},
	shorttitle = {Challenges and {Remedies} to {Privacy} and {Security} in {AIGC}},
	url = {http://arxiv.org/abs/2306.00419},
	doi = {10.48550/arXiv.2306.00419},
	abstract = {Artificial Intelligence Generated Content (AIGC) is one of the latest achievements in AI development. The content generated by related applications, such as text, images and audio, has sparked a heated discussion. Various derived AIGC applications are also gradually entering all walks of life, bringing unimaginable impact to people's daily lives. However, the rapid development of such generative tools has also raised concerns about privacy and security issues, and even copyright issues in AIGC. We note that advanced technologies such as blockchain and privacy computing can be combined with AIGC tools, but no work has yet been done to investigate their relevance and prospect in a systematic and detailed way. Therefore it is necessary to investigate how they can be used to protect the privacy and security of data in AIGC by fully exploring the aforementioned technologies. In this paper, we first systematically review the concept, classification and underlying technologies of AIGC. Then, we discuss the privacy and security challenges faced by AIGC from multiple perspectives and purposefully list the countermeasures that currently exist. We hope our survey will help researchers and industry to build a more secure and robust AIGC system.},
	urldate = {2024-02-23},
	publisher = {arXiv},
	author = {Chen, Chuan and Wu, Zhenpeng and Lai, Yanyi and Ou, Wenlin and Liao, Tianchi and Zheng, Zibin},
	month = jun,
	year = {2023},
	note = {arXiv:2306.00419},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, survey},
}

@misc{chang_design_2023,
	title = {On the {Design} {Fundamentals} of {Diffusion} {Models}: {A} {Survey}},
	shorttitle = {On the {Design} {Fundamentals} of {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2306.04542},
	doi = {10.48550/arXiv.2306.04542},
	abstract = {Diffusion models are generative models, which gradually add and remove noise to learn the underlying distribution of training data for data generation. The components of diffusion models have gained significant attention with many design choices proposed. Existing reviews have primarily focused on higher-level solutions, thereby covering less on the design fundamentals of components. This study seeks to address this gap by providing a comprehensive and coherent review on component-wise design choices in diffusion models. Specifically, we organize this review according to their three key components, namely the forward process, the reverse process, and the sampling procedure. This allows us to provide a fine-grained perspective of diffusion models, benefiting future studies in the analysis of individual components, the applicability of design choices, and the implementation of diffusion models.},
	urldate = {2023-12-05},
	publisher = {arXiv},
	author = {Chang, Ziyi and Koulieris, George Alex and Shum, Hubert P. H.},
	month = oct,
	year = {2023},
	note = {arXiv:2306.04542},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{carlini_quantifying_2023,
	title = {Quantifying {Memorization} {Across} {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/2202.07646},
	doi = {10.48550/arXiv.2202.07646},
	abstract = {Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others). We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes more complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
	month = mar,
	year = {2023},
	note = {arXiv:2202.07646},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, memorisation},
}

@misc{carlini_extracting_2023,
	title = {Extracting {Training} {Data} from {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2301.13188},
	doi = {10.48550/arXiv.2301.13188},
	abstract = {Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Carlini, Nicholas and Hayes, Jamie and Nasr, Milad and Jagielski, Matthew and Sehwag, Vikash and Tramèr, Florian and Balle, Borja and Ippolito, Daphne and Wallace, Eric},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13188},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, data replication, memorisation, potential application for IPinGenAI, training data, training data sanitisation, violation},
}

@misc{cao_comprehensive_2023,
	title = {A {Comprehensive} {Survey} of {AI}-{Generated} {Content} ({AIGC}): {A} {History} of {Generative} {AI} from {GAN} to {ChatGPT}},
	shorttitle = {A {Comprehensive} {Survey} of {AI}-{Generated} {Content} ({AIGC})},
	url = {http://arxiv.org/abs/2303.04226},
	doi = {10.48550/arXiv.2303.04226},
	abstract = {Recently, ChatGPT, along with DALL-E-2 and Codex,has been gaining significant attention from society. As a result, many individuals have become interested in related resources and are seeking to uncover the background and secrets behind its impressive performance. In fact, ChatGPT and other Generative AI (GAI) techniques belong to the category of Artificial Intelligence Generated Content (AIGC), which involves the creation of digital content, such as images, music, and natural language, through AI models. The goal of AIGC is to make the content creation process more efficient and accessible, allowing for the production of high-quality content at a faster pace. AIGC is achieved by extracting and understanding intent information from instructions provided by human, and generating the content according to its knowledge and the intent information. In recent years, large-scale models have become increasingly important in AIGC as they provide better intent extraction and thus, improved generation results. With the growth of data and the size of the models, the distribution that the model can learn becomes more comprehensive and closer to reality, leading to more realistic and high-quality content generation. This survey provides a comprehensive review on the history of generative models, and basic components, recent advances in AIGC from unimodal interaction and multimodal interaction. From the perspective of unimodality, we introduce the generation tasks and relative models of text and image. From the perspective of multimodality, we introduce the cross-application between the modalities mentioned above. Finally, we discuss the existing open problems and future challenges in AIGC.},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Cao, Yihan and Li, Siyu and Liu, Yixin and Yan, Zhiling and Dai, Yutong and Yu, Philip S. and Sun, Lichao},
	month = mar,
	year = {2023},
	note = {arXiv:2303.04226},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, survey},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2024-02-19},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165},
	keywords = {Computer Science - Computation and Language},
}

@misc{berman_multigrain_2019,
	title = {{MultiGrain}: a unified image embedding for classes and instances},
	shorttitle = {{MultiGrain}},
	url = {http://arxiv.org/abs/1902.05509},
	doi = {10.48550/arXiv.1902.05509},
	abstract = {MultiGrain is a network architecture producing compact vector representations that are suited both for image classification and particular object retrieval. It builds on a standard classification trunk. The top of the network produces an embedding containing coarse and fine-grained information, so that images can be recognized based on the object class, particular object, or if they are distorted copies. Our joint training is simple: we minimize a cross-entropy loss for classification and a ranking loss that determines if two images are identical up to data augmentation, with no need for additional labels. A key component of MultiGrain is a pooling layer that takes advantage of high-resolution images with a network trained at a lower resolution. When fed to a linear classifier, the learned embeddings provide state-of-the-art classification accuracy. For instance, we obtain 79.4\% top-1 accuracy with a ResNet-50 learned on Imagenet, which is a +1.8\% absolute improvement over the AutoAugment method. When compared with the cosine similarity, the same embeddings perform on par with the state-of-the-art for image retrieval at moderate resolutions.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Berman, Maxim and Jégou, Hervé and Vedaldi, Andrea and Kokkinos, Iasonas and Douze, Matthijs},
	month = apr,
	year = {2019},
	note = {arXiv:1902.05509},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{bao_cvae-gan_2017,
	title = {{CVAE}-{GAN}: {Fine}-{Grained} {Image} {Generation} through {Asymmetric} {Training}},
	shorttitle = {{CVAE}-{GAN}},
	url = {http://arxiv.org/abs/1703.10155},
	abstract = {We present variational generative adversarial networks, a general learning framework that combines a variational auto-encoder with a generative adversarial network, for synthesizing images in fine-grained categories, such as faces of a specific person or objects in a category. Our approach models an image as a composition of label and latent attributes in a probabilistic model. By varying the fine-grained category label fed into the resulting generative model, we can generate images in a specific category with randomly drawn values on a latent attribute vector. Our approach has two novel aspects. First, we adopt a cross entropy loss for the discriminative and classifier network, but a mean discrepancy objective for the generative network. This kind of asymmetric loss function makes the GAN training more stable. Second, we adopt an encoder network to learn the relationship between the latent space and the real image space, and use pairwise feature matching to keep the structure of generated images. We experiment with natural images of faces, flowers, and birds, and demonstrate that the proposed models are capable of generating realistic and diverse samples with fine-grained category labels. We further show that our models can be applied to other tasks, such as image inpainting, super-resolution, and data augmentation for training better face recognition models.},
	urldate = {2024-02-20},
	publisher = {arXiv},
	author = {Bao, Jianmin and Chen, Dong and Wen, Fang and Li, Houqiang and Hua, Gang},
	month = oct,
	year = {2017},
	note = {arXiv:1703.10155},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ahn_imperceptible_2024,
	title = {Imperceptible {Protection} against {Style} {Imitation} from {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2403.19254},
	doi = {10.48550/arXiv.2403.19254},
	abstract = {Recent progress in diffusion models has profoundly enhanced the fidelity of image generation. However, this has raised concerns about copyright infringements. While prior methods have introduced adversarial perturbations to prevent style imitation, most are accompanied by the degradation of artworks' visual quality. Recognizing the importance of maintaining this, we develop a visually improved protection method that preserves its protection capability. To this end, we create a perceptual map to identify areas most sensitive to human eyes. We then adjust the protection intensity guided by an instance-aware refinement. We also integrate a perceptual constraints bank to further improve the imperceptibility. Results show that our method substantially elevates the quality of the protected image without compromising on protection efficacy.},
	urldate = {2024-04-10},
	publisher = {arXiv},
	author = {Ahn, Namhyuk and Ahn, Wonhyuk and Yoo, KiYoon and Kim, Daesik and Nam, Seung-Hun},
	month = mar,
	year = {2024},
	note = {arXiv:2403.19254},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, todo},
}

@article{senftleben_generative_2023,
	title = {Generative {AI} and {Author} {Remuneration}},
	volume = {54},
	issn = {2195-0237},
	url = {https://doi.org/10.1007/s40319-023-01399-4},
	doi = {10.1007/s40319-023-01399-4},
	abstract = {With the evolution of generative AI systems, machine-made productions in the literary and artistic field have reached a level of refinement that allows them to replace human creations. The increasing sophistication of AI systems will inevitably disrupt the market for human literary and artistic works. Generative AI systems provide literary and artistic output much faster and cheaper. It is therefore foreseeable that human authors will be exposed to substitution effects. They may lose income as they are replaced by machines in sectors ranging from journalism and writing to music and visual arts. Considering this trend, the question arises whether it is advisable to take measures to compensate human authors for the reduction in their market share and income. Copyright law could serve as a tool to introduce an AI levy system and ensure the payment of equitable remuneration. In combination with mandatory collective rights management, the new revenue stream could be used to finance social and cultural funds that improve the working and living conditions of flesh-and-blood authors.},
	language = {en},
	number = {10},
	urldate = {2024-04-19},
	journal = {IIC - International Review of Intellectual Property and Competition Law},
	author = {Senftleben, Martin},
	month = nov,
	year = {2023},
	keywords = {Art autonomy, Collective rights management, Copyright, Domaine public payant, Equitable remuneration, Freedom of expression, Levy system, Reservation of rights, Text and data mining, Three-step test, survey},
	pages = {1535--1560},
}

@misc{batlle-roca_transparency_2023,
	title = {Transparency in {Music}-{Generative} {AI}: {A} {Systematic} {Literature} {Review}},
	shorttitle = {Transparency in {Music}-{Generative} {AI}},
	url = {https://www.researchsquare.com/article/rs-3708077/v1},
	doi = {10.21203/rs.3.rs-3708077/v1},
	abstract = {Music-generative AI raises multiple challenges particularly related to the work of artists, the existing music industry model, the role of AI in creative processes, and the discussion of intellectual property rights. Our study addresses these challenges by examining transparency in music generation. We conduct a systematic literature review, following the PRISMA methodology, to gain a comprehensive understanding of the associations between algorithmic transparency, music generation, the evaluation in terms of creativity and originality, and the connections to intellectual property rights.We identify 1,111 publications by formulating four research questions. Following a rigorous review process, we narrow down the selection to 66 relevant investigations published by 2022, covering multiple AI domains. Acknowledging the rapid growth of the music generation field, we then incorporate 18 publications from 2023, focusing our search on the music-specific domain and novel applications. Thus, the present review overviews 84 publications. Our findings highlight a growing interest in AI transparency and the ethical consequences of generative models. However, transparent strategies in music-generative AI remain an under-explored topic. Our main contribution is the identification of research gaps and challenges in transparency for music-generative AI.},
	urldate = {2024-04-19},
	author = {Batlle-Roca, Roser and Gómez, Emila and Liao, WeiHsiang and Serra, Xavier and Mitsufuji, Yuki},
	month = dec,
	year = {2023},
	note = {ISSN: 2693-5015},
	keywords = {survey},
}

@article{xue_intellectual_2022,
	title = {Intellectual {Property} {Protection} for {Deep} {Learning} {Models}: {Taxonomy}, {Methods}, {Attacks}, and {Evaluations}},
	volume = {3},
	issn = {2691-4581},
	shorttitle = {Intellectual {Property} {Protection} for {Deep} {Learning} {Models}},
	url = {https://ieeexplore.ieee.org/abstract/document/9645219},
	doi = {10.1109/TAI.2021.3133824},
	abstract = {The training and creation of deep learning model is usually costly, thus the trained model can be regarded as an intellectual property (IP) of the model creator. However, malicious users who obtain high-performance models may illegally copy, redistribute, or abuse the models without permission. To deal with such security threats, a few deep neural networks (DNN) IP protection methods have been proposed in recent years. This article attempts to provide a review of the existing DNN IP protection works and also an outlook. First, we propose the first taxonomy for DNN IP protection methods in terms of six attributes—scenario, mechanism, capacity, type, function, and target models. Then, we present a survey on existing DNN IP protection works in terms of the above six attributes, especially focusing on the challenges these methods face, whether these methods can provide proactive protection, and their resistances to different levels of attacks. After that, we analyze the potential attacks on DNN IP protection methods from the aspects of model modifications, evasion attacks, and active attacks. Besides, a systematic evaluation method for DNN IP protection methods with respect to basic functional metrics, attack-resistance metrics, and customized metrics for different application scenarios is given. Finally, challenges and future research opportunities on DNN IP protection are presented.},
	number = {6},
	urldate = {2024-04-19},
	journal = {IEEE Transactions on Artificial Intelligence},
	author = {Xue, Mingfu and Zhang, Yushu and Wang, Jian and Liu, Weiqiang},
	month = dec,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Artificial Intelligence},
	keywords = {Attack resistance, Deep learning, Intellectual property, Machine learning, Security, Taxonomy, Watermarking, deep neural network (DNN), intellectual property (IP) protection, machine learning security, survey, taxonomy},
	pages = {908--923},
}

@inproceedings{hwang_brief_2023,
	title = {A {Brief} {Survey} of {Watermarks} in {Generative} {AI}},
	url = {https://ieeexplore.ieee.org/abstract/document/10392465},
	doi = {10.1109/ICTC58733.2023.10392465},
	abstract = {Generative AI technology is now capable of producing images and text at a level comparable to that of humans, showcasing its remarkable utility. However, this advancement comes with its share of challenges, such as misuse, prompting discussions on effective response strategies. Consequently, recommendations and regulations, including the adoption of watermark technology, are under deliberation on a country-by-country basis. Many companies are also integrating watermark technology into their services as a means of addressing this issue. This paper presents an analysis of the current status of watermark adoption across various countries and companies. Furthermore, it delves into further research topics that should be taken into account when implementing watermark technology. This analysis aims to provide valuable insights to those who are contemplating the implementation of watermarking in their future generative AI services.},
	urldate = {2024-04-19},
	booktitle = {2023 14th {International} {Conference} on {Information} and {Communication} {Technology} {Convergence} ({ICTC})},
	author = {Hwang, JaeYoung and Oh, SangHoon},
	month = oct,
	year = {2023},
	note = {ISSN: 2162-1241},
	keywords = {Artificial Intelligence (AI), Companies, Generative AI, Industries, Information and communication technology, Regulation, Surveys, Watermarking, Watermarks, survey},
	pages = {1157--1160},
}

@article{kop_ai_2019,
	title = {{AI} \& {Intellectual} {Property}: {Towards} an {Articulated} {Public} {Domain}},
	volume = {28},
	shorttitle = {{AI} \& {Intellectual} {Property}},
	url = {https://heinonline.org/HOL/Page?handle=hein.journals/tipj28&id=309&div=&collection=},
	journal = {Texas Intellectual Property Law Journal},
	author = {Kop, Mauritz},
	year = {2019},
	keywords = {legal, survey},
	pages = {297},
}

@article{mandapuram_investigating_2018,
	title = {Investigating the {Prospects} of {Generative} {Artificial} {Intelligence}},
	volume = {5},
	copyright = {Copyright (c) 2018 Mounika Mandapuram, Sai Srujan Gutlapalli, Anusha Bodepudi,  Manjunath Reddy},
	issn = {2312-2021},
	url = {https://i-proclaim.my/journals/index.php/ajhal/article/view/659},
	doi = {10.18034/ajhal.v5i2.659},
	abstract = {In this exploratory work, we investigate cutting-edge techniques in machine learning known as Generative Artificial Intelligence (GenAI). The costs of trial and error during product development can be significantly reduced if faster, more affordable, and more accurate multi-scale materials simulations powered by fully generative artificial intelligence are available. Engineers have spent decades attempting to develop humanoid robots that are both practical and resemble people in appearance and behavior. Because it enables us to circumvent the inherent dimensionality of this obstacle, generative artificial intelligence has the potential to be a beneficial instrument for the current creation process. Moreover, the research underlines that generative artificial intelligence, capable of producing media such as text, images, and audio in response to prompts, appears to improve daily. In addition, numerous technological companies are currently building and releasing their competing systems.},
	language = {en},
	number = {2},
	urldate = {2024-04-19},
	journal = {Asian Journal of Humanity, Art and Literature},
	author = {Mandapuram, Mounika and Gutlapalli, Sai Srujan and Bodepudi, Anusha and Reddy, Manjunath},
	month = dec,
	year = {2018},
	note = {Number: 2},
	keywords = {Competing Systems, Current Creation Process, GenAI, Technological Company, survey},
	pages = {167--174},
}

@article{shah_creative_2023,
	title = {Creative {Computing} and {Harnessing} the {Power} of {Generative} {Artificial} {Intelligence}},
	volume = {2},
	copyright = {Copyright (c) 2024},
	issn = {3006-046X},
	url = {https://jest.com.pk/index.php/jest/article/view/123},
	abstract = {Generative artificial intelligence (AI) has emerged as a powerful tool for creative computing, enabling the generation of novel and diverse content across various domains. In this paper, we delve into the field of generative AI, exploring its underlying principles, techniques, and applications in creative computing. We provide an overview of generative models, including generative adversarial networks (GANs), variational autoencoders (VAEs), and autoregressive models, highlighting their capabilities in generating realistic and diverse outputs. Through a review of state-of-the-art research and case studies, we showcase the creative potential of generative AI in generating images, videos, music compositions, and text-based content. We discuss the unique challenges and opportunities associated with generative AI, including issues related to training data, model complexity, and evaluation metrics. Furthermore, we explore the societal and ethical implications of generative AI, including concerns related to intellectual property rights, authenticity, and misuse. Our study aims to inspire further exploration and innovation in the field of creative computing, leveraging generative AI to push the boundaries of human creativity and expression. By harnessing the power of generative AI, researchers, artists, and creators can unlock new possibilities for artistic expression, storytelling, and design. We believe that creative computing with generative AI has the potential to revolutionize various industries, from entertainment and advertising to education and healthcare, paving the way for a more imaginative and innovative future.},
	language = {en},
	number = {1},
	urldate = {2024-04-19},
	journal = {Journal Environmental Sciences And Technology},
	author = {Shah, Varun and Shukla, Shubham},
	month = mar,
	year = {2023},
	note = {Number: 1},
	keywords = {Artificial, Computing, Creative, Generative, Harnessing, Intelligence, Power, survey},
	pages = {556--579},
}

@article{golda_privacy_2024,
	title = {Privacy and {Security} {Concerns} in {Generative} {AI}: {A} {Comprehensive} {Survey}},
	volume = {12},
	issn = {2169-3536},
	shorttitle = {Privacy and {Security} {Concerns} in {Generative} {AI}},
	url = {https://ieeexplore.ieee.org/abstract/document/10478883},
	doi = {10.1109/ACCESS.2024.3381611},
	abstract = {Generative Artificial Intelligence (GAI) has sparked a transformative wave across various domains, including machine learning, healthcare, business, and entertainment, owing to its remarkable ability to generate lifelike data. This comprehensive survey offers a meticulous examination of the privacy and security challenges inherent to GAI. It provides five pivotal perspectives essential for a comprehensive understanding of these intricacies. The paper encompasses discussions on GAI architectures, diverse generative model types, practical applications, and recent advancements within the field. In addition, it highlights current security strategies and proposes sustainable solutions, emphasizing user, developer, institutional, and policymaker involvement.},
	urldate = {2024-04-19},
	journal = {IEEE Access},
	author = {Golda, Abenezer and Mekonen, Kidus and Pandey, Amit and Singh, Anushka and Hassija, Vikas and Chamola, Vinay and Sikdar, Biplab},
	year = {2024},
	note = {Conference Name: IEEE Access},
	keywords = {Artificial intelligence, Computational modeling, Computer security, Data models, Data privacy, Deep learning, Deepfake, Ethics, Fake news, Generative AI, Generative adversarial networks, Generative artificial intelligence, Homomorphic encryption, Privacy, Security, Surveys, Threat assessment, adversarial attacks, artificial intelligence, cybersecurity, data security, deep learning, ethical implications, ethical responsibility, machine learning, misinformation, privacy concerns, privacy preservation, privacy protection, regulatory compliance, security concerns, social engineering, survey, synthetic data, threat analysis},
	pages = {48126--48144},
}

@article{boenisch_systematic_2021,
	title = {A {Systematic} {Review} on {Model} {Watermarking} for {Neural} {Networks}},
	volume = {4},
	issn = {2624-909X},
	url = {https://www.frontiersin.org/articles/10.3389/fdata.2021.729663},
	doi = {10.3389/fdata.2021.729663},
	abstract = {Machine learning (ML) models are applied in an increasing variety of domains. The availability of large amounts of data and computational resources encourages the development of ever more complex and valuable models. These models are considered intellectual property of the legitimate parties who have trained them, which makes their protection against stealing, illegitimate redistribution, and unauthorized application an urgent need. Digital watermarking presents a strong mechanism for marking model ownership and, thereby, offers protection against those threats. This work presents a taxonomy identifying and analyzing different classes of watermarking schemes for ML models. It introduces a unified threat model to allow structured reasoning on and comparison of the effectiveness of watermarking methods in different scenarios. Furthermore, it systematizes desired security requirements and attacks against ML model watermarking. Based on that framework, representative literature from the field is surveyed to illustrate the taxonomy. Finally, shortcomings and general limitations of existing approaches are discussed, and an outlook on future research directions is given.},
	language = {English},
	urldate = {2024-04-18},
	journal = {Frontiers in Big Data},
	author = {Boenisch, Franziska},
	month = nov,
	year = {2021},
	note = {Publisher: Frontiers},
	keywords = {Watermarking, intellectual property protection, machine learning, model IP protection, model stealing, model watermarking, neural networks, survey},
}

@article{wan_comprehensive_2022,
	title = {A comprehensive survey on robust image watermarking},
	volume = {488},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231222002533},
	doi = {10.1016/j.neucom.2022.02.083},
	abstract = {With the rapid development and popularity of the Internet, multimedia security has become a general essential concern. Especially, as manipulation of digital images gets much easier, the challenges it brings to authentication certification are increasing. As part of the solution, digital watermarking has made significant contributions to image content security and has attracted increasing attention. In this paper, we present a comprehensive review on digital image watermarking methods that were published in recent years illustrating the conventional schemes in different domains. We provide an overview of geometric invariant techniques and emerging watermarking methods for novel medias, such as depth image based rendering (DIBR), high dynamic range (HDR), screen content images (SCIs), and point cloud model. Particularly, as deep learning has achieved a great success in the field of image processing, and has also successfully been used in the field of digital watermarking, learning-based watermarking methods using various neural networks are summarized according to the utilization of neural networks in the single stage training (SST) and double stage training (DST). Finally, we provide an analysis and summary on those methods, and suggest some future research directions.},
	urldate = {2024-04-19},
	journal = {Neurocomputing},
	author = {Wan, Wenbo and Wang, Jun and Zhang, Yunming and Li, Jing and Yu, Hui and Sun, Jiande},
	month = jun,
	year = {2022},
	keywords = {Deep learning, HDR image, Image watermarking, Model watermarking, Robustness, survey, watermarking},
	pages = {226--247},
}

@article{lederer_identifying_2023,
	title = {Identifying {Appropriate} {Intellectual} {Property} {Protection} {Mechanisms} for {Machine} {Learning} {Models}: {A} {Systematization} of {Watermarking}, {Fingerprinting}, {Model} {Access}, and {Attacks}},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2162-237X, 2162-2388},
	shorttitle = {Identifying {Appropriate} {Intellectual} {Property} {Protection} {Mechanisms} for {Machine} {Learning} {Models}},
	url = {https://ieeexplore.ieee.org/document/10143370/},
	doi = {10.1109/TNNLS.2023.3270135},
	urldate = {2024-04-17},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Lederer, Isabell and Mayer, Rudolf and Rauber, Andreas},
	year = {2023},
	keywords = {Attacks on intellectual property protection (IPP), Computational modeling, Data models, Fingerprint recognition, IPP, Surveys, Taxonomy, Threat modeling, Watermarking, fingerprinting, machine learning (ML), model IP protection, model access control, survey, watermarking},
	pages = {1--19},
}

@article{regazzoni_protecting_2021,
	title = {Protecting artificial intelligence {IPs}: a survey of watermarking and fingerprinting for machine learning},
	volume = {6},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2468-2322, 2468-2322},
	shorttitle = {Protecting artificial intelligence {IPs}},
	url = {https://onlinelibrary.wiley.com/doi/10.1049/cit2.12029},
	doi = {10.1049/cit2.12029},
	language = {en},
	number = {2},
	urldate = {2024-04-17},
	journal = {CAAI Transactions on Intelligence Technology},
	author = {Regazzoni, Francesco and Palmieri, Paolo and Smailbegovic, Fethulah and Cammarota, Rosario and Polian, Ilia},
	month = jun,
	year = {2021},
	keywords = {discriminative models, model IP protection, model watermarking, survey},
	pages = {180--191},
}

@article{parnagian_should_nodate,
	title = {Should {AI} have {Intellectual} {Property} {Rights}? {An} {Analysis} of {Copyright} {Law} on {Generative} {AI}},
	abstract = {The rapid advancement of artiﬁcial intelligence (AI) technologies in recent years has challenged our understanding of intellectual property and created a complex legal landscape with far-reaching implications for copyright laws. This research aims to explore the intersection of copyright law, current lawsuits, and the impact of AI advancements on our perceptions of creativity and intelligence. We will ﬁrst examine the fundamentals of copyright law, followed by an analysis of recent AI-related lawsuits that highlight the legal challenges posed by AI-generated works. Finally, we will discuss whether copyright protection should extend to new forms of intelligence, taking into consideration how AI might change our understanding of creativity and ownership.},
	language = {en},
	author = {Parnagian, Ani},
	keywords = {discussion, ethical, legal},
}

@article{cina_wild_2023,
	title = {Wild {Patterns} {Reloaded}: {A} {Survey} of {Machine} {Learning} {Security} against {Training} {Data} {Poisoning}},
	volume = {55},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Wild {Patterns} {Reloaded}},
	url = {https://dl.acm.org/doi/10.1145/3585385},
	doi = {10.1145/3585385},
	abstract = {The success of machine learning is fueled by the increasing availability of computing power and large training datasets. The training data is used to learn new models or update existing ones, assuming that it is sufficiently representative of the data that will be encountered at test time. This assumption is challenged by the threat of poisoning, an attack that manipulates the training data to compromise the model’s performance at test time. Although poisoning has been acknowledged as a relevant threat in industry applications, and a variety of different attacks and defenses have been proposed so far, a complete systematization and critical review of the field is still missing. In this survey, we provide a comprehensive systematization of poisoning attacks and defenses in machine learning, reviewing more than 100 papers published in the field in the past 15 years. We start by categorizing the current threat models and attacks and then organize existing defenses accordingly. While we focus mostly on computer-vision applications, we argue that our systematization also encompasses state-of-the-art attacks and defenses for other data modalities. Finally, we discuss existing resources for research in poisoning and shed light on the current limitations and open research questions in this research field.},
	language = {en},
	number = {13s},
	urldate = {2024-04-15},
	journal = {ACM Computing Surveys},
	author = {Cinà, Antonio Emanuele and Grosse, Kathrin and Demontis, Ambra and Vascon, Sebastiano and Zellinger, Werner and Moser, Bernhard A. and Oprea, Alina and Biggio, Battista and Pelillo, Marcello and Roli, Fabio},
	month = dec,
	year = {2023},
	keywords = {discriminative models, poisoning, survey},
	pages = {1--39},
}

@article{zhang_adversarial_2019,
	title = {Adversarial {Examples}: {Opportunities} and {Challenges}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {2162-237X, 2162-2388},
	shorttitle = {Adversarial {Examples}},
	url = {https://ieeexplore.ieee.org/document/8842604/},
	doi = {10.1109/TNNLS.2019.2933524},
	urldate = {2024-04-17},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Zhang, Jiliang and Li, Chen},
	year = {2019},
	keywords = {Adversarial examples (AEs), Artificial intelligence, Biological neural networks, Neurons, Perturbation methods, Robots, Security, Training, artificial intelligence (AI), deep neural networks (DNNs)},
	pages = {1--16},
}

@misc{noauthor_faketagger_nodate,
	title = {{FakeTagger} {\textbar} {Proceedings} of the 29th {ACM} {International} {Conference} on {Multimedia}},
	url = {https://dl.acm.org/doi/abs/10.1145/3474085.3475518},
	urldate = {2024-04-16},
}

@article{qu_df-rap_2024,
	title = {{DF}-{RAP}: {A} {Robust} {Adversarial} {Perturbation} for {Defending} against {Deepfakes} in {Real}-world {Social} {Network} {Scenarios}},
	issn = {1556-6021},
	shorttitle = {{DF}-{RAP}},
	url = {https://ieeexplore.ieee.org/abstract/document/10458678},
	doi = {10.1109/TIFS.2024.3372803},
	abstract = {The misuse of Deepfakes to create unauthorized fake facial images and videos poses a growing threat to personal privacy and social stability. Proactive defense algorithms have been proposed to prevent this fraud by injecting adversarial perturbations into facial images. However, these perturbations are sensitive to the lossy compression on online social networks (OSNs). Recent studies have attempted to produce compression resistance by modeling compression at the pixel level. However, accurate modeling is challenging due to the customization of proprietary compression mechanisms by different OSNs. In this paper, we propose a Robust Adversarial Perturbation (DF-RAP) that provides persistent protection for facial images under OSN compression. Specifically, a novel Compression Approximation GAN (ComGAN) is designed to explicitly model OSN compression. The well-trained ComGAN is then incorporated as a sub-module of the target Deepfake model to derive DF-RAP. Furthermore, we reveal a commonality among various OSNs, i.e., that the lossy compression employed tends to destroy perturbations. Based on this, a novel objective-level destruction-aware constraint (DAC) is introduced during ComGAN training. The extensive experimental results show that DF-RAP can effectively protect facial images from Deepfakes under complex OSN compression, especially for OSNs employing more stringent compression. We also investigate the lossy operation mechanisms employed by widely used OSN platforms and build an OSN-transmission dataset based on the CelebA to facilitate future research.},
	urldate = {2024-04-16},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Qu, Zuomin and Xi, Zuping and Lu, Wei and Luo, Xiangyang and Wang, Qian and Li, Bin},
	year = {2024},
	note = {Conference Name: IEEE Transactions on Information Forensics and Security},
	keywords = {Blogs, Deepfakes, Faces, Image coding, Message services, Perturbation methods, Social networking (online), adversarial attack, compression approximation, deepfake, proactive defense},
	pages = {1--1},
}

@inproceedings{zeng_loft_2024,
	title = {{LOFT}: {Latent} {Space} {Optimization} and {Generator} {Fine}-{Tuning} for {Defending} {Against} {Deepfakes}},
	shorttitle = {{LOFT}},
	url = {https://ieeexplore.ieee.org/document/10447890?denied=},
	doi = {10.1109/ICASSP48485.2024.10447890},
	abstract = {DeepFakes pose a significant threat to individual reputations and society as a whole. Existing proactive defense strategies concentrate on adding adversarial perturbations to images to disrupt or nullify the generation of DeepFakes, but these approaches are easily detectable by human perception and can be removed. To address this challenge, we propose a three-stage framework called LOFT (Latent Space Optimization and Generator Fine-Tuning for Defending against DeepFakes). First, encoding the original image into the latent space to obtain a latent code that captures facial features. Second, utilizing Adversarial Latent Optimization to optimize the latent code for reconstructing the image and defending against DeepFake manipulation. Third, fine-tuning the generator to enhance the reconstructed image’s visual quality and defense capability further. Our study evaluates the effectiveness of our proposed framework through two distinct DeepFake tasks: attribute editing and face reenactment. Various experimental results demonstrate that our proposed framework outperforms the existing benchmark in both visual quality and defense capability.},
	urldate = {2024-04-16},
	booktitle = {{ICASSP} 2024 - 2024 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Zeng, Shaoyou and Wang, Wenhao and Huang, Fangjun and Fang, Yanmei},
	month = apr,
	year = {2024},
	note = {ISSN: 2379-190X},
	keywords = {Codes, DeepFake, Deepfakes, Generators, Semantics, Signal processing, Task analysis, Visualization, adversarial attack, deepfake, image forensics, latent representations, proactive defense},
	pages = {4750--4754},
}

@inproceedings{bax_generative_2023,
	address = {Cham},
	title = {Generative {AI}: {Citations} for {Trust} and {Consensus}},
	isbn = {978-3-031-47454-5},
	shorttitle = {Generative {AI}},
	doi = {10.1007/978-3-031-47454-5_14},
	abstract = {Generative AI uses a large set of sources to create content. The content generated by large language models is text. Often, that text contains statements that are inaccurate or false, sometimes called hallucinations. We explore how identifying citations for the generated text can enable people to determine whether to trust the statements in the text, by allowing different users to specify different trusted sets of sources as candidates for citations. Then we propose methods to eliminate or correct untrustworthy statements. We also consider how citations can help build consensus among people who have different trusted sources of information, by using a large language model to construct text, then editing the text so that it is supported by citations drawn from multiple sets of trusted sources. By using generative AI as a go-between, such a process may allow parties with mutual distrust to discover and confirm areas of agreement. This paper is a proposal for systems that enhance large language models’ usefulness and an outline of some challenges and methods for such systems; it is not a record of system development or testing.},
	language = {en},
	booktitle = {Proceedings of the {Future} {Technologies} {Conference} ({FTC}) 2023, {Volume} 1},
	publisher = {Springer Nature Switzerland},
	author = {Bax, Eric and Gerber, Melissa and Giaffo, Lisa and Sarkar, Arundhyoti and Thompson, Nikki and Wagner, Will and Williams, Kimberly},
	editor = {Arai, Kohei},
	year = {2023},
	keywords = {Consensus, Fact-Checking, Generative AI, Large Language Model},
	pages = {188--195},
}

@misc{noauthor_attribution_2022,
	title = {The attribution problem with generative {AI}},
	url = {https://hackingsemantics.xyz/2022/attribution/},
	abstract = {Some argue that any publicly available text/art data is fair game for commercial models because human text/art also has sources. But unlike models, we know when attribution is due…},
	language = {en},
	urldate = {2024-04-15},
	journal = {Hacking semantics},
	month = nov,
	year = {2022},
}

@misc{brodkin_us_2023,
	title = {{US} judge: {Art} created solely by artificial intelligence cannot be copyrighted},
	url = {https://arstechnica.com/tech-policy/2023/08/us-judge-art-created-solely-by-artificial-intelligence-cannot-be-copyrighted/},
	urldate = {2024-03-15},
	journal = {Ars Technica},
	author = {Brodkin, Jon},
	month = aug,
	year = {2023},
	note = {https://arstechnica.com/tech-policy/2023/08/us-judge-art-created-solely-by-artificial-intelligence-cannot-be-copyrighted/},
}

@misc{roose_ai-generated_2022,
	title = {{AI}-{Generated} {Art} {Won} a {Prize}. {Artists} {Aren}’t {Happy}.},
	url = {https://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html},
	urldate = {2023-07-26},
	journal = {The New York Times},
	author = {Roose, Kevin},
	month = feb,
	year = {2022},
	note = {https://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html},
}

@misc{noauthor_michellejielinsfw_text_classifier_nodate,
	title = {michellejieli/{NSFW}\_text\_classifier · {Hugging} {Face}},
	url = {https://huggingface.co/michellejieli/NSFW_text_classifier},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-04-11},
}

@inproceedings{hacker_regulating_2023,
	address = {New York, NY, USA},
	series = {{FAccT} '23},
	title = {Regulating {ChatGPT} and other {Large} {Generative} {AI} {Models}},
	isbn = {9798400701924},
	url = {https://dl.acm.org/doi/10.1145/3593013.3594067},
	doi = {10.1145/3593013.3594067},
	abstract = {Large generative AI models (LGAIMs), such as ChatGPT, GPT-4 or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of society at large. Rules in the AI Act and other direct regulation must match the specificities of pre-trained models. The paper argues for three layers of obligations concerning LGAIMs (minimum standards for all LGAIMs; high-risk obligations for high-risk use cases; collaborations along the AI value chain). In general, regulation should focus on concrete high-risk applications, and not the pre-trained model itself, and should include (i) obligations regarding transparency and (ii) risk management. Non-discrimination provisions (iii) may, however, apply to LGAIM developers. Lastly, (iv) the core of the DSA's content moderation rules should be expanded to cover LGAIMs. This includes notice and action mechanisms, and trusted flaggers.},
	urldate = {2024-04-11},
	booktitle = {Proceedings of the 2023 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Hacker, Philipp and Engel, Andreas and Mauer, Marco},
	month = jun,
	year = {2023},
	pages = {1112--1123},
}

@inproceedings{kandpal_deduplicating_2022,
	title = {Deduplicating {Training} {Data} {Mitigates} {Privacy} {Risks} in {Language} {Models}},
	url = {https://proceedings.mlr.press/v162/kandpal22a.html},
	abstract = {Past work has shown that large language models are susceptible to privacy attacks, where adversaries generate sequences from a trained model and detect which sequences are memorized from the training set. In this work, we show that the success of these attacks is largely due to duplication in commonly used web-scraped training sets. We first show that the rate at which language models regenerate training sequences is superlinearly related to a sequence’s count in the training set. For instance, a sequence that is present 10 times in the training data is on average generated 1000x more often than a sequence that is present only once. We next show that existing methods for detecting memorized sequences have near-chance accuracy on non-duplicated training sequences. Finally, we find that after applying methods to deduplicate training data, language models are considerably more secure against these types of privacy attacks. Taken together, our results motivate an increased focus on deduplication in privacy-sensitive applications and a reevaluation of the practicality of existing privacy attacks.},
	language = {en},
	urldate = {2024-04-10},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kandpal, Nikhil and Wallace, Eric and Raffel, Colin},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {10697--10707},
}

@inproceedings{ong_protecting_2021,
	address = {Nashville, TN, USA},
	title = {Protecting {Intellectual} {Property} of {Generative} {Adversarial} {Networks} from {Ambiguity} {Attacks}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9577609/},
	doi = {10.1109/CVPR46437.2021.00363},
	urldate = {2024-04-04},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Ong, Ding Sheng and Seng Chan, Chee and Ng, Kam Woh and Fan, Lixin and Yang, Qiang},
	month = jun,
	year = {2021},
	pages = {3629--3638},
}

@misc{noauthor_pdf_nodate,
	title = {[{PDF}] {WaveNet}: {A} {Generative} {Model} for {Raw} {Audio} {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/paper/WaveNet%3A-A-Generative-Model-for-Raw-Audio-Oord-Dieleman/df0402517a7338ae28bc54acaac400de6b456a46},
	urldate = {2024-04-04},
}

@inproceedings{arik_deep_2017,
	title = {Deep {Voice}: {Real}-time {Neural} {Text}-to-{Speech}},
	shorttitle = {Deep {Voice}},
	url = {https://proceedings.mlr.press/v70/arik17a.html},
	abstract = {We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.},
	language = {en},
	urldate = {2024-04-04},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Arık, Sercan Ö and Chrzanowski, Mike and Coates, Adam and Diamos, Gregory and Gibiansky, Andrew and Kang, Yongguo and Li, Xian and Miller, John and Ng, Andrew and Raiman, Jonathan and Sengupta, Shubho and Shoeybi, Mohammad},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {195--204},
}

@article{knott_generative_2023,
	title = {Generative {AI} models should include detection mechanisms as a condition for public release},
	volume = {25},
	issn = {1572-8439},
	url = {https://doi.org/10.1007/s10676-023-09728-4},
	doi = {10.1007/s10676-023-09728-4},
	abstract = {The new wave of ‘foundation models’—general-purpose generative AI models, for production of text (e.g., ChatGPT) or images (e.g., MidJourney)—represent a dramatic advance in the state of the art for AI. But their use also introduces a range of new risks, which has prompted an ongoing conversation about possible regulatory mechanisms. Here we propose a specific principle that should be incorporated into legislation: that any organization developing a foundation model intended for public use must demonstrate a reliable detection mechanism for the content it generates, as a condition of its public release. The detection mechanism should be made publicly available in a tool that allows users to query, for an arbitrary item of content, whether the item was generated (wholly or partly) by the model. In this paper, we argue that this requirement is technically feasible and would play an important role in reducing certain risks from new AI models in many domains. We also outline a number of options for the tool’s design, and summarize a number of points where further input from policymakers and researchers would be required.},
	language = {en},
	number = {4},
	urldate = {2024-04-03},
	journal = {Ethics and Information Technology},
	author = {Knott, Alistair and Pedreschi, Dino and Chatila, Raja and Chakraborti, Tapabrata and Leavy, Susan and Baeza-Yates, Ricardo and Eyers, David and Trotman, Andrew and Teal, Paul D. and Biecek, Przemyslaw and Russell, Stuart and Bengio, Yoshua},
	month = oct,
	year = {2023},
	keywords = {AI ethics, AI regulation, AI social impacts, Foundation models, Generative AI},
	pages = {55},
}

@inproceedings{fredrikson_model_2015,
	address = {Denver Colorado USA},
	series = {{CCS}},
	title = {Model {Inversion} {Attacks} that {Exploit} {Confidence} {Information} and {Basic} {Countermeasures}},
	isbn = {978-1-4503-3832-5},
	doi = {10.1145/2810103.2813677},
	language = {en},
	booktitle = {{ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
	month = oct,
	year = {2015},
	pages = {1322--1333},
}

@inproceedings{shokri_membership_2017,
	address = {San Jose, CA, USA},
	title = {Membership {Inference} {Attacks} {Against} {Machine} {Learning} {Models}},
	doi = {10.1109/SP.2017.41},
	booktitle = {{IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	publisher = {IEEE},
	author = {Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
	year = {2017},
	pages = {3--18},
}

@article{ulku_kahveci_attribution_2023,
	title = {Attribution problem of generative {AI}: a view from {US} copyright law},
	volume = {18},
	url = {https://academic.oup.com/jiplp/article/18/11/796/7271384},
	number = {11},
	urldate = {2024-03-13},
	journal = {Journal of Intellectual Property Law \& Practice},
	author = {Ülkü Kahveci, Zeynep},
	year = {2023},
	pages = {796--807},
}

@misc{noauthor_synthid_2023,
	title = {{SynthID}},
	url = {https://deepmind.google/technologies/synthid/},
	abstract = {Robust and scalable tool for watermarking and identifying AI-generated images.},
	language = {en},
	urldate = {2024-02-22},
	journal = {Google DeepMind},
	month = nov,
	year = {2023},
}

@misc{published_ais_2023,
	title = {{AI}’s {Dreadful} {December}: {Lawsuits}, plagiarism and child abuse images show the perils of training on data taken without consent.},
	shorttitle = {{AI}’s {Dreadful} {December}},
	url = {https://www.tomshardware.com/tech-industry/artificial-intelligence/ai-dreadful-december-shows-flaws-of-taking-data-without-consent},
	abstract = {A NY Times lawsuit claiming copyright infringement is just the latest black eye for AI software.},
	language = {en},
	urldate = {2024-03-13},
	journal = {Tom's Hardware},
	author = {published, Avram Piltch},
	month = dec,
	year = {2023},
}

@misc{updated_google_2023,
	title = {Google {Bard} {Plagiarized} {Our} {Article}, {Then} {Apologized} {When} {Caught}},
	url = {https://www.tomshardware.com/news/google-bard-plagiarizing-article},
	abstract = {The chatbot implied that it had conducted its own CPU tests.},
	language = {en},
	urldate = {2024-03-13},
	journal = {Tom's Hardware},
	author = {updated, Avram Piltch last},
	month = mar,
	year = {2023},
	keywords = {plagiarism},
}

@inproceedings{hayes_generating_2017,
	title = {Generating steganographic images via adversarial training},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/fe2d010308a6b3799a3d9c728ee74244-Abstract.html},
	abstract = {Adversarial training has proved to be competitive against supervised learning methods on computer vision tasks. However, studies have mainly been confined to generative tasks such as image synthesis. In this paper, we apply adversarial training techniques to the discriminative task of learning a steganographic algorithm. Steganography is a collection of techniques for concealing the existence of information by embedding it within a non-secret medium, such as cover texts or images. We show that adversarial training can produce robust steganographic techniques: our unsupervised training scheme produces a steganographic algorithm that competes with state-of-the-art steganographic techniques. We also show that supervised training of our adversarial model produces a robust steganalyzer, which performs the discriminative task of deciding if an image contains secret information. We define a game between three parties, Alice, Bob and Eve, in order to simultaneously train both a steganographic algorithm and a steganalyzer. Alice and Bob attempt to communicate a secret message contained within an image, while Eve eavesdrops on their conversation and attempts to determine if secret information is embedded within the image. We represent Alice, Bob and Eve by neural networks, and validate our scheme on two independent image datasets, showing our novel method of studying steganographic problems is surprisingly competitive against established steganographic techniques.},
	urldate = {2024-02-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Hayes, Jamie and Danezis, George},
	year = {2017},
	keywords = {gans, training data ownership, watermarking},
}

@misc{noauthor_spawningai_nodate,
	title = {Spawning.ai},
	url = {https://www.spawning.ai},
	abstract = {We believe that a future of consenting data will benefit both AI development and the people it is trained on.},
	language = {en},
	urldate = {2023-07-21},
	journal = {Spawning.ai},
	keywords = {copyright protection by design, ethical GenAI, protection, protection of training data, website},
}

@techreport{kitchenham_guidelines_2007,
	title = {Guidelines for performing {Systematic} {Literature} {Reviews} in {Software} {Engineering}},
	number = {EBSE-2007-01},
	institution = {Department of Computer Science, University of Durham, Durham, UK},
	author = {Kitchenham, Barbara and Charters, Stuart},
	year = {2007},
}

@misc{herndon_holly_nodate,
	title = {Holly+},
	url = {https://holly.mirror.xyz/54ds2IiOnvthjGFkokFCoaI4EabytH9xjAYy1irHy94},
	urldate = {2023-07-21},
	author = {Herndon, Holly},
	note = {https://holly.mirror.xyz/54ds2IiOnvthjGFkokFCoaI4EabytH9xjAYy1irHy94},
	keywords = {ethical GenAI, protection, website},
}

@inproceedings{hu_radar_2023,
	title = {{RADAR}: {Robust} {AI}-{Text} {Detection} via {Adversarial} {Learning}},
	copyright = {© Copyright IBM Corp. 2021},
	shorttitle = {{RADAR}},
	url = {https://research.ibm.com/publications/radar-robust-ai-text-detection-via-adversarial-learning},
	abstract = {RADAR: Robust AI-Text Detection via Adversarial Learning for NeurIPS 2023 by Xiaomeng Hu et al.},
	language = {en-US},
	urldate = {2023-12-05},
	author = {Hu, Xiaomeng and Chen, Pin-Yu and Ho, Tsung-Yi},
	month = dec,
	year = {2023},
	keywords = {LLMs, detection of generated content, protection},
}

@article{weber-wulff_testing_2023,
	title = {Testing of detection tools for {AI}-generated text},
	volume = {19},
	issn = {1833-2595},
	url = {https://doi.org/10.1007/s40979-023-00146-z},
	doi = {10.1007/s40979-023-00146-z},
	abstract = {Recent advances in generative pre-trained transformer large language models have emphasised the potential risks of unfair use of artificial intelligence (AI) generated content in an academic environment and intensified efforts in searching for solutions to detect such content. The paper examines the general functionality of detection tools for AI-generated text and evaluates them based on accuracy and error type analysis. Specifically, the study seeks to answer research questions about whether existing detection tools can reliably differentiate between human-written text and ChatGPT-generated text, and whether machine translation and content obfuscation techniques affect the detection of AI-generated text. The research covers 12 publicly available tools and two commercial systems (Turnitin and PlagiarismCheck) that are widely used in the academic setting. The researchers conclude that the available detection tools are neither accurate nor reliable and have a main bias towards classifying the output as human-written rather than detecting AI-generated text. Furthermore, content obfuscation techniques significantly worsen the performance of tools. The study makes several significant contributions. First, it summarises up-to-date similar scientific and non-scientific efforts in the field. Second, it presents the result of one of the most comprehensive tests conducted so far, based on a rigorous research methodology, an original document set, and a broad coverage of tools. Third, it discusses the implications and drawbacks of using detection tools for AI-generated text in academic settings.},
	language = {en},
	number = {1},
	urldate = {2024-02-22},
	journal = {International Journal for Educational Integrity},
	author = {Weber-Wulff, Debora and Anohina-Naumeca, Alla and Bjelobaba, Sonja and Foltýnek, Tomáš and Guerrero-Dib, Jean and Popoola, Olumide and Šigut, Petr and Waddington, Lorna},
	month = dec,
	year = {2023},
	keywords = {AI detectors, Academic integrity, Artificial intelligence, ChatGPT, Detection of AI-generated text, Generative pre-trained transformers, Machine-generated text},
	pages = {26},
}

@inproceedings{zhang_udh_2020,
	title = {{UDH}: {Universal} {Deep} {Hiding} for {Steganography}, {Watermarking}, and {Light} {Field} {Messaging}},
	volume = {33},
	shorttitle = {{UDH}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/73d02e4344f71a0b0d51a925246990e7-Abstract.html},
	urldate = {2024-02-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Chaoning and Benz, Philipp and Karjauv, Adil and Sun, Geng and Kweon, In So},
	year = {2020},
	pages = {10223--10234},
}

@misc{noauthor_generative_nodate,
	title = {Generative {AI} at the {BBC}},
	url = {https://www.bbc.co.uk/mediacentre/articles/2023/generative-ai-at-the-bbc/},
	abstract = {Rhodri Talfan Davies, the BBC’s Director of Nations, sets out the latest on our plans},
	language = {en},
	urldate = {2024-02-20},
}

@inproceedings{leotta_not_2023,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Not with {My} {Name}! {Inferring} {Artists}’ {Names} of {Input} {Strings} {Employed} by {Diffusion} {Models}},
	isbn = {978-3-031-43148-7},
	doi = {10.1007/978-3-031-43148-7_31},
	abstract = {Diffusion Models (DM) are highly effective at generating realistic, high-quality images. However, these models lack creativity and merely compose outputs based on their training data, guided by a textual input provided at creation time. Is it acceptable to generate images reminiscent of an artist, employing his name as input? This imply that if the DM is able to replicate an artist’s work then it was trained on some or all of his artworks thus violating copyright. In this paper, a preliminary study to infer the probability of use of an artist’s name in the input string of a generated image is presented. To this aim we focused only on images generated by the famous DALL-E 2 and collected images (both original and generated) of five renowned artists. Finally, a dedicated Siamese Neural Network was employed to have a first kind of probability. Experimental results demonstrate that our approach is an optimal starting point and can be employed as a prior for predicting a complete input string of an investigated image. Dataset and code are available at: https://github.com/ictlab-unict/not-with-my-name.},
	language = {en},
	booktitle = {Image {Analysis} and {Processing} – {ICIAP} 2023},
	publisher = {Springer Nature Switzerland},
	author = {Leotta, Roberto and Giudice, Oliver and Guarnera, Luca and Battiato, Sebastiano},
	editor = {Foresti, Gian Luca and Fusiello, Andrea and Hancock, Edwin},
	year = {2023},
	keywords = {Artist Recognition, Diffusion Models, Multimedia Forensics},
	pages = {364--375},
}

@inproceedings{chen_gan-leaks_2020,
	address = {New York, NY, USA},
	series = {{CCS} '20},
	title = {{GAN}-{Leaks}: {A} {Taxonomy} of {Membership} {Inference} {Attacks} against {Generative} {Models}},
	isbn = {978-1-4503-7089-9},
	shorttitle = {{GAN}-{Leaks}},
	url = {https://doi.org/10.1145/3372297.3417238},
	doi = {10.1145/3372297.3417238},
	abstract = {Deep learning has achieved overwhelming success, spanning from discriminative models to generative models. In particular, deep generative models have facilitated a new level of performance in a myriad of areas, ranging from media manipulation to sanitized dataset generation. Despite the great success, the potential risks of privacy breach caused by generative models have not been analyzed systematically. In this paper, we focus on membership inference attack against deep generative models that reveals information about the training data used for victim models. Specifically, we present the first taxonomy of membership inference attacks, encompassing not only existing attacks but also our novel ones. In addition, we propose the first generic attack model that can be instantiated in a large range of settings and is applicable to various kinds of deep generative models. Moreover, we provide a theoretically grounded attack calibration technique, which consistently boosts the attack performance in all cases, across different attack settings, data modalities, and training configurations. We complement the systematic analysis of attack performance by a comprehensive experimental study, that investigates the effectiveness of various attacks w.r.t. model type and training configurations, over three diverse application scenarios (i.e., images, medical data, and location data).},
	urldate = {2024-02-19},
	booktitle = {Proceedings of the 2020 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Dingfan and Yu, Ning and Zhang, Yang and Fritz, Mario},
	month = nov,
	year = {2020},
	keywords = {deep learning, generative models, machine learning, membership inference, membership inference attacks, privacy-preserving},
	pages = {343--362},
}

@article{hilprecht_monte_2019,
	title = {Monte {Carlo} and {Reconstruction} {Membership} {Inference} {Attacks} against {Generative} {Models}},
	volume = {2019},
	issn = {2299-0984},
	url = {https://petsymposium.org/popets/2019/popets-2019-0067.php},
	doi = {10.2478/popets-2019-0067},
	abstract = {We present two information leakage attacks that outperform previous work on membership inference against generative models. The ﬁrst attack allows membership inference without assumptions on the type of the generative model. Contrary to previous evaluation metrics for generative models, like Kernel Density Estimation, it only considers samples of the model which are close to training data records. The second attack speciﬁcally targets Variational Autoencoders, achieving high membership inference accuracy. Furthermore, previous work mostly considers membership inference adversaries who perform single record membership inference. We argue for considering regulatory actors who perform set membership inference to identify the use of speciﬁc datasets for training. The attacks are evaluated on two generative model architectures, Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), trained on standard image datasets. Our results show that the two attacks yield success rates superior to previous work on most data sets while at the same time having only very mild assumptions. We envision the two attacks in combination with the membership inference attack type formalization as especially useful. For example, to enforce data privacy standards and automatically assessing model quality in machine learning as a service setups. In practice, our work motivates the use of GANs since they prove less vulnerable against information leakage attacks while producing detailed samples.},
	language = {en},
	number = {4},
	urldate = {2024-02-19},
	journal = {Proceedings on Privacy Enhancing Technologies},
	author = {Hilprecht, Benjamin and Härterich, Martin and Bernau, Daniel},
	month = oct,
	year = {2019},
	pages = {232--249},
}

@article{solaiman_release_nodate,
	title = {Release {Strategies} and the {Social} {Impacts} of {Language} {Models}},
	language = {en},
	author = {Solaiman, Irene and Brundage, Miles and Clark, Jack and Askell, Amanda and Herbert-Voss, Ariel and Wu, Jeff and Radford, Alec and Krueger, Gretchen and Kim, Jong Wook and Kreps, Sarah and McCain, Miles and Newhouse, Alex and Blazakis, Jason and McGuffie, Kris and Wang, Jasmine},
}

@misc{sohl-dickstein_deep_2015,
	title = {Deep {Unsupervised} {Learning} using {Nonequilibrium} {Thermodynamics}},
	url = {http://arxiv.org/abs/1503.03585},
	doi = {10.48550/arXiv.1503.03585},
	abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
	urldate = {2024-02-19},
	publisher = {arXiv},
	author = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
	month = nov,
	year = {2015},
	note = {arXiv:1503.03585 [cond-mat, q-bio, stat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@misc{noauthor_meine_nodate,
	title = {Meine {Wien} {Energie}},
	url = {https://meine.wienenergie.at/privat},
	urldate = {2024-02-10},
}

@article{sunray_train_2021,
	title = {Train in {Vain}: {A} {Theoretical} {Assessment} of {Intermediate} {Copying} and {Fair} {Use} in {Machine} {AI} {Music} {Generator} {Training}},
	volume = {13},
	shorttitle = {Train in {Vain}},
	url = {https://heinonline.org/HOL/Page?handle=hein.journals/inprobr13&id=9&div=&collection=},
	journal = {American University Intellectual Property Brief},
	author = {Sunray, Eric},
	year = {2021},
	pages = {1},
}

@article{sunray_sounds_2020,
	title = {Sounds of {Science}: {Copyright} {Infringement} in {AI} {Music} {Generator} {Outputs}},
	volume = {29},
	shorttitle = {Sounds of {Science}},
	url = {https://heinonline.org/HOL/Page?handle=hein.journals/cconsp29&id=428&div=&collection=},
	journal = {Catholic University Journal of Law and Technology},
	author = {Sunray, Eric},
	year = {2020},
	pages = {185},
}

@inproceedings{yang_defending_2021,
	title = {Defending against {GAN}-based {DeepFake} {Attacks} via {Transformation}-aware {Adversarial} {Faces}},
	url = {https://ieeexplore.ieee.org/document/9533868},
	doi = {10.1109/IJCNN52387.2021.9533868},
	abstract = {DeepFake represents a category of face-swapping attacks that leverage machine learning models such as autoen-coders or generative adversarial networks. Although the concept of the face-swapping is not new, its recent technical advances make fake content (e.g., images, videos) imperceptible to Humans. Various detection techniques for DeepFake attacks have been explored. These methods, however, are passive measures against DeepFakes as they are mitigation strategies after the high-quality fake content is generated. This work aims to take an offensive measure to impede the generation of high-quality fake images or videos. Specifically, we propose to use novel transformation-aware adversarially perturbed faces as a defense against GAN-based DeepFake attacks, which leverages differentiable random image transformations during the generation. We also propose an ensemble-based approach to enhance the defense robustness against GAN-based DeepFake variants under the black-box setting. We show that training a DeepFake model with adversarial faces can lead to a significant degradation in the quality of synthesized faces.},
	urldate = {2024-01-10},
	booktitle = {2021 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Yang, Chaofei and Ding, Leah and Chen, Yiran and Li, Hai},
	month = jul,
	year = {2021},
	note = {ISSN: 2161-4407},
	pages = {1--8},
}

@misc{noauthor_fresh_nodate,
	title = {A fresh perspective on machine unlearning, with a real-world solution! {\textbar} by {Ali} {Borji} {\textbar} {Medium}},
	url = {https://medium.com/@aliborji/a-fresh-perspective-on-machine-unlearning-with-a-real-world-solution-203821dd01c0},
	urldate = {2024-01-09},
	keywords = {machine unlearning},
}

@inproceedings{noauthor_machine_2023,
	title = {Machine {Unlearning} for {Image}-to-{Image} {Generative} {Models}},
	url = {https://openreview.net/forum?id=9hjVoPWPnh},
	abstract = {Machine unlearning has emerged as a new paradigm to deliberately forget data samples from a given model in order to adhere to stringent regulations. However, existing machine unlearning methods have been primarily focused on classification models, leaving the landscape of unlearning for generative models relatively unexplored. This paper serves as a bridge, addressing the gap by providing a unifying framework of machine unlearning for image-to-image generative models. Within this framework, we propose a computationally-efficient algorithm, underpinned by rigorous theoretical analysis, that demonstrates negligible performance degradation on the retain samples, while effectively removing the information from the forget samples. Empirical studies on two large-scale datasets, ImageNet-1K and Places-365, further show that our algorithm does not rely on the availability of the retain samples, which further complies with data retention policy. To our best knowledge, this work is the first that represents systemic, theoretical, empirical explorations of machine unlearning specifically tailored for image-to-image generative models.},
	language = {en},
	urldate = {2024-01-09},
	month = oct,
	year = {2023},
	keywords = {machine unlearning},
}

@misc{noauthor_imagen_nodate,
	title = {Imagen on {Vertex} {AI} {\textbar} {AI} {Image} {Generator}},
	url = {https://cloud.google.com/vertex-ai/docs/generative-ai/image/overview},
	abstract = {Explore Imagen on Vertex AI, a text-to-image generator that brings Google's image generation AI capabilities to application developers.},
	language = {en},
	urldate = {2024-01-09},
	journal = {Google Cloud},
}

@misc{noauthor_stable_2023,
	title = {The {Stable} {Signature}: {Rooting} {Watermarks} in {Latent} {Diffusion} {Models}},
	shorttitle = {The {Stable} {Signature}},
	url = {https://pierrefdz.github.io/publications/stablesignature/},
	abstract = {TL;DR (Summary)},
	urldate = {2024-01-09},
	journal = {Pierre Fernandez},
	month = mar,
	year = {2023},
}

@article{knibbs_researchers_nodate,
	title = {Researchers {Tested} {AI} {Watermarks}—and {Broke} {All} of {Them}},
	issn = {1059-1028},
	url = {https://www.wired.com/story/artificial-intelligence-watermarking-issues/},
	abstract = {A research team found it's easy to evade current methods of watermarking—and even add fake watermarks to real images.},
	language = {en-US},
	urldate = {2024-01-09},
	journal = {Wired},
	author = {Knibbs, Kate},
	note = {Section: tags},
	keywords = {artificial intelligence, deepmind, openai},
}

@misc{kan_painter_nodate,
	title = {Painter by {Numbers}},
	url = {https://kaggle.com/competitions/painter-by-numbers},
	abstract = {Does every painter leave a fingerprint?},
	language = {en},
	urldate = {2023-12-11},
	author = {Kan, Wendy},
	keywords = {dataset},
}

@misc{noauthor_web_nodate,
	title = {Web scraping is legal, {US} appeals court reaffirms {\textbar} {TechCrunch}},
	url = {https://techcrunch.com/2022/04/18/web-scraping-legal-court/?guccounter=1},
	urldate = {2023-07-21},
	keywords = {blog},
}

@misc{stokel-walker_this_2022,
	title = {This couple is launching an organization to protect artists in the {AI} era},
	url = {https://www.inverse.com/input/culture/mat-dryhurst-holly-herndon-artists-ai-spawning-source-dall-e-midjourney},
	abstract = {Mat Dryhurst \& Holly Herndon want creatives to be able to opt into or out of having their work used as training data for DALL-E and the like.},
	language = {en},
	urldate = {2023-07-21},
	journal = {Input},
	author = {Stokel-Walker, Chris},
	month = sep,
	year = {2022},
	keywords = {blog},
}

@inproceedings{foley_matching_2023,
	title = {Matching {Pairs}: {Attributing} {Fine}-{Tuned} {Models} to their {Pre}-{Trained} {Large} {Language} {Models}},
	copyright = {© Copyright IBM Corp. 2021},
	shorttitle = {Matching {Pairs}},
	url = {https://research.ibm.com/publications/matching-pairs-attributing-fine-tuned-models-to-their-pre-trained-large-language-models},
	abstract = {Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models for ACL 2023 by Myles Foley et al.},
	language = {en-US},
	urldate = {2023-12-05},
	author = {Foley, Myles and Rawat, Ambrish and Lee, Taesung and Hou, Yufang and Picco, Gabriele and Zizzo, Giulio},
	month = jul,
	year = {2023},
	keywords = {LLMs, Model copyright, protection},
}

@article{baumhauer_machine_2022,
	title = {Machine unlearning: linear filtration for logit-based classifiers},
	volume = {111},
	issn = {1573-0565},
	shorttitle = {Machine unlearning},
	url = {https://doi.org/10.1007/s10994-022-06178-9},
	doi = {10.1007/s10994-022-06178-9},
	abstract = {Recently enacted legislation grants individuals certain rights to decide in what fashion their personal data may be used and in particular a “right to be forgotten”. This poses a challenge to machine learning: how to proceed when an individual retracts permission to use data which has been part of the training process of a model? From this question emerges the field of machine unlearning, which could be broadly described as the investigation of how to “delete training data from models”. Our work complements this direction of research for the specific setting of class-wide deletion requests for classification models (e.g. deep neural networks). As a first step, we propose linear filtration as an intuitive, computationally efficient sanitization method. Our experiments demonstrate benefits in an adversarial setting over naive deletion schemes.},
	language = {en},
	number = {9},
	urldate = {2023-11-28},
	journal = {Machine Learning},
	author = {Baumhauer, Thomas and Schöttle, Pascal and Zeppelzauer, Matthias},
	month = sep,
	year = {2022},
	keywords = {DNN, Machine learning, Machine unlearning, Privacy, image classification},
	pages = {3203--3226},
}

@misc{noauthor_machine_nodate,
	title = {Machine {Unlearning}},
	url = {https://ieeexplore.ieee.org/abstract/document/9519428/},
	abstract = {Once users have shared their data online, it is generally difficult for them to revoke access and ask for the data to be deleted. Machine learning (ML) exacerbates this problem because any model trained with said data may have memorized it, putting users at risk of a successful privacy attack exposing their information. Yet, having models unlearn is notoriously difficult.We introduce SISA training, a framework that expedites the unlearning process by strategically limiting the influence of a data point in the training procedure. While our framework is applicable to any learning algorithm, it is designed to achieve the largest improvements for stateful algorithms like stochastic gradient descent for deep neural networks. SISA training reduces the computational overhead associated with unlearning, even in the worst-case setting where unlearning requests are made uniformly across the training set. In some cases, the service provider may have a prior on the distribution of unlearning requests that will be issued by users. We may take this prior into account to partition and order data accordingly, and further decrease overhead from unlearning.Our evaluation spans several datasets from different domains, with corresponding motivations for unlearning. Under no distributional assumptions, for simple learning tasks, we observe that SISA training improves time to unlearn points from the Purchase dataset by 4.63×, and 2.45× for the SVHN dataset, over retraining from scratch. SISA training also provides a speed-up of 1.36× in retraining for complex learning tasks such as ImageNet classification; aided by transfer learning, this results in a small degradation in accuracy. Our work contributes to practical data governance in machine unlearning.},
	language = {en-US},
	urldate = {2023-07-23},
	keywords = {DNN, Machine unlearning, image classification},
}

@article{schuhmann_laion-5b_2022,
	title = {{LAION}-{5B}: {An} open large-scale dataset for training next generation image-text models},
	volume = {35},
	shorttitle = {{LAION}-{5B}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/a1859debfb3b59d094f3504d5ebb6c25-Abstract-Datasets_and_Benchmarks.html},
	language = {en},
	urldate = {2023-07-21},
	journal = {Advances in Neural Information Processing Systems},
	author = {Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and Schramowski, Patrick and Kundurthy, Srivatsa and Crowson, Katherine and Schmidt, Ludwig and Kaczmarczyk, Robert and Jitsev, Jenia},
	month = dec,
	year = {2022},
	keywords = {dataset, paper},
	pages = {25278--25294},
}

@misc{noauthor_laion-5b_nodate,
	title = {{LAION}-{5B}: {A} {NEW} {ERA} {OF} {OPEN} {LARGE}-{SCALE} {MULTI}-{MODAL} {DATASETS} {\textbar} {LAION}},
	shorttitle = {{LAION}-{5B}},
	url = {https://laion.ai/blog/laion-5b},
	abstract = {{\textless}p{\textgreater}We present a dataset of 5,85 billion CLIP-filtered image-text pairs, 14x bigger than LAION-400M, previously the biggest openly accessible image-text datas...},
	language = {en},
	urldate = {2023-07-21},
	keywords = {dataset, website},
}

@misc{noauthor_have_nodate,
	title = {Have {I} {Been} {Trained}?},
	url = {https://haveibeentrained.com/},
	urldate = {2023-07-22},
	keywords = {detection of participation, detection of similarity, protection, website},
}

@inproceedings{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Nets}},
	volume = {27},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html},
	abstract = {We propose a new framework for estimating generative models via adversarial nets, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitatively evaluation of the generated samples.},
	urldate = {2023-07-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	year = {2014},
	keywords = {GANs},
}

@inproceedings{shan_fawkes_2020,
	title = {Fawkes: {Protecting} {Privacy} against {Unauthorized} {Deep} {Learning} {Models}},
	isbn = {978-1-939133-17-5},
	shorttitle = {Fawkes},
	url = {https://www.usenix.org/conference/usenixsecurity20/presentation/shan},
	language = {en},
	urldate = {2023-07-20},
	author = {Shan, Shawn and Wenger, Emily and Zhang, Jiayun and Li, Huiying and Zheng, Haitao and Zhao, Ben Y.},
	year = {2020},
	keywords = {Privacy, Training data copyright, cloaking, potential application for IPinGenAI, protection},
	pages = {1589--1604},
}

@inproceedings{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
	urldate = {2023-07-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	year = {2020},
	keywords = {diffusion},
	pages = {6840--6851},
}

@article{franceschelli_copyright_2022,
	title = {Copyright in generative deep learning},
	volume = {4},
	issn = {2632-3249},
	url = {https://www.cambridge.org/core/journals/data-and-policy/article/copyright-in-generative-deep-learning/C401539FDF79A6AC6CEE8C5256508B5E},
	doi = {10.1017/dap.2022.10},
	abstract = {Machine-generated artworks are now part of the contemporary art scene: they are attracting significant investments and they are presented in exhibitions together with those created by human artists. These artworks are mainly based on generative deep learning (GDL) techniques, which have seen a formidable development and remarkable refinement in the very recent years. Given the inherent characteristics of these techniques, a series of novel legal problems arise. In this article, we consider a set of key questions in the area of GDL for the arts, including the following: is it possible to use copyrighted works as training set for generative models? How do we legally store their copies in order to perform the training process? Who (if someone) will own the copyright on the generated data? We try to answer these questions considering the law in force in both the United States and the European Union, and potential future alternatives. We then extend our analysis to code generation, which is an emerging area of GDL. Finally, we also formulate a set of practical guidelines for artists and developers working on deep learning generated art, as well as some policy suggestions for policymakers.},
	language = {en},
	urldate = {2023-12-06},
	journal = {Data \& Policy},
	author = {Franceschelli, Giorgio and Musolesi, Mirco},
	month = jan,
	year = {2022},
	note = {Publisher: Cambridge University Press},
	keywords = {deep learning, guidelines, intellectual property, legal, machine-generated artworks, societal},
	pages = {e17},
}

@misc{noauthor_content_nodate,
	title = {Content {Authenticity} {Initiative}},
	url = {https://contentauthenticity.org},
	abstract = {Creating the standard for digital content provenance.},
	language = {en-US},
	urldate = {2023-07-21},
	journal = {Content Authenticity Initiative},
	keywords = {attribution, protection, provenance, website},
}

@article{strowel_chatgpt_2023,
	title = {{ChatGPT} and {Generative} {AI} {Tools}: {Theft} of {Intellectual} {Labor}?},
	volume = {54},
	issn = {2195-0237},
	shorttitle = {{ChatGPT} and {Generative} {AI} {Tools}},
	url = {https://doi.org/10.1007/s40319-023-01321-y},
	doi = {10.1007/s40319-023-01321-y},
	language = {en},
	number = {4},
	urldate = {2023-06-26},
	journal = {IIC - International Review of Intellectual Property and Competition Law},
	author = {Strowel, Alain},
	month = apr,
	year = {2023},
	keywords = {legal, societal},
	pages = {491--494},
}

@misc{noauthor_authors_2023,
	title = {\textit{{Authors} {Guild}, {Inc}. v. {Google}, {Inc}.}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Authors_Guild,_Inc._v._Google,_Inc.&oldid=1165284601},
	abstract = {Authors Guild v. Google 721 F.3d 132 (2nd Cir. 2015) was a copyright case heard in federal court for the Southern District of New York, and then the Second Circuit Court of Appeals between 2005 and 2015. It concerned fair use in copyright law and the transformation of printed copyrighted books into an online searchable database through scanning and digitization. It centered on the legality of the Google Book Search (originally named as Google Print) Library Partner project that had been launched in 2003.
Though there was general agreement that Google's attempt to digitize books through scanning and computer-aided recognition for searching online was seen as a transformative step for libraries, many authors and publishers had expressed concern that Google had not sought their permission to make scans of the books still under copyright and offered them to users. Two separate lawsuits, including one from three authors represented by the Authors Guild and another by Association of American Publishers, were filed in 2005 charging Google with copyright infringement. Google worked with the litigants in both suits to develop a settlement agreement (the Google Book Search Settlement Agreement) that would have allowed it to continue the program though paying out for works it had previously scanned, creating a revenue program for future books that were part of the search engine, and allowing authors and publishers to opt-out. The settlement received much criticism as it also applied to all books worldwide, included works that may have been out of print but still under copyright, and may have violated antitrust aspects given Google's dominant position within the Internet industry. A reworked proposal to address some of these concerns was met with similar criticism, and ultimately the settlement was rejected by 2011, allowing the two lawsuits to be joined for a combined trial.
In late 2013, after the class action status was challenged, the District Court granted summary judgement in favor of Google, dismissing the lawsuit and affirming the Google Books project met all legal requirements for fair use. The Second Circuit Court of Appeal upheld the District Court's summary judgement in October 2015, ruling Google's "project provides a public service without violating intellectual property law." The U.S. Supreme Court subsequently denied a petition to hear the case.},
	language = {en},
	urldate = {2023-07-21},
	journal = {Wikipedia},
	month = jul,
	year = {2023},
	note = {Page Version ID: 1165284601},
	keywords = {legal, violation},
}

@article{yu_artificial_2020,
	title = {Artificial {GAN} {Fingerprints}: {Rooting} {Deepfake} {Attribution} in {Training} {Data}},
	shorttitle = {Artificial {GAN} {Fingerprints}},
	url = {https://www.semanticscholar.org/paper/Artificial-GAN-Fingerprints%3A-Rooting-Deepfake-in-Yu-Skripniuk/365ce79f4cf6283367387c1a93b57e5272e834d0},
	abstract = {Photorealistic image generation is progressing rapidly and has reached a new level of quality, thanks to the invention and breakthroughs of generative adversarial networks (GANs). Yet the dark side of such deepfakes, the malicious use of generated media, never stops raising concerns of visual misinformation. Existing research works on deepfake detection demonstrate impressive accuracy, while it is accompanied by adversarial iterations on detection countermeasure techniques. In order to lead this arms race to the end, we investigate a fundamental solution on deepfake detection, agnostic to the evolution of GANs in order to enable a responsible disclosure or regulation of such double-edged techniques. We propose to embed artificial fingerprints into GAN training data, and show a surprising discovery on the transferability of such fingerprints from training data to GAN models, which in turn enables reliable detection and attribution of deepfakes. Our empirical study shows that our fingerprinting technique (1) holds for different state-of-the-art GAN configurations, (2) turns more effective along with the development of GAN techniques, (3) has a negligible side effect on the generation quality, and (4) stays robust against image-level and model-level perturbations. When we allocate each GAN publisher a unique artificial fingerprint, the margins between real data and deepfakes, and the margins among different deepfake sources are fundamentally guaranteed. As a result, we are able to evidence accurate deepfake detection/attribution using our fingerprint decoder, which makes this solution stand out from the current arms race.},
	urldate = {2023-12-06},
	journal = {arXiv: Cryptography and Security},
	author = {Yu, Ning and Skripniuk, Vladislav and Abdelnabi, Sahar and Fritz, Mario},
	month = jul,
	year = {2020},
	keywords = {GANs, detection of generated content, fingerprint, paper, protection, watermark},
}

@misc{noauthor_ai-generated_nodate,
	title = {{AI}-{Generated} {Media} {Recognition}},
	url = {https://docs.thehive.ai/docs/ai-generated-media-recognition},
	abstract = {OverviewHive's AI-Generated Media Recognition API takes an input image and determines whether or not that image is entirely AI-generated. The model was trained on a large dataset comprising millions of artificially generated images and human-created images such as photographs, digital and traditiona...},
	language = {en},
	urldate = {2023-12-05},
	journal = {Documentation {\textbar} Hive},
	keywords = {detection of generated content, protection, website},
}

@misc{murray_generative_2023,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Generative {AI} {Art}: {Copyright} {Infringement} and {Fair} {Use}},
	shorttitle = {Generative {AI} {Art}},
	url = {https://papers.ssrn.com/abstract=4483539},
	doi = {10.2139/ssrn.4483539},
	abstract = {Generative AI Art: Copyright Infringement and Fair Use, SMU SCIENCE \& TECHNOLOGY LAW REVIEW, vol. 26-2 (forthcoming, 2023).-------------------------------------------------------------------------------------------------------------The discussion of AI copyright infringement or fair use often skips over all of the required steps of the infringement analysis in order to focus on the most intriguing question, “Could a visual generative AI generate a work that potentially infringes a preexisting copyrighted work?” and then the discussion skips further ahead to, “Would the AI have a fair use defense, most likely under the transformative test?”  These are relevant questions, but in isolation from the actual steps of the copyright infringement analysis, the discussion is misleading or even irrelevant. This skipping of topics and stages of the infringement analysis does not train our attention to a properly accused party or entity whose actions prompt the question. The leaping from a question of infringement in the creation of training datasets to the creation of foundation models that draw from the training data to the actual operation of the generative AI system to produce images makes a false equivalency regarding the processes themselves and the persons responsible for them. The questions ought to shift focus from the persons compiling the training dataset used to train the AI system and the designers and creators of the AI system itself to the end users of the AI system who actually conceive of and cause the creation of images. The analysis of infringement or fair use in the generative AI context has suffered from widespread misunderstanding concerning the generative AI processes and the control and authorship of the end-user. Claimants, commentators, and regulators have made incorrect assumptions and inaccurate simplifications concerning the process, which I refer to as the Magic File Drawer theory, the Magic Copy Machine theory, and the Magic Box Artist theory. These theories, if they were true, would be much easier to envision and understand than the actual science and technology that goes into the creation and operation of a contemporary visual generative AI system. Throughout this Article, I will attempt to clarify and correct the understanding of the science and technology of the generative AI processes and explain the different roles of the training dataset designers, the generative AI system designers, and the end-users in the rendering of visual works by a generative AI system. Part II will discuss the requirements of a claim of copyright infringement including each step from the copyrightability of the claimant’s work, the doctrines that limit copyrightability, the requirement of an act of copying, and the infringement elements. Part III will summarize the copyright fair use test paying particular attention to the purpose and character of the use analysis, 17 U.S.C. § 107(1), and the current interpretation of the “transformative” test after Andy Warhol Foundation v. Goldsmith, particularly in circumstances relating to technology and the use of copyrighted or copyrightable data sources. Part IV will analyze potential infringement or fair use by the creators of generative AI training datasets. Part V will analyze potential infringement or fair use by the creators of visual generative AI systems. Part VI will analyze potential infringement or fair use by the end-users of visual generative AI systems.For all their complexity, visual generative AI systems are tools that depend on an end-user who conceives of and designs the image and provides the system with a prompt to set the generative process in motion. The end-users are responsible for crafting the prompt or series of prompts used, for evaluating the outputs of the generative AI, for adjusting and editing the iterations of images offered by the AI system, and ultimately for selecting and adopting one of the images generated by the AI as the final image. The end-users then make further decisions about the actual use and its function and purpose for the images the end-users selected and adopted from the outputs of the AI. In the course of working with the AI tool to try to produce a certain image, an end-user might steer the system to produce a work that could, under an infringement analysis, be regarded as potentially infringing, which would lead us again to the fair use analysis based on the end-user’s use of the image.},
	language = {en},
	urldate = {2023-12-06},
	author = {Murray, Michael D.},
	month = aug,
	year = {2023},
	keywords = {AI, artificial intelligence, copyright, derivative work, diffusion, fair use, foundation model, generative AI, generative pretrained transformer, infringement, latent space, legal, machine learning, prompt engineering, training data, transformative},
}

@inproceedings{tao_galip_2023,
	title = {{GALIP}: {Generative} {Adversarial} {CLIPs} for {Text}-to-{Image} {Synthesis}},
	shorttitle = {{GALIP}},
	url = {https://ieeexplore.ieee.org/document/10203358},
	doi = {10.1109/CVPR52729.2023.01366},
	abstract = {Synthesizing high-fidelity complex images from text is challenging. Based on large pretraining, the autoregressive and diffusion models can synthesize photo-realistic images. Although these large models have shown notable progress, there remain three flaws. 1) These models require tremendous training data and parameters to achieve good performance. 2) The multi-step generation design slows the image synthesis process heavily. 3) The synthesized visual features are challenging to control and require delicately designed prompts. To enable high-quality, efficient, fast, and controllable text-to-image synthesis, we propose Generative Adversarial CLIPs, namely GALIP. GALIP leverages the powerful pretrained CLIP model both in the discriminator and generator. Specifically, we propose a CLIP-based discriminator. The complex scene understanding ability of CLIP enables the discriminator to accurately assess the image quality. Furthermore, we propose a CLIP-empowered generator that induces the visual concepts from CLIP through bridge features and prompts. The CLIP-integrated generator and discriminator boost training efficiency, and as a result, our model only requires about 3\% training data and 6\% learnable parameters, achieving comparable results to large pretrained autoregressive and diffusion models. Moreover, our model achieves 120×faster synthesis speed and inherits the smooth latent space from GAN. The extensive experimental results demonstrate the excellent performance of our GALIP. Code is available at https://github.com/tobran/GALIP.},
	urldate = {2023-12-06},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Tao, Ming and Bao, Bing-Kun and Tang, Hao and Xu, Changsheng},
	month = jun,
	year = {2023},
	note = {ISSN: 2575-7075},
	pages = {14214--14223},
}

@misc{noauthor_neurips_nodate,
	title = {{NeurIPS} 2023 {Machine} {Unlearning} {Challenge}},
	url = {https://unlearning-challenge.github.io/unlearning-challenge.github.io/},
	abstract = {Website for the NeurIPS 2023 Machine Unlearning Challenge.},
	language = {en},
	urldate = {2023-12-05},
	journal = {NeurIPS 2023 Machine Unlearning Challenge},
}

@misc{noauthor_what_2021,
	title = {What is {AI} forensics?},
	copyright = {© Copyright IBM Corp. 2021},
	url = {https://research.ibm.com/blog/AI-forensics-attribution},
	abstract = {IBM researchers are developing AI-text detection and attribution tools to make generative AI more transparent and trustworthy.},
	language = {en-US},
	urldate = {2023-12-05},
	journal = {IBM Research Blog},
	month = feb,
	year = {2021},
	keywords = {blog},
}

@misc{wiggers_spawning_2023,
	title = {Spawning lays out plans for letting creators opt out of generative {AI} training},
	url = {https://techcrunch.com/2023/05/03/spawning-lays-out-its-plans-for-letting-creators-opt-out-of-generative-ai-training/},
	abstract = {Spawning AI, a startup creating tools to enable creators to opt out of generative AI training, has raised fresh capital -- and released a product roadmap.},
	language = {en-US},
	urldate = {2023-12-05},
	journal = {TechCrunch},
	author = {Wiggers, Kyle},
	month = may,
	year = {2023},
	keywords = {blog},
}

@misc{noauthor_neural_nodate,
	title = {Neural style transfer {\textbar} {TensorFlow} {Core}},
	url = {https://www.tensorflow.org/tutorials/generative/style_transfer},
	language = {en},
	urldate = {2023-12-05},
	journal = {TensorFlow},
	keywords = {GANs, style mimicry},
}

@inproceedings{sekhari_remember_2021,
	title = {Remember {What} {You} {Want} to {Forget}: {Algorithms} for {Machine} {Unlearning}},
	volume = {34},
	shorttitle = {Remember {What} {You} {Want} to {Forget}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/9627c45df543c816a3ddf2d8ea686a99-Abstract.html},
	urldate = {2023-07-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sekhari, Ayush and Acharya, Jayadev and Kamath, Gautam and Suresh, Ananda Theertha},
	year = {2021},
	keywords = {Machine unlearning},
	pages = {18075--18086},
}

@misc{noauthor_ayush_nodate,
	title = {Ayush {Sekhari}, {Jayadev} {Acharya}, {Gautam} {Kamath}, and {Ananda} {Theertha} {Suresh}. {Remember} what you want to forget: {Algorithms} for machine unlearning - {Google} {Search}},
	url = {https://www.google.com/search?q=Ayush+Sekhari%2C+Jayadev+Acharya%2C+Gautam+Kamath%2C+and+Ananda+Theertha+Suresh.+Remember+what+you+want+to+forget%3A+Algorithms+for+machine+unlearning&rlz=1C1GCEA_enAT1021AT1021&oq=Ayush+Sekhari%2C+Jayadev+Acharya%2C+Gautam+Kamath%2C+and+Ananda+Theertha+Suresh.+Remember+what+you+want+to+forget%3A+Algorithms+for+machine+unlearning&aqs=chrome..69i57.256j0j7&sourceid=chrome&ie=UTF-8},
	urldate = {2023-07-23},
	keywords = {Machine unlearning},
}

@misc{rodriguez_maffioli_copyright_2023,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Copyright in {Generative} {AI} training: {Balancing} {Fair} {Use} through {Standardization} and {Transparency}},
	shorttitle = {Copyright in {Generative} {AI} training},
	url = {https://papers.ssrn.com/abstract=4579322},
	doi = {10.2139/ssrn.4579322},
	abstract = {The rapid evolution of Generative Artificial Intelligence (GAI) has brought about transformative changes across industries, often raising challenging questions surrounding data rights, especially within the context of copyrighted content. This paper delves into the nuances of the relationship between GAI and the fair use doctrine, highlighting the complexities that emerge when copyrighted data serves as the backbone for the development of large-scale AI models. By combining Benjamin Sobel’s training data taxonomy with the distinct stages of the Generative AI cycle, a hybrid framework is presented, offering a more granulated perspective on the applicability of fair use in GAI contexts. Recognizing the inherent limitations of the current legal paradigms, the paper introduces actionable proposals, emphasizing the need for enhanced transparency, data provenance measures, and the implementation of Standardized Data Licensing Agreements (SDLAs). Such measures aim to bridge the gap between AI developers and copyright holders, facilitating smoother negotiations and fostering trust. While the core discussion revolves around the interplay of GAI and fair use, the paper acknowledges broader policy challenges in the AI domain, urging for continuous exploration. Overall, this work underscores the necessity of adaptive, collaborative, and transparent strategies in harmonizing the objectives of innovation with the imperatives of intellectual property rights in the GAI landscape.},
	language = {en},
	urldate = {2023-11-23},
	author = {Rodriguez Maffioli, Daniel},
	month = aug,
	year = {2023},
	keywords = {AI, AI training, artificial intelligence, copyright, data, data provenance, fair use, generative AI, generative artificial intelligence, scraping, standardization, transparency},
}

@article{dermawan_text_nodate,
	title = {Text and data mining exceptions in the development of generative {AI} models: {What} the {EU} member states could learn from the {Japanese} “nonenjoyment” purposes?},
	volume = {n/a},
	copyright = {© 2023 The Authors. The Journal of World Intellectual Property published by John Wiley \& Sons Ltd.},
	issn = {1747-1796},
	shorttitle = {Text and data mining exceptions in the development of generative {AI} models},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jwip.12285},
	doi = {10.1111/jwip.12285},
	abstract = {The European Union (EU) text and data mining (TDM) provisions are a progressive move, but the horizon is still uncertain for both generative artificial intelligence (GenAI) models researchers and developers. This article suggests that to drive innovation and further the commitment to the digital single market, during the national implementation, EU Member States could consider taking the Japanese broad, all-encompassing and “nonenjoyment-based” TDM as an example. The Japanese “nonenjoyment” purposes, however, are not foreign to the European continental view of copyright. A similar concept can be found under the German concept of “Freier Werkgenuss” or enjoyment of the work. A flexible TDM exception built upon the German notion of nonenjoyment purposes could become an opening clause to foster innovation and creativity in the age of GenAI. Moreover, the article argues that an opening clause allowing TDM with “nonenjoyment” purposes could be permissible under the so-called three-step test. This article further suggests, if there is no political will to safeguard “the right to read should be the right to mine” and to provide a welcoming environment for GenAI researchers and developers, when shaping the legal interpretation through national case law, the EU Member States could consider the following: (1) advocate for 72 h of response if technological protection measures (TPMs) are preventing TDM, and (2) Robot Exclusion Standard (robot.txt) as a warning when TDM is not allowed on a website. It is now in the hands of the EU Member States, whether to protect the interests of rightholders or to create a balance between safeguarding “the right to read should be the right to mine,” protecting rightholders exclusivity, and creating a supportive environment for the GenAI models researcher and developers.},
	language = {en},
	number = {n/a},
	urldate = {2023-11-23},
	journal = {The Journal of World Intellectual Property},
	author = {Dermawan, Artha},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jwip.12285},
	keywords = {copyright and related rights, freier werkgenuss, generative AI models, innovation, text and data mining},
}

@article{fui-hoon_nah_generative_2023,
	title = {Generative {AI} and {ChatGPT}: {Applications}, challenges, and {AI}-human collaboration},
	volume = {25},
	issn = {1522-8053},
	shorttitle = {Generative {AI} and {ChatGPT}},
	url = {https://doi.org/10.1080/15228053.2023.2233814},
	doi = {10.1080/15228053.2023.2233814},
	number = {3},
	urldate = {2023-11-23},
	journal = {Journal of Information Technology Case and Application Research},
	author = {Fui-Hoon Nah, Fiona and Zheng, Ruilin and Cai, Jingyuan and Siau, Keng and Chen, Langtao},
	month = jul,
	year = {2023},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/15228053.2023.2233814},
	pages = {277--304},
}

@misc{cheng_innovae_2022,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {{InnoVAE}: {Generative} {AI} for {Mapping} {Patents} and {Firm} {Innovation}},
	shorttitle = {{InnoVAE}},
	url = {https://papers.ssrn.com/abstract=3868599},
	doi = {10.2139/ssrn.3868599},
	abstract = {We propose a generative AI approach (InnoVAE) to map unstructured patent text into an interpretable, spatial representation of firms' innovative activities. InnoVAE learns a vector representation of patents and places them within a "disentangled" space to facilitate managerial intuition and action. After validating the internal consistency of our approach, we apply it to three decades of AI patents to show that it can be used to construct interpretable measures, at scale, that characterize firms' AI-based IP portfolios. We demonstrate three use cases including (1) generating technology landscapes that inform businesses about their competitive positions, (2) engineering new, intuitive features from unstructured text that facilitate analysis of patent activity, and (3) augmenting patent applications to mitigate the risk of patent rejection. Our approach demonstrates the potential of generative AI methods to make actionable the vast quantities of text stored in unstructured corporate databases.},
	language = {en},
	urldate = {2023-11-23},
	author = {Cheng, Zhaoqi and Lee, Dokyun and Tambe, Prasanna},
	month = mar,
	year = {2022},
	keywords = {ChatGPT, economics of innovation, generative AI, interpretability, large language models, machine learning, managerial decision support, patents},
}

@misc{noauthor_deviantart_nodate,
	title = {{DeviantArt} - {The} {Largest} {Online} {Art} {Gallery} and {Community}},
	url = {https://www.deviantart.com/},
	urldate = {2023-07-26},
}

@misc{noauthor_midjourney_nodate,
	title = {Midjourney},
	url = {https://www.midjourney.com/home/?callbackUrl=%2Fapp%2F},
	abstract = {An independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species.},
	urldate = {2023-07-26},
	journal = {Midjourney},
}

@misc{noauthor_musenet_nodate,
	title = {{MuseNet}},
	url = {https://openai.com/research/musenet},
	abstract = {We’ve created MuseNet, a deep neural network that can generate 4-minute musical compositions with 10 different instruments, and can combine styles from country to Mozart to the Beatles. MuseNet was not explicitly programmed with our understanding of music, but instead discovered patterns of harmony, rhythm, and style by learning to predict the next token in hundreds of thousands of MIDI files. MuseNet uses the same general-purpose unsupervised technology as GPT-2, a large-scale transformer model trained to predict the next token in a sequence, whether audio or text.},
	language = {en-US},
	urldate = {2023-07-26},
}

@misc{noauthor_stable_nodate,
	title = {Stable {Diffusion} {Online}},
	url = {https://stablediffusionweb.com/},
	urldate = {2023-07-26},
}

@misc{noauthor_dalle2_nodate,
	title = {dalle2},
	url = {http://adityaramesh.com/posts/dalle2/dalle2.html},
	urldate = {2023-07-26},
}

@misc{noauthor_dalle_nodate,
	title = {{DALL}·{E} 2},
	url = {https://openai.com/dall-e-2},
	abstract = {DALL·E 2 is an AI system that can create realistic images and art from a description in natural language.},
	language = {en-US},
	urldate = {2023-07-26},
}

@misc{noauthor_proposal_2016,
	title = {Proposal for a {DIRECTIVE} {OF} {THE} {EUROPEAN} {PARLIAMENT} {AND} {OF} {THE} {COUNCIL} on copyright in the {Digital} {Single} {Market}},
	url = {https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52016PC0593},
	language = {en},
	urldate = {2023-07-25},
	year = {2016},
}

@misc{noauthor_piano_nodate,
	title = {Piano • {Client} {Dashboard}},
	url = {https://buy.tinypass.com/checkout/template/cacheableShow?aid=WUOCNSUgpu&templateId=OTK3MV0EY5CW&templateVariantId=OTVSL3D8ZYF30&offerId=fakeOfferId&experienceId=EXMHBEMJU9UF&iframeId=offer_30f4defe339a0d8f7268-0&displayMode=inline&pianoIdUrl=https%3A%2F%2Fauth.technologyreview.com%2Fid%2F&widget=template&url=https%3A%2F%2Fwww.technologyreview.com},
	urldate = {2023-07-23},
}

@inproceedings{golatkar_eternal_2020,
	title = {Eternal {Sunshine} of the {Spotless} {Net}: {Selective} {Forgetting} in {Deep} {Networks}},
	shorttitle = {Eternal {Sunshine} of the {Spotless} {Net}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Golatkar_Eternal_Sunshine_of_the_Spotless_Net_Selective_Forgetting_in_Deep_CVPR_2020_paper.html},
	urldate = {2023-07-23},
	author = {Golatkar, Aditya and Achille, Alessandro and Soatto, Stefano},
	year = {2020},
	pages = {9304--9312},
}

@misc{wiggers_this_2022,
	title = {This site tells you if photos of you were used to train the {AI}},
	url = {https://techcrunch.com/2022/09/21/who-fed-the-ai/},
	abstract = {Deepfakes, AI-generated porn and a thousand more innocent uses — there’s been a lot of news about neural network-generated images. It makes sense that people started getting curious; were my photos used to train the robots? Are photos of me in the image-generating training sets? A brand new site tries to give you an answer. […]},
	language = {en-US},
	urldate = {2023-07-22},
	journal = {TechCrunch},
	author = {Wiggers, Haje Jan Kamps {and} Kyle},
	month = sep,
	year = {2022},
}

@misc{noauthor_euus_nodate,
	title = {{EU}/{US} {Copyright} {Law} and {Implications} on {ML} {Training} {Data}},
	url = {https://valohai.com/blog/copyright-laws-and-machine-learning/},
	abstract = {How does EU and US copyright laws differ? Learn why training your models with copyright protected content is not an outright copyright infringement.},
	language = {en},
	urldate = {2023-07-21},
}

@misc{noauthor_artificial_nodate,
	title = {Artificial {Intelligence} and {Intellectual} {Property}: copyright and patents: {Government} response to consultation},
	shorttitle = {Artificial {Intelligence} and {Intellectual} {Property}},
	url = {https://www.gov.uk/government/consultations/artificial-intelligence-and-ip-copyright-and-patents/outcome/artificial-intelligence-and-intellectual-property-copyright-and-patents-government-response-to-consultation},
	language = {en},
	urldate = {2023-07-21},
	journal = {GOV.UK},
}

@misc{noauthor_newgrounds_nodate,
	title = {Newgrounds {Wiki} - {Art} {Guidelines}},
	url = {https://www.newgrounds.com/wiki/help-information/terms-of-use/art-guidelines},
	language = {en},
	urldate = {2023-07-21},
	journal = {Newgrounds.com},
}

@misc{noauthor_glaze_nodate,
	title = {Glaze - {Downloads}},
	url = {https://glaze.cs.uchicago.edu/download.html},
	urldate = {2023-07-20},
}

@misc{macey-dare_how_2023,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {How {ChatGPT} and {Generative} {AI} {Systems} will {Revolutionize} {Legal} {Services} and the {Legal} {Profession}},
	url = {https://papers.ssrn.com/abstract=4366749},
	doi = {10.2139/ssrn.4366749},
	abstract = {In this paper, ChatGPT, is asked to provide c.150+ paragraphs of detailed prediction and insight into the following overlapping questions, concerning the potential impact of ChatGPT and successor generative AI systems on the evolving practice of law and the legal professions as we know them:• Which are the individual legal business areas where ChatGPT could make a significant/ transformative impact and reduce costs and increase efficiencies?• Where can ChatGPT use its special NLP abilities to assist in legal analysis and advice?• Which are the specific areas where generative AI systems like ChatGPT can revolutionize and improve the legal profession?• How can systems like ChatGPT help ordinary people with legal questions and legal problems?• What is the likely timeframe for ChatGPT and other generative AI systems to transform legal services and the legal profession?• What are the potential implications for new and intending law students?• How will ChatGPT and similar systems impact professional lawyers in future?Some of ChatGPT’s key insights and predictions (see full paper attached for detailed responses and analysis) are as follows:ChatGPT identifies the following key individual legal business areas where it could make a significant/ transformative impact and reduce costs and increase efficiencies: Alternative dispute resolution, Automated billing, Case analysis, Case management, Compliance monitoring, Contract management, Contract review, Document automation, Document review, Discovery and E-discovery, Drafting legal documents, Due diligence, Expertise matching, Intellectual Property and IP management, Legal advice, Legal analytics, Legal chatbots, Legal drafting, Legal document review, Legal education, Legal marketing, Legal research, Litigation support, Natural language processing (NLP), Patent analysis, Predictive analytics, Regulatory compliance, Research, Risk assessment, Training and education, Translation and Virtual assistants.ChatGPT flags up its special NLP abilities to assist in legal analysis and advice, particularly in the following key areas: Contract analysis, Document classification, Document summarization, Due diligence, Legal chatbots, Legal document review, Legal document summarization, Legal drafting, Legal language translation, Legal research, Named entity recognition, Predictive analytics, Regulatory compliance, Sentiment analysis and Topic modelling.On the question of which are the specific areas where generative AI systems like ChatGPT can revolutionize and improve the legal profession, ChatGPT identifies: Accessibility, Accuracy, Collaboration, Cost reduction, Customization, Decision-making, Efficiency, Error-reduction, New business and innovation, Job displacement potential, Legal research, Risk management and Scalability.On the question of how can systems like ChatGPT help ordinary people with legal questions and legal problems, ChatGPT identifies the following areas: 24/7 availability, Automated legal services, Consistency of advice, Contract review, Cost-effectiveness, Court filings, Customization, Document preparation, Education, Empowerment, Faster response times, Language translation, Legal advice, Legal chatbots, Legal education, Legal research, Mediation and dispute resolution, Privacy, Scalability and Simplified language.On the question of the likely timeframe for ChatGPT and other generative AI systems to transform legal services and the legal profession, Chat GPT comments that “It is difficult to predict with certainty, as it will depend on a variety of factors such as technological advancements, regulatory changes, and market demand. [However] There are several reasons to believe that the transformation of legal services through generative AI systems like ChatGPT will happen relatively quickly...the transformation of legal services through generative AI systems is likely to happen relatively quickly, potentially within the next 5-10 years.”On the potential implications for new and intending law students, ChatGPT comments that: “It is essential for students contemplating studying law to be aware of the ability of ChatGPT and generative AI systems to perform legal jobs instead of humans in the future. These technologies are already transforming the legal industry, and their impact is likely to continue to grow in the years to come. Understanding how these systems work and how they can be used in legal practice will be crucial for anyone seeking a career in law, and may also provide a competitive advantage in the job market. By understanding the potential impact of these technologies on the legal industry, students can better prepare themselves for the changing nature of legal work and take advantage of the new opportunities that are emerging as a result of these developments.”On the all-important question for lawyers of how ChatGPT and similar systems will impact professional lawyers in future, ChatGPT comments that: “Generative AI systems like ChatGPT have the potential to significantly impact the demand and payment for professional lawyers in the future. As these systems become more advanced and capable of performing a wider range of legal tasks, it is likely that they will begin to replace some of the work that is currently performed by human lawyers.""One area where this is already happening is in document review. AI systems can review and analyze large volumes of documents much more quickly and accurately than human lawyers, which can save significant time and cost for law firms and their clients. As these systems become more sophisticated, they may also be able to perform other tasks such as legal research and analysis, drafting legal documents, and even providing legal advice.""The impact of generative AI systems on the demand for lawyers will depend on the specific tasks and areas of law that they are able to perform. It is likely that some areas of law will be more heavily impacted than others. For example, areas such as contract law and intellectual property may be more susceptible to automation, while litigation and dispute resolution may be less so.""In terms of payment for professional lawyers, the impact of generative AI systems is also likely to vary depending on the specific tasks and areas of law. In some cases, these systems may allow lawyers to perform their work more quickly and efficiently, which could potentially lead to higher billable hours and increased income. However, if these systems are able to replace some of the work that is currently performed by human lawyers, it could also lead to a reduction in demand for these services and a decrease in fees.""One potential impact of systems like ChatGPT on the legal industry is a reduction in the demand for certain types of legal work that can be automated or performed more efficiently by AI systems. For example, tasks like document review, contract drafting, and legal research may be performed more accurately and quickly by AI systems than by humans, leading to a decrease in the number of lawyers needed to perform these tasks.""It is also possible that the development of AI systems like ChatGPT will lead to changes in the way that legal services are priced and delivered. As these technologies become more common, it is likely that clients will begin to expect lower costs and faster turnaround times for certain types of legal work. This could lead to increased competition among legal service providers, which in turn could put pressure on lawyers to lower their rates or find ways to deliver legal services more efficiently….it is clear that these technologies have the potential to significantly change the legal industry, and that lawyers will need to adapt in order to remain competitive and relevant in a rapidly changing market. This may involve developing new skills and knowledge related to working alongside AI systems, or focusing on areas of law that are less susceptible to automation.”Interestingly, although ChatGPT does discuss practical contract management, IP and evidence, it does not seem to predict inroads being made into academic legal analysis, statutory construction, complex case analysis or the development of new legal thinking and principles, so not into the theoretical domain of law professors and senior lawyers and judges, (although there are additional reasons why there are likely to be knock-on reductions in demand for these specialist lawyers too). But for the vast majority of procedural (routinely turning-the-handle type) practitioner law and practice, ChatGPT seems to be predicting a seismic sectoral shock, a reduction in human-centric legal work, an increase in legal self-help for clients and the public, and a technological transformation in and fundamental repricing and manpower shock for the legal sector within a timeframe of 5-10 years. N.B. This is only one set of predictions, which could prove right or wrong, indeed from an unconscious chatbot machine ChatGPT. However it has the credibility of being made based on both a huge body of knowledge data, and on the consistent rules programmed into ChatGPT itself, and by apparently coherently reasoned responses. Time will soon tell of course...},
	language = {en},
	urldate = {2023-06-26},
	author = {Macey-Dare, Rupert},
	month = feb,
	year = {2023},
	keywords = {ChatGPT, China, LLMs, Legal AI, Legal Tech, OpenAI, expert system, generative AI, legal expert, legal technology, robolawyer},
}
