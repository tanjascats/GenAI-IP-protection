
@misc{shan_glaze_2023,
	title = {Glaze: {Protecting} {Artists} from {Style} {Mimicry} by {Text}-to-{Image} {Models}},
	shorttitle = {Glaze},
	url = {http://arxiv.org/abs/2302.04222},
	doi = {10.48550/arXiv.2302.04222},
	abstract = {Recent text-to-image diffusion models such as MidJourney and Stable Diffusion threaten to displace many in the professional artist community. In particular, models can learn to mimic the artistic style of specific artists after "fine-tuning" on samples of their art. In this paper, we describe the design, implementation and evaluation of Glaze, a tool that enables artists to apply "style cloaks" to their art before sharing online. These cloaks apply barely perceptible perturbations to images, and when used as training data, mislead generative models that try to mimic a specific artist. In coordination with the professional artist community, we deploy user studies to more than 1000 artists, assessing their views of AI art, as well as the efficacy of our tool, its usability and tolerability of perturbations, and robustness across different scenarios and against adaptive countermeasures. Both surveyed artists and empirical CLIP-based scores show that even at low perturbation levels (p=0.05), Glaze is highly successful at disrupting mimicry under normal conditions ({\textgreater}92\%) and against adaptive countermeasures ({\textgreater}85\%).},
	urldate = {2023-06-21},
	publisher = {arXiv},
	author = {Shan, Shawn and Cryan, Jenna and Wenger, Emily and Zheng, Haitao and Hanocka, Rana and Zhao, Ben Y.},
	month = jun,
	year = {2023},
	note = {arXiv:2302.04222 [cs]},
	keywords = {Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\3RKEQVNX\\Shan et al. - 2023 - Glaze Protecting Artists from Style Mimicry by Te.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\H35N9UHS\\2302.html:text/html},
}

@misc{salman_raising_2023,
	title = {Raising the {Cost} of {Malicious} {AI}-{Powered} {Image} {Editing}},
	url = {http://arxiv.org/abs/2302.06588},
	doi = {10.48550/arXiv.2302.06588},
	abstract = {We present an approach to mitigating the risks of malicious image editing posed by large diffusion models. The key idea is to immunize images so as to make them resistant to manipulation by these models. This immunization relies on injection of imperceptible adversarial perturbations designed to disrupt the operation of the targeted diffusion models, forcing them to generate unrealistic images. We provide two methods for crafting such perturbations, and then demonstrate their efficacy. Finally, we discuss a policy component necessary to make our approach fully effective and practical -- one that involves the organizations developing diffusion models, rather than individual users, to implement (and support) the immunization process.},
	urldate = {2023-06-23},
	publisher = {arXiv},
	author = {Salman, Hadi and Khaddaj, Alaa and Leclerc, Guillaume and Ilyas, Andrew and Madry, Aleksander},
	month = feb,
	year = {2023},
	note = {arXiv:2302.06588 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\G2AX9KZ9\\Salman et al. - 2023 - Raising the Cost of Malicious AI-Powered Image Edi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\ZDL3LI2U\\2302.html:text/html},
}

@article{parnagian_should_nodate,
	title = {Should {AI} have {Intellectual} {Property} {Rights}? {An} {Analysis} of {Copyright} {Law} on {Generative} {AI}},
	abstract = {The rapid advancement of artiﬁcial intelligence (AI) technologies in recent years has challenged our understanding of intellectual property and created a complex legal landscape with far-reaching implications for copyright laws. This research aims to explore the intersection of copyright law, current lawsuits, and the impact of AI advancements on our perceptions of creativity and intelligence. We will ﬁrst examine the fundamentals of copyright law, followed by an analysis of recent AI-related lawsuits that highlight the legal challenges posed by AI-generated works. Finally, we will discuss whether copyright protection should extend to new forms of intelligence, taking into consideration how AI might change our understanding of creativity and ownership.},
	language = {en},
	author = {Parnagian, Ani},
	file = {Parnagian - Should AI have Intellectual Property Rights An An.pdf:C\:\\Users\\tsarcevic\\Zotero\\storage\\LSHREB9D\\Parnagian - Should AI have Intellectual Property Rights An An.pdf:application/pdf},
}

@misc{macey-dare_how_2023,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {How {ChatGPT} and {Generative} {AI} {Systems} will {Revolutionize} {Legal} {Services} and the {Legal} {Profession}},
	url = {https://papers.ssrn.com/abstract=4366749},
	doi = {10.2139/ssrn.4366749},
	abstract = {In this paper, ChatGPT, is asked to provide c.150+ paragraphs of detailed prediction and insight into the following overlapping questions, concerning the potential impact of ChatGPT and successor generative AI systems on the evolving practice of law and the legal professions as we know them:• Which are the individual legal business areas where ChatGPT could make a significant/ transformative impact and reduce costs and increase efficiencies?• Where can ChatGPT use its special NLP abilities to assist in legal analysis and advice?• Which are the specific areas where generative AI systems like ChatGPT can revolutionize and improve the legal profession?• How can systems like ChatGPT help ordinary people with legal questions and legal problems?• What is the likely timeframe for ChatGPT and other generative AI systems to transform legal services and the legal profession?• What are the potential implications for new and intending law students?• How will ChatGPT and similar systems impact professional lawyers in future?Some of ChatGPT’s key insights and predictions (see full paper attached for detailed responses and analysis) are as follows:ChatGPT identifies the following key individual legal business areas where it could make a significant/ transformative impact and reduce costs and increase efficiencies: Alternative dispute resolution, Automated billing, Case analysis, Case management, Compliance monitoring, Contract management, Contract review, Document automation, Document review, Discovery and E-discovery, Drafting legal documents, Due diligence, Expertise matching, Intellectual Property and IP management, Legal advice, Legal analytics, Legal chatbots, Legal drafting, Legal document review, Legal education, Legal marketing, Legal research, Litigation support, Natural language processing (NLP), Patent analysis, Predictive analytics, Regulatory compliance, Research, Risk assessment, Training and education, Translation and Virtual assistants.ChatGPT flags up its special NLP abilities to assist in legal analysis and advice, particularly in the following key areas: Contract analysis, Document classification, Document summarization, Due diligence, Legal chatbots, Legal document review, Legal document summarization, Legal drafting, Legal language translation, Legal research, Named entity recognition, Predictive analytics, Regulatory compliance, Sentiment analysis and Topic modelling.On the question of which are the specific areas where generative AI systems like ChatGPT can revolutionize and improve the legal profession, ChatGPT identifies: Accessibility, Accuracy, Collaboration, Cost reduction, Customization, Decision-making, Efficiency, Error-reduction, New business and innovation, Job displacement potential, Legal research, Risk management and Scalability.On the question of how can systems like ChatGPT help ordinary people with legal questions and legal problems, ChatGPT identifies the following areas: 24/7 availability, Automated legal services, Consistency of advice, Contract review, Cost-effectiveness, Court filings, Customization, Document preparation, Education, Empowerment, Faster response times, Language translation, Legal advice, Legal chatbots, Legal education, Legal research, Mediation and dispute resolution, Privacy, Scalability and Simplified language.On the question of the likely timeframe for ChatGPT and other generative AI systems to transform legal services and the legal profession, Chat GPT comments that “It is difficult to predict with certainty, as it will depend on a variety of factors such as technological advancements, regulatory changes, and market demand. [However] There are several reasons to believe that the transformation of legal services through generative AI systems like ChatGPT will happen relatively quickly...the transformation of legal services through generative AI systems is likely to happen relatively quickly, potentially within the next 5-10 years.”On the potential implications for new and intending law students, ChatGPT comments that: “It is essential for students contemplating studying law to be aware of the ability of ChatGPT and generative AI systems to perform legal jobs instead of humans in the future. These technologies are already transforming the legal industry, and their impact is likely to continue to grow in the years to come. Understanding how these systems work and how they can be used in legal practice will be crucial for anyone seeking a career in law, and may also provide a competitive advantage in the job market. By understanding the potential impact of these technologies on the legal industry, students can better prepare themselves for the changing nature of legal work and take advantage of the new opportunities that are emerging as a result of these developments.”On the all-important question for lawyers of how ChatGPT and similar systems will impact professional lawyers in future, ChatGPT comments that: “Generative AI systems like ChatGPT have the potential to significantly impact the demand and payment for professional lawyers in the future. As these systems become more advanced and capable of performing a wider range of legal tasks, it is likely that they will begin to replace some of the work that is currently performed by human lawyers.""One area where this is already happening is in document review. AI systems can review and analyze large volumes of documents much more quickly and accurately than human lawyers, which can save significant time and cost for law firms and their clients. As these systems become more sophisticated, they may also be able to perform other tasks such as legal research and analysis, drafting legal documents, and even providing legal advice.""The impact of generative AI systems on the demand for lawyers will depend on the specific tasks and areas of law that they are able to perform. It is likely that some areas of law will be more heavily impacted than others. For example, areas such as contract law and intellectual property may be more susceptible to automation, while litigation and dispute resolution may be less so.""In terms of payment for professional lawyers, the impact of generative AI systems is also likely to vary depending on the specific tasks and areas of law. In some cases, these systems may allow lawyers to perform their work more quickly and efficiently, which could potentially lead to higher billable hours and increased income. However, if these systems are able to replace some of the work that is currently performed by human lawyers, it could also lead to a reduction in demand for these services and a decrease in fees.""One potential impact of systems like ChatGPT on the legal industry is a reduction in the demand for certain types of legal work that can be automated or performed more efficiently by AI systems. For example, tasks like document review, contract drafting, and legal research may be performed more accurately and quickly by AI systems than by humans, leading to a decrease in the number of lawyers needed to perform these tasks.""It is also possible that the development of AI systems like ChatGPT will lead to changes in the way that legal services are priced and delivered. As these technologies become more common, it is likely that clients will begin to expect lower costs and faster turnaround times for certain types of legal work. This could lead to increased competition among legal service providers, which in turn could put pressure on lawyers to lower their rates or find ways to deliver legal services more efficiently….it is clear that these technologies have the potential to significantly change the legal industry, and that lawyers will need to adapt in order to remain competitive and relevant in a rapidly changing market. This may involve developing new skills and knowledge related to working alongside AI systems, or focusing on areas of law that are less susceptible to automation.”Interestingly, although ChatGPT does discuss practical contract management, IP and evidence, it does not seem to predict inroads being made into academic legal analysis, statutory construction, complex case analysis or the development of new legal thinking and principles, so not into the theoretical domain of law professors and senior lawyers and judges, (although there are additional reasons why there are likely to be knock-on reductions in demand for these specialist lawyers too). But for the vast majority of procedural (routinely turning-the-handle type) practitioner law and practice, ChatGPT seems to be predicting a seismic sectoral shock, a reduction in human-centric legal work, an increase in legal self-help for clients and the public, and a technological transformation in and fundamental repricing and manpower shock for the legal sector within a timeframe of 5-10 years. N.B. This is only one set of predictions, which could prove right or wrong, indeed from an unconscious chatbot machine ChatGPT. However it has the credibility of being made based on both a huge body of knowledge data, and on the consistent rules programmed into ChatGPT itself, and by apparently coherently reasoned responses. Time will soon tell of course...},
	language = {en},
	urldate = {2023-06-26},
	author = {Macey-Dare, Rupert},
	month = feb,
	year = {2023},
	keywords = {ChatGPT, China, expert system, generative AI, Legal AI, legal expert, Legal Tech, legal technology, LLMs, OpenAI, robolawyer},
	file = {Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\KZQ9WRVP\\Macey-Dare - 2023 - How ChatGPT and Generative AI Systems will Revolut.pdf:application/pdf},
}

@article{strowel_chatgpt_2023,
	title = {{ChatGPT} and {Generative} {AI} {Tools}: {Theft} of {Intellectual} {Labor}?},
	volume = {54},
	issn = {2195-0237},
	shorttitle = {{ChatGPT} and {Generative} {AI} {Tools}},
	url = {https://doi.org/10.1007/s40319-023-01321-y},
	doi = {10.1007/s40319-023-01321-y},
	language = {en},
	number = {4},
	urldate = {2023-06-26},
	journal = {IIC - International Review of Intellectual Property and Competition Law},
	author = {Strowel, Alain},
	month = apr,
	year = {2023},
	pages = {491--494},
	file = {Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\8UCF4H2N\\Strowel - 2023 - ChatGPT and Generative AI Tools Theft of Intellec.pdf:application/pdf},
}

@inproceedings{zhong_copyright_2023,
	title = {Copyright {Protection} and {Accountability} of {Generative} {AI}:{Attack}, {Watermarking} and {Attribution}},
	shorttitle = {Copyright {Protection} and {Accountability} of {Generative} {AI}},
	url = {http://arxiv.org/abs/2303.09272},
	doi = {10.1145/3543873.3587321},
	abstract = {Generative AI (e.g., Generative Adversarial Networks - GANs) has become increasingly popular in recent years. However, Generative AI introduces significant concerns regarding the protection of Intellectual Property Rights (IPR) (resp. model accountability) pertaining to images (resp. toxic images) and models (resp. poisoned models) generated. In this paper, we propose an evaluation framework to provide a comprehensive overview of the current state of the copyright protection measures for GANs, evaluate their performance across a diverse range of GAN architectures, and identify the factors that affect their performance and future research directions. Our findings indicate that the current IPR protection methods for input images, model watermarking, and attribution networks are largely satisfactory for a wide range of GANs. We highlight that further attention must be directed towards protecting training sets, as the current approaches fail to provide robust IPR protection and provenance tracing on training sets.},
	urldate = {2023-06-26},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2023},
	author = {Zhong, Haonan and Chang, Jiamin and Yang, Ziyue and Wu, Tingmin and Arachchige, Pathum Chamikara Mahawaga and Pathmabandu, Chehara and Xue, Minhui},
	month = apr,
	year = {2023},
	note = {arXiv:2303.09272 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Multimedia},
	pages = {94--98},
	file = {arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\IN5NW2TP\\2303.html:text/html;Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\YR3P59CM\\Zhong et al. - 2023 - Copyright Protection and Accountability of Generat.pdf:application/pdf},
}

@incollection{smits_generative_2022,
	address = {The Hague},
	series = {Information {Technology} and {Law} {Series}},
	title = {Generative {AI} and {Intellectual} {Property} {Rights}},
	isbn = {978-94-6265-523-2},
	url = {https://doi.org/10.1007/978-94-6265-523-2_17},
	abstract = {Since the inception of AI, researchers have tried to generate novel worksWork in media ranging from musicWorksMusic through text to images. The quality of worksWork produced by generativeAIGenerative AI-systems is starting to reach levels that make them usable in contexts where until now human creations are employed. In addition, new contexts are emerging in which humans unskilled in a creative domain can generate worksWork by cooperating with generative AI-toolsAITools. GenerativeAIGenerative AI could lead to an abundance of individually customized content, where worksWork are generated for a particular user in a specific situation and presented once, perhaps never to be repeated again. These developments challenge core concepts of Intellectual Property Rights: “authorship”Authorship obviously, but also “work”Work. Although the content produced by generative systems is new, these systems are often trained on a corpus of (parts of) existing worksWork produced by humans. Hence, practices of (un)authorised imitation need to be considered. In this chapter we want to study these questions, which are emerging in all creative domains, with generativeAIGenerative AI for musicWorksMusic as the central example.},
	language = {en},
	urldate = {2023-06-26},
	booktitle = {Law and {Artificial} {Intelligence}: {Regulating} {AI} and {Applying} {AI} in {Legal} {Practice}},
	publisher = {T.M.C. Asser Press},
	author = {Smits, Jan and Borghuis, Tijn},
	editor = {Custers, Bart and Fosch-Villaronga, Eduard},
	year = {2022},
	doi = {10.1007/978-94-6265-523-2_17},
	keywords = {Authorship, Era of Abundance, Generative AI, Human-Al Cooperation, Public Domain, Unauthorized Imitation, Work},
	pages = {323--344},
	file = {Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\28ZCHAPJ\\Smits and Borghuis - 2022 - Generative AI and Intellectual Property Rights.pdf:application/pdf},
}

@inproceedings{shan_fawkes_2020,
	title = {Fawkes: {Protecting} {Privacy} against {Unauthorized} {Deep} {Learning} {Models}},
	isbn = {978-1-939133-17-5},
	shorttitle = {Fawkes},
	url = {https://www.usenix.org/conference/usenixsecurity20/presentation/shan},
	language = {en},
	urldate = {2023-07-20},
	author = {Shan, Shawn and Wenger, Emily and Zhang, Jiayun and Li, Huiying and Zheng, Haitao and Zhao, Ben Y.},
	year = {2020},
	pages = {1589--1604},
	file = {Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\B8I9IIBN\\Shan et al. - 2020 - Fawkes Protecting Privacy against Unauthorized De.pdf:application/pdf},
}

@misc{noauthor_glaze_nodate,
	title = {Glaze - {Downloads}},
	url = {https://glaze.cs.uchicago.edu/download.html},
	urldate = {2023-07-20},
	file = {Glaze - Downloads:C\:\\Users\\tsarcevic\\Zotero\\storage\\M4G8NYY8\\download.html:text/html},
}

@misc{gal_image_2022,
	title = {An {Image} is {Worth} {One} {Word}: {Personalizing} {Text}-to-{Image} {Generation} using {Textual} {Inversion}},
	shorttitle = {An {Image} is {Worth} {One} {Word}},
	url = {http://arxiv.org/abs/2208.01618},
	doi = {10.48550/arXiv.2208.01618},
	abstract = {Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new "words" in the embedding space of a frozen text-to-image model. These "words" can be composed into natural language sentences, guiding personalized creation in an intuitive way. Notably, we find evidence that a single word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks. Our code, data and new words will be available at: https://textual-inversion.github.io},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Gal, Rinon and Alaluf, Yuval and Atzmon, Yuval and Patashnik, Or and Bermano, Amit H. and Chechik, Gal and Cohen-Or, Daniel},
	month = aug,
	year = {2022},
	note = {arXiv:2208.01618 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\KJVTPP4S\\2208.html:text/html},
}

@inproceedings{ruiz_dreambooth_2023,
	title = {{DreamBooth}: {Fine} {Tuning} {Text}-to-{Image} {Diffusion} {Models} for {Subject}-{Driven} {Generation}},
	shorttitle = {{DreamBooth}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Ruiz_DreamBooth_Fine_Tuning_Text-to-Image_Diffusion_Models_for_Subject-Driven_Generation_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-07-20},
	author = {Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
	year = {2023},
	pages = {22500--22510},
	file = {Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\6EAP5NUF\\Ruiz et al. - 2023 - DreamBooth Fine Tuning Text-to-Image Diffusion Mo.pdf:application/pdf},
}

@misc{baio_invasive_2022,
	title = {Invasive {Diffusion}: {How} one unwilling illustrator found herself turned into an {AI} model},
	shorttitle = {Invasive {Diffusion}},
	url = {https://waxy.org/2022/11/invasive-diffusion-how-one-unwilling-illustrator-found-herself-turned-into-an-ai-model/},
	abstract = {How does it feel to be turned into an AI image model? To find out, I opened a door to the multiverse and interviewed the creator and unwilling subject of a controversial DreamBooth model.},
	language = {en-US},
	urldate = {2023-07-20},
	journal = {Waxy.org},
	author = {Baio, Andy},
	month = nov,
	year = {2022},
	file = {Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\HRT4HYNB\\invasive-diffusion-how-one-unwilling-illustrator-found-herself-turned-into-an-ai-model.html:text/html},
}

@misc{kumari_ablating_2023,
	title = {Ablating {Concepts} in {Text}-to-{Image} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2303.13516},
	doi = {10.48550/arXiv.2303.13516},
	abstract = {Large-scale text-to-image diffusion models can generate high-fidelity images with powerful compositional ability. However, these models are typically trained on an enormous amount of Internet data, often containing copyrighted material, licensed images, and personal photos. Furthermore, they have been found to replicate the style of various living artists or memorize exact training samples. How can we remove such copyrighted concepts or images without retraining the model from scratch? To achieve this goal, we propose an efficient method of ablating concepts in the pretrained model, i.e., preventing the generation of a target concept. Our algorithm learns to match the image distribution for a target style, instance, or text prompt we wish to ablate to the distribution corresponding to an anchor concept. This prevents the model from generating target concepts given its text condition. Extensive experiments show that our method can successfully prevent the generation of the ablated concept while preserving closely related concepts in the model.},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Kumari, Nupur and Zhang, Bingliang and Wang, Sheng-Yu and Shechtman, Eli and Zhang, Richard and Zhu, Jun-Yan},
	month = may,
	year = {2023},
	note = {arXiv:2303.13516 [cs]
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\6CD573MJ\\Kumari et al. - 2023 - Ablating Concepts in Text-to-Image Diffusion Model.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\IUNL4RLG\\2303.html:text/html},
}

@misc{schramowski_safe_2023,
	title = {Safe {Latent} {Diffusion}: {Mitigating} {Inappropriate} {Degeneration} in {Diffusion} {Models}},
	shorttitle = {Safe {Latent} {Diffusion}},
	url = {http://arxiv.org/abs/2211.05105},
	doi = {10.48550/arXiv.2211.05105},
	abstract = {Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer, as we demonstrate, from degenerated and biased human behavior. In turn, they may even reinforce such biases. To help combat these undesired side effects, we present safe latent diffusion (SLD). Specifically, to measure the inappropriate degeneration due to unfiltered and imbalanced training sets, we establish a novel image generation test bed-inappropriate image prompts (I2P)-containing dedicated, real-world image-to-text prompts covering concepts such as nudity and violence. As our exhaustive empirical evaluation demonstrates, the introduced SLD removes and suppresses inappropriate image parts during the diffusion process, with no additional training required and no adverse effect on overall image quality or text alignment.},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Schramowski, Patrick and Brack, Manuel and Deiseroth, Björn and Kersting, Kristian},
	month = apr,
	year = {2023},
	note = {arXiv:2211.05105 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\MCLHT9XX\\Schramowski et al. - 2023 - Safe Latent Diffusion Mitigating Inappropriate De.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\KL6G9GEG\\2211.html:text/html},
}

@misc{gandikota_erasing_2023,
	title = {Erasing {Concepts} from {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2303.07345},
	doi = {10.48550/arXiv.2303.07345},
	abstract = {Motivated by recent advancements in text-to-image diffusion, we study erasure of specific concepts from the model's weights. While Stable Diffusion has shown promise in producing explicit or realistic artwork, it has raised concerns regarding its potential for misuse. We propose a fine-tuning method that can erase a visual concept from a pre-trained diffusion model, given only the name of the style and using negative guidance as a teacher. We benchmark our method against previous approaches that remove sexually explicit content and demonstrate its effectiveness, performing on par with Safe Latent Diffusion and censored training. To evaluate artistic style removal, we conduct experiments erasing five modern artists from the network and conduct a user study to assess the human perception of the removed styles. Unlike previous methods, our approach can remove concepts from a diffusion model permanently rather than modifying the output at the inference time, so it cannot be circumvented even if a user has access to model weights. Our code, data, and results are available at https://erasing.baulab.info/},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Gandikota, Rohit and Materzynska, Joanna and Fiotto-Kaufman, Jaden and Bau, David},
	month = jun,
	year = {2023},
	note = {arXiv:2303.07345 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\5SGWJFWF\\Gandikota et al. - 2023 - Erasing Concepts from Diffusion Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\FRNQTE3L\\2303.html:text/html},
}

@misc{kong_data_2023,
	title = {Data {Redaction} from {Pre}-trained {GANs}},
	url = {http://arxiv.org/abs/2206.14389},
	doi = {10.48550/arXiv.2206.14389},
	abstract = {Large pre-trained generative models are known to occasionally output undesirable samples, which undermines their trustworthiness. The common way to mitigate this is to re-train them differently from scratch using different data or different regularization -- which uses a lot of computational resources and does not always fully address the problem. In this work, we take a different, more compute-friendly approach and investigate how to post-edit a model after training so that it ''redacts'', or refrains from outputting certain kinds of samples. We show that redaction is a fundamentally different task from data deletion, and data deletion may not always lead to redaction. We then consider Generative Adversarial Networks (GANs), and provide three different algorithms for data redaction that differ on how the samples to be redacted are described. Extensive evaluations on real-world image datasets show that our algorithms out-perform data deletion baselines, and are capable of redacting data while retaining high generation quality at a fraction of the cost of full re-training.},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Kong, Zhifeng and Chaudhuri, Kamalika},
	month = jan,
	year = {2023},
	note = {arXiv:2206.14389 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\ZBIWKQMF\\Kong and Chaudhuri - 2023 - Data Redaction from Pre-trained GANs.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\BQESTAFB\\2206.html:text/html},
}

@misc{carlini_extracting_2023,
	title = {Extracting {Training} {Data} from {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2301.13188},
	doi = {10.48550/arXiv.2301.13188},
	abstract = {Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Carlini, Nicholas and Hayes, Jamie and Nasr, Milad and Jagielski, Matthew and Sehwag, Vikash and Tramèr, Florian and Balle, Borja and Ippolito, Daphne and Wallace, Eric},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13188 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\YH9SXENL\\2301.html:text/html},
}

@misc{somepalli_understanding_2023,
	title = {Understanding and {Mitigating} {Copying} in {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2305.20086},
	doi = {10.48550/arXiv.2305.20086},
	abstract = {Images generated by diffusion models like Stable Diffusion are increasingly widespread. Recent works and even lawsuits have shown that these models are prone to replicating their training data, unbeknownst to the user. In this paper, we first analyze this memorization problem in text-to-image diffusion models. While it is widely believed that duplicated images in the training set are responsible for content replication at inference time, we observe that the text conditioning of the model plays a similarly important role. In fact, we see in our experiments that data replication often does not happen for unconditional models, while it is common in the text-conditional case. Motivated by our findings, we then propose several techniques for reducing data replication at both training and inference time by randomizing and augmenting image captions in the training set.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Somepalli, Gowthami and Singla, Vasu and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
	month = may,
	year = {2023},
	note = {arXiv:2305.20086 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\UJ2TPJD6\\Somepalli et al. - 2023 - Understanding and Mitigating Copying in Diffusion .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\QSMDKDP7\\2305.html:text/html},
}

@article{schuhmann_laion-5b_2022,
	title = {{LAION}-{5B}: {An} open large-scale dataset for training next generation image-text models},
	volume = {35},
	shorttitle = {{LAION}-{5B}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/a1859debfb3b59d094f3504d5ebb6c25-Abstract-Datasets_and_Benchmarks.html},
	language = {en},
	urldate = {2023-07-21},
	journal = {Advances in Neural Information Processing Systems},
	author = {Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and Schramowski, Patrick and Kundurthy, Srivatsa and Crowson, Katherine and Schmidt, Ludwig and Kaczmarczyk, Robert and Jitsev, Jenia},
	month = dec,
	year = {2022},
	pages = {25278--25294},
	file = {Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\MPARU2X4\\Schuhmann et al. - 2022 - LAION-5B An open large-scale dataset for training.pdf:application/pdf},
}

@misc{noauthor_laion-5b_nodate,
	title = {{LAION}-{5B}: {A} {NEW} {ERA} {OF} {OPEN} {LARGE}-{SCALE} {MULTI}-{MODAL} {DATASETS} {\textbar} {LAION}},
	shorttitle = {{LAION}-{5B}},
	url = {https://laion.ai/blog/laion-5b},
	abstract = {{\textless}p{\textgreater}We present a dataset of 5,85 billion CLIP-filtered image-text pairs, 14x bigger than LAION-400M, previously the biggest openly accessible image-text datas...},
	language = {en},
	urldate = {2023-07-21},
	file = {Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\LZVIZH5Q\\laion-5b.html:text/html},
}

@misc{noauthor_this_nodate,
	title = {This artist is dominating {AI}-generated art. {And} he’s not happy about it.},
	url = {https://www.technologyreview.com/2022/09/16/1059598/this-artist-is-dominating-ai-generated-art-and-hes-not-happy-about-it/},
	abstract = {Greg Rutkowski is a more popular prompt than Picasso.},
	language = {en},
	urldate = {2023-07-21},
        publisher = {MIT Technology Review},
        author = {Melissa Heikkilä},
        month = sep,
        year = {2002},
	file = {Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\SCL6P85I\\this-artist-is-dominating-ai-generated-art-and-hes-not-happy-about-it.html:text/html},
}

@misc{noauthor_newgrounds_nodate,
	title = {Newgrounds {Wiki} - {Art} {Guidelines}},
	url = {https://www.newgrounds.com/wiki/help-information/terms-of-use/art-guidelines},
        note = {https://www.newgrounds.com/wiki/help-information/terms-of-use/art-guidelines},
        year = {2023},
	language = {en},
	urldate = {2023-07-21},
	journal = {Newgrounds.com},
	file = {Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\666PC4UU\\art-guidelines.html:text/html},
}

@misc{noauthor_content_nodate,
	title = {Content {Authenticity} {Initiative}},
	url = {https://contentauthenticity.org},
	abstract = {Creating the standard for digital content provenance.},
	language = {en-US},
	urldate = {2023-07-21},
	journal = {Content Authenticity Initiative},
	file = {Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\SWM9GYXY\\contentauthenticity.org.html:text/html},
}

@misc{noauthor_web_nodate,
	title = {Web scraping is legal, {US} appeals court reaffirms},
	url = {https://techcrunch.com/2022/04/18/web-scraping-legal-court/?guccounter=1},
	urldate = {2023-07-21},
        author = {Zach Whittaker},
        month = apr,
        year = {2022},
	file = {Web scraping is legal, US appeals court reaffirms | TechCrunch:C\:\\Users\\tsarcevic\\Zotero\\storage\\ZL962ARP\\web-scraping-legal-court.html:text/html},
}

@misc{noauthor_authors_2023,
	title = {\textit{{Authors} {Guild}, {Inc}. v. {Google}, {Inc}.}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Authors_Guild,_Inc._v._Google,_Inc.&oldid=1165284601},
	abstract = {Authors Guild v. Google 721 F.3d 132 (2nd Cir. 2015) was a copyright case heard in federal court for the Southern District of New York, and then the Second Circuit Court of Appeals between 2005 and 2015. It concerned fair use in copyright law and the transformation of printed copyrighted books into an online searchable database through scanning and digitization. It centered on the legality of the Google Book Search (originally named as Google Print) Library Partner project that had been launched in 2003.
Though there was general agreement that Google's attempt to digitize books through scanning and computer-aided recognition for searching online was seen as a transformative step for libraries, many authors and publishers had expressed concern that Google had not sought their permission to make scans of the books still under copyright and offered them to users. Two separate lawsuits, including one from three authors represented by the Authors Guild and another by Association of American Publishers, were filed in 2005 charging Google with copyright infringement. Google worked with the litigants in both suits to develop a settlement agreement (the Google Book Search Settlement Agreement) that would have allowed it to continue the program though paying out for works it had previously scanned, creating a revenue program for future books that were part of the search engine, and allowing authors and publishers to opt-out. The settlement received much criticism as it also applied to all books worldwide, included works that may have been out of print but still under copyright, and may have violated antitrust aspects given Google's dominant position within the Internet industry. A reworked proposal to address some of these concerns was met with similar criticism, and ultimately the settlement was rejected by 2011, allowing the two lawsuits to be joined for a combined trial.
In late 2013, after the class action status was challenged, the District Court granted summary judgement in favor of Google, dismissing the lawsuit and affirming the Google Books project met all legal requirements for fair use. The Second Circuit Court of Appeal upheld the District Court's summary judgement in October 2015, ruling Google's "project provides a public service without violating intellectual property law." The U.S. Supreme Court subsequently denied a petition to hear the case.},
	language = {en},
	urldate = {2023-07-21},
	journal = {Wikipedia},
	month = jul,
	year = {2023},
	note = {Page Version ID: 1165284601},
	file = {Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\KWX9ABX6\\Authors_Guild,_Inc._v._Google,_Inc..html:text/html},
}

@misc{noauthor_artificial_nodate,
	title = {Artificial {Intelligence} and {Intellectual} {Property}: copyright and patents: {Government} response to consultation},
	shorttitle = {Artificial {Intelligence} and {Intellectual} {Property}},
	url = {https://www.gov.uk/government/consultations/artificial-intelligence-and-ip-copyright-and-patents/outcome/artificial-intelligence-and-intellectual-property-copyright-and-patents-government-response-to-consultation},
	language = {en},
	urldate = {2023-07-21},
	journal = {GOV.UK},
	file = {Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\ZBBGFCMZ\\artificial-intelligence-and-intellectual-property-copyright-and-patents-government-response-to-.html:text/html},
}

@misc{stokel-walker_this_2022,
	title = {This couple is launching an organization to protect artists in the {AI} era},
	url = {https://www.inverse.com/input/culture/mat-dryhurst-holly-herndon-artists-ai-spawning-source-dall-e-midjourney},
	abstract = {Mat Dryhurst \& Holly Herndon want creatives to be able to opt into or out of having their work used as training data for DALL-E and the like.},
	language = {en},
	urldate = {2023-07-21},
	journal = {Input},
	author = {Stokel-Walker, Chris},
	month = sep,
	year = {2022},
	file = {Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\W52TAF6Q\\mat-dryhurst-holly-herndon-artists-ai-spawning-source-dall-e-midjourney.html:text/html},
}

@misc{noauthor_euus_nodate,
	title = {{EU}/{US} {Copyright} {Law} and {Implications} on {ML} {Training} {Data}},
	url = {https://valohai.com/blog/copyright-laws-and-machine-learning/},
	abstract = {How does EU and US copyright laws differ? Learn why training your models with copyright protected content is not an outright copyright infringement.},
	language = {en},
	urldate = {2023-07-21},
	file = {Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\I5GLYQB3\\copyright-laws-and-machine-learning.html:text/html},
}

@misc{noauthor_holly_nodate,
	title = {Holly+ — {Holly} {Herndon}},
	url = {https://holly.mirror.xyz/54ds2IiOnvthjGFkokFCoaI4EabytH9xjAYy1irHy94},
        note = {https://holly.plus/},
        year = {2023},
	urldate = {2023-07-21},
	file = {Holly+ �� ��️ �� — Holly Herndon:C\:\\Users\\tsarcevic\\Zotero\\storage\\Y3LY5M9F\\54ds2IiOnvthjGFkokFCoaI4EabytH9xjAYy1irHy94.html:text/html},
}

@misc{noauthor_how_2023,
	title = {How to {Know} if {Your} {Images} {Trained} an {AI} {Model} (and {How} to {Opt} {Out})},
	url = {https://www.makeuseof.com/how-to-know-images-trained-ai-art-generator/},
	abstract = {Were your images used to train an AI art generator without your permission? Here's how to find out, and opt out!},
	language = {en},
        author = {Garling Wu}, 
	urldate = {2023-07-21},
	journal = {MUO},
	month = jan,
	year = {2023},
	file = {Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\EY8BM9ZY\\how-to-know-images-trained-ai-art-generator.html:text/html},
}

@misc{noauthor_spawningai_nodate,
	title = {Spawning.ai},
	url = {https://www.spawning.ai},
	abstract = {We believe that a future of consenting data will benefit both AI development and the people it is trained on.},
	language = {en},
	urldate = {2023-07-21},
        year = {2023},
	journal = {Spawning.ai},
	file = {Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\GBZNTBBM\\spawning-api.html:text/html},
note = {https://www.spawning.ai}
}

@misc{carlini_quantifying_2023,
	title = {Quantifying {Memorization} {Across} {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/2202.07646},
	doi = {10.48550/arXiv.2202.07646},
	abstract = {Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others). We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes more complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
	month = mar,
	year = {2023},
	note = {arXiv:2202.07646 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\MBVJ4ZUL\\Carlini et al. - 2023 - Quantifying Memorization Across Neural Language Mo.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\8E2DZ4HK\\2202.html:text/html},
}

@inproceedings{pizzi_self-supervised_2022,
	title = {A {Self}-{Supervised} {Descriptor} for {Image} {Copy} {Detection}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Pizzi_A_Self-Supervised_Descriptor_for_Image_Copy_Detection_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-07-21},
	author = {Pizzi, Ed and Roy, Sreya Dutta and Ravindra, Sugosh Nagavara and Goyal, Priya and Douze, Matthijs},
	year = {2022},
	pages = {14532--14542},
	file = {Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\XBQ4AI6Y\\Pizzi et al. - 2022 - A Self-Supervised Descriptor for Image Copy Detect.pdf:application/pdf},
}

@inproceedings{arpit_closer_2017,
	title = {A {Closer} {Look} at {Memorization} in {Deep} {Networks}},
	url = {https://proceedings.mlr.press/v70/arpit17a.html},
	abstract = {We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs.{\textasciitilde}real data. We also demonstrate that for appropriately tuned explicit regularization (e.g.,{\textasciitilde}dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.},
	language = {en},
	urldate = {2023-07-21},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Arpit, Devansh and Jastrzębski, Stanis{\l}aw and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S. and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and Lacoste-Julien, Simon},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {233--242},
	file = {Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\MV3AMANX\\Arpit et al. - 2017 - A Closer Look at Memorization in Deep Networks.pdf:application/pdf},
}

@inproceedings{feldman_does_2020,
	address = {New York, NY, USA},
	series = {{STOC} 2020},
	title = {Does learning require memorization? a short tale about a long tail},
	isbn = {978-1-4503-6979-4},
	shorttitle = {Does learning require memorization?},
	url = {https://dl.acm.org/doi/10.1145/3357713.3384290},
	doi = {10.1145/3357713.3384290},
	abstract = {State-of-the-art results on image recognition tasks are achieved using over-parameterized learning algorithms that (nearly) perfectly fit the training set and are known to fit well even random labels. This tendency to memorize seemingly useless training data labels is not explained by existing theoretical analyses. Memorization of the training data also presents significant privacy risks when the training data contains sensitive personal information and thus it is important to understand whether such memorization is necessary for accurate learning. We provide a simple conceptual explanation and a theoretical model demonstrating that for natural data distributions memorization of labels is necessary for achieving close-to-optimal generalization error. The model is motivated and supported by the results of several recent empirical works. In our model, data is sampled from a mixture of subpopulations and the frequencies of these subpopulations are chosen from some prior. The model allows to quantify the effect of not fitting the training data on the generalization performance of the learned classifier and demonstrates that memorization is necessary whenever frequencies are long-tailed. Image and text data are known to follow such distributions and therefore our results establish a formal link between these empirical phenomena. Our results also have concrete implications for the cost of ensuring differential privacy in learning.},
	urldate = {2023-07-21},
	booktitle = {Proceedings of the 52nd {Annual} {ACM} {SIGACT} {Symposium} on {Theory} of {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Feldman, Vitaly},
	month = jun,
	year = {2020},
	keywords = {Generalization, Interpolation, Long-tailed Distribution, Overfitting, Privacy-preserving Learning},
	pages = {954--959},
	file = {Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\SBHUAC6R\\Feldman - 2020 - Does learning require memorization a short tale a.pdf:application/pdf},
}

@article{feldman_what_2020,
	title = {What {Neural} {Networks} {Memorize} and {Why}: {Discovering} the {Long} {Tail} via {Influence} {Estimation}},
	volume = {33},
	shorttitle = {What {Neural} {Networks} {Memorize} and {Why}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/1e14bfe2714193e7af5abc64ecbd6b46-Abstract.html?ref=dl-staging-website.ghost.io},
	language = {en},
	urldate = {2023-07-21},
	journal = {Advances in Neural Information Processing Systems},
	author = {Feldman, Vitaly and Zhang, Chiyuan},
	year = {2020},
	pages = {2881--2891},
	file = {Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\2Z8IRRIJ\\Feldman and Zhang - 2020 - What Neural Networks Memorize and Why Discovering.pdf:application/pdf},
}

@misc{wen_canary_2023,
	title = {Canary in a {Coalmine}: {Better} {Membership} {Inference} with {Ensembled} {Adversarial} {Queries}},
	shorttitle = {Canary in a {Coalmine}},
	url = {http://arxiv.org/abs/2210.10750},
	doi = {10.48550/arXiv.2210.10750},
	abstract = {As industrial applications are increasingly automated by machine learning models, enforcing personal data ownership and intellectual property rights requires tracing training data back to their rightful owners. Membership inference algorithms approach this problem by using statistical techniques to discern whether a target sample was included in a model's training set. However, existing methods only utilize the unaltered target sample or simple augmentations of the target to compute statistics. Such a sparse sampling of the model's behavior carries little information, leading to poor inference capabilities. In this work, we use adversarial tools to directly optimize for queries that are discriminative and diverse. Our improvements achieve significantly more accurate membership inference than existing methods, especially in offline scenarios and in the low false-positive regime which is critical in legal settings. Code is available at https://github.com/YuxinWenRick/canary-in-a-coalmine.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Wen, Yuxin and Bansal, Arpit and Kazemi, Hamid and Borgnia, Eitan and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
	month = jun,
	year = {2023},
	note = {arXiv:2210.10750 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\Y8JY8IDU\\Wen et al. - 2023 - Canary in a Coalmine Better Membership Inference .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\2QCF2DP5\\2210.html:text/html},
}

@misc{webster_this_2021,
	title = {This {Person} ({Probably}) {Exists}. {Identity} {Membership} {Attacks} {Against} {GAN} {Generated} {Faces}},
	url = {http://arxiv.org/abs/2107.06018},
	doi = {10.48550/arXiv.2107.06018},
	abstract = {Recently, generative adversarial networks (GANs) have achieved stunning realism, fooling even human observers. Indeed, the popular tongue-in-cheek website \{{\textbackslash}small {\textbackslash}url\{ http://thispersondoesnotexist.com\}\}, taunts users with GAN generated images that seem too real to believe. On the other hand, GANs do leak information about their training data, as evidenced by membership attacks recently demonstrated in the literature. In this work, we challenge the assumption that GAN faces really are novel creations, by constructing a successful membership attack of a new kind. Unlike previous works, our attack can accurately discern samples sharing the same identity as training samples without being the same samples. We demonstrate the interest of our attack across several popular face datasets and GAN training procedures. Notably, we show that even in the presence of significant dataset diversity, an over represented person can pose a privacy concern.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Webster, Ryan and Rabin, Julien and Simon, Loic and Jurie, Frederic},
	month = jul,
	year = {2021},
	note = {arXiv:2107.06018 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\XXKIIFAR\\Webster et al. - 2021 - This Person (Probably) Exists. Identity Membership.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\7LHJLZPI\\2107.html:text/html},
}

@misc{matsumoto_membership_2023,
	title = {Membership {Inference} {Attacks} against {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2302.03262},
	doi = {10.48550/arXiv.2302.03262},
	abstract = {Diffusion models have attracted attention in recent years as innovative generative models. In this paper, we investigate whether a diffusion model is resistant to a membership inference attack, which evaluates the privacy leakage of a machine learning model. We primarily discuss the diffusion model from the standpoints of comparison with a generative adversarial network (GAN) as conventional models and hyperparameters unique to the diffusion model, i.e., time steps, sampling steps, and sampling variances. We conduct extensive experiments with DDIM as a diffusion model and DCGAN as a GAN on the CelebA and CIFAR-10 datasets in both white-box and black-box settings and then confirm if the diffusion model is comparably resistant to a membership inference attack as GAN. Next, we demonstrate that the impact of time steps is significant and intermediate steps in a noise schedule are the most vulnerable to the attack. We also found two key insights through further analysis. First, we identify that DDIM is vulnerable to the attack for small sample sizes instead of achieving a lower FID. Second, sampling steps in hyperparameters are important for resistance to the attack, whereas the impact of sampling variances is quite limited.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Matsumoto, Tomoya and Miura, Takayuki and Yanai, Naoto},
	month = mar,
	year = {2023},
	note = {arXiv:2302.03262 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\32C9QBFT\\Matsumoto et al. - 2023 - Membership Inference Attacks against Diffusion Mod.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\RJG7QXK8\\2302.html:text/html},
}

@misc{hu_membership_2023,
	title = {Membership {Inference} of {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2301.09956},
	doi = {10.48550/arXiv.2301.09956},
	abstract = {Recent years have witnessed the tremendous success of diffusion models in data synthesis. However, when diffusion models are applied to sensitive data, they also give rise to severe privacy concerns. In this paper, we systematically present the first study about membership inference attacks against diffusion models, which aims to infer whether a sample was used to train the model. Two attack methods are proposed, namely loss-based and likelihood-based attacks. Our attack methods are evaluated on several state-of-the-art diffusion models, over different datasets in relation to privacy-sensitive data. Extensive experimental evaluations show that our attacks can achieve remarkable performance. Furthermore, we exhaustively investigate various factors which can affect attack performance. Finally, we also evaluate the performance of our attack methods on diffusion models trained with differential privacy.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Hu, Hailong and Pang, Jun},
	month = jan,
	year = {2023},
	note = {arXiv:2301.09956 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\ZD4IMHA3\\Hu and Pang - 2023 - Membership Inference of Diffusion Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\RURUEFXR\\2301.html:text/html},
}

@misc{ghiasi_plug-inversion_2021,
      title={Plug-In Inversion: Model-Agnostic Inversion for Vision with Data Augmentations}, 
      author={Amin Ghiasi and Hamid Kazemi and Steven Reich and Chen Zhu and Micah Goldblum and Tom Goldstein},
      year={2022},
      eprint={2201.12961},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@inproceedings{yin_dreaming_2020,
	title = {Dreaming to {Distill}: {Data}-{Free} {Knowledge} {Transfer} via {DeepInversion}},
	shorttitle = {Dreaming to {Distill}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Yin_Dreaming_to_Distill_Data-Free_Knowledge_Transfer_via_DeepInversion_CVPR_2020_paper.html},
	urldate = {2023-07-21},
	author = {Yin, Hongxu and Molchanov, Pavlo and Alvarez, Jose M. and Li, Zhizhong and Mallya, Arun and Hoiem, Derek and Jha, Niraj K. and Kautz, Jan},
	year = {2020},
	pages = {8715--8724},
	file = {Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\DUFJT5T4\\Yin et al. - 2020 - Dreaming to Distill Data-Free Knowledge Transfer .pdf:application/pdf},
}

@inproceedings{carlini_extracting_2021,
	title = {Extracting {Training} {Data} from {Large} {Language} {Models}},
	isbn = {978-1-939133-24-3},
	url = {https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting},
	language = {en},
	urldate = {2023-07-21},
	author = {Carlini, Nicholas and Tramèr, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, {\'U}lfar and Oprea, Alina and Raffel, Colin},
	year = {2021},
	pages = {2633--2650},
}

@misc{jagielski_measuring_2023,
	title = {Measuring {Forgetting} of {Memorized} {Training} {Examples}},
	url = {http://arxiv.org/abs/2207.00099},
	doi = {10.48550/arXiv.2207.00099},
	abstract = {Machine learning models exhibit two seemingly contradictory phenomena: training data memorization, and various forms of forgetting. In memorization, models overfit specific training examples and become susceptible to privacy attacks. In forgetting, examples which appeared early in training are forgotten by the end. In this work, we connect these phenomena. We propose a technique to measure to what extent models "forget" the specifics of training examples, becoming less susceptible to privacy attacks on examples they have not seen recently. We show that, while non-convex models can memorize data forever in the worst-case, standard image, speech, and language models empirically do forget examples over time. We identify nondeterminism as a potential explanation, showing that deterministically trained models do not forget. Our results suggest that examples seen early when training with extremely large datasets - for instance those examples used to pre-train a model - may observe privacy benefits at the expense of examples seen later.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Jagielski, Matthew and Thakkar, Om and Tramèr, Florian and Ippolito, Daphne and Lee, Katherine and Carlini, Nicholas and Wallace, Eric and Song, Shuang and Thakurta, Abhradeep and Papernot, Nicolas and Zhang, Chiyuan},
	month = may,
	year = {2023},
	note = {arXiv:2207.00099 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\Q2GKE2HT\\Jagielski et al. - 2023 - Measuring Forgetting of Memorized Training Example.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\72HPZTUB\\2207.html:text/html},
}

@inproceedings{lee_language_2023,
	title = {Do {Language} {Models} {Plagiarize}?},
	url = {http://arxiv.org/abs/2203.07618},
	doi = {10.1145/3543507.3583199},
	abstract = {Past literature has illustrated that language models (LMs) often memorize parts of training instances and reproduce them in natural language generation (NLG) processes. However, it is unclear to what extent LMs "reuse" a training corpus. For instance, models can generate paraphrased sentences that are contextually similar to training samples. In this work, therefore, we study three types of plagiarism (i.e., verbatim, paraphrase, and idea) among GPT-2 generated texts, in comparison to its training data, and further analyze the plagiarism patterns of fine-tuned LMs with domain-specific corpora which are extensively used in practice. Our results suggest that (1) three types of plagiarism widely exist in LMs beyond memorization, (2) both size and decoding methods of LMs are strongly associated with the degrees of plagiarism they exhibit, and (3) fine-tuned LMs' plagiarism patterns vary based on their corpus similarity and homogeneity. Given that a majority of LMs' training data is scraped from the Web without informing content owners, their reiteration of words, phrases, and even core ideas from training sets into generated texts has ethical implications. Their patterns are likely to exacerbate as both the size of LMs and their training data increase, raising concerns about indiscriminately pursuing larger models with larger training corpora. Plagiarized content can also contain individuals' personal and sensitive information. These findings overall cast doubt on the practicality of current LMs in mission-critical writing tasks and urge more discussions around the observed phenomena. Data and source code are available at https://github.com/Brit7777/LM-plagiarism.},
	urldate = {2023-07-21},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2023},
	author = {Lee, Jooyoung and Le, Thai and Chen, Jinghui and Lee, Dongwon},
	month = apr,
	year = {2023},
	note = {arXiv:2203.07618 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	pages = {3637--3647},
	file = {arXiv Fulltext PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\3GSH2D6P\\Lee et al. - 2023 - Do Language Models Plagiarize.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\28XH7W7M\\2203.html:text/html},
}

@article{tirumala_memorization_2022,
	title = {Memorization {Without} {Overfitting}: {Analyzing} the {Training} {Dynamics} of {Large} {Language} {Models}},
	volume = {35},
	shorttitle = {Memorization {Without} {Overfitting}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/fa0509f4dab6807e2cb465715bf2d249-Abstract-Conference.html},
	language = {en},
	urldate = {2023-07-21},
	journal = {Advances in Neural Information Processing Systems},
	author = {Tirumala, Kushal and Markosyan, Aram and Zettlemoyer, Luke and Aghajanyan, Armen},
	month = dec,
	year = {2022},
	pages = {38274--38290},
	file = {Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\R6WJXKV8\\Tirumala et al. - 2022 - Memorization Without Overfitting Analyzing the Tr.pdf:application/pdf},
}

@misc{somepalli_diffusion_2022,
	title = {Diffusion {Art} or {Digital} {Forgery}? {Investigating} {Data} {Replication} in {Diffusion} {Models}},
	shorttitle = {Diffusion {Art} or {Digital} {Forgery}?},
	url = {http://arxiv.org/abs/2212.03860},
	abstract = {Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Somepalli, Gowthami and Singla, Vasu and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
	month = dec,
	year = {2022},
	note = {arXiv:2212.03860 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\LKFW9IKJ\\2212.html:text/html;Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\GP5IUD3R\\Somepalli et al. - 2022 - Diffusion Art or Digital Forgery Investigating Da.pdf:application/pdf},
}

@misc{berman_multigrain_2019,
	title = {{MultiGrain}: a unified image embedding for classes and instances},
	shorttitle = {{MultiGrain}},
	url = {http://arxiv.org/abs/1902.05509},
	doi = {10.48550/arXiv.1902.05509},
	abstract = {MultiGrain is a network architecture producing compact vector representations that are suited both for image classification and particular object retrieval. It builds on a standard classification trunk. The top of the network produces an embedding containing coarse and fine-grained information, so that images can be recognized based on the object class, particular object, or if they are distorted copies. Our joint training is simple: we minimize a cross-entropy loss for classification and a ranking loss that determines if two images are identical up to data augmentation, with no need for additional labels. A key component of MultiGrain is a pooling layer that takes advantage of high-resolution images with a network trained at a lower resolution. When fed to a linear classifier, the learned embeddings provide state-of-the-art classification accuracy. For instance, we obtain 79.4\% top-1 accuracy with a ResNet-50 learned on Imagenet, which is a +1.8\% absolute improvement over the AutoAugment method. When compared with the cosine similarity, the same embeddings perform on par with the state-of-the-art for image retrieval at moderate resolutions.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Berman, Maxim and Jégou, Hervé and Vedaldi, Andrea and Kokkinos, Iasonas and Douze, Matthijs},
	month = apr,
	year = {2019},
	note = {arXiv:1902.05509 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\INCX5HNQ\\1902.html:text/html},
}

@inproceedings{kandpal_large_2023,
	title = {Large {Language} {Models} {Struggle} to {Learn} {Long}-{Tail} {Knowledge}},
	url = {https://proceedings.mlr.press/v202/kandpal23a.html},
	abstract = {The Internet contains a wealth of knowledge—from the birthdays of historical figures to tutorials on how to code—all of which may be learned by language models. However, while certain pieces of information are ubiquitous on the web, others appear extremely rarely. In this paper, we study the relationship between the knowledge memorized by large language models and the information in pre-training datasets scraped from the web. In particular, we show that a language model’s ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models are better at learning long-tail knowledge, we estimate that today’s models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data. Finally, we show that retrieval-augmentation can reduce the dependence on relevant pre-training information, presenting a promising approach for capturing the long-tail.},
	language = {en},
	urldate = {2023-07-21},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kandpal, Nikhil and Deng, Haikang and Roberts, Adam and Wallace, Eric and Raffel, Colin},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {15696--15707},
	file = {Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\KC4ZC7KD\\Kandpal et al. - 2023 - Large Language Models Struggle to Learn Long-Tail .pdf:application/pdf},
}

@inproceedings{carlini_secret_2019,
	title = {The {Secret} {Sharer}: {Evaluating} and {Testing} {Unintended} {Memorization} in {Neural} {Networks}},
	isbn = {978-1-939133-06-9},
	shorttitle = {The {Secret} {Sharer}},
	url = {https://www.usenix.org/conference/usenixsecurity19/presentation/carlini},
	language = {en},
	urldate = {2023-07-21},
	author = {Carlini, Nicholas and Liu, Chang and Erlingsson, Úlfar and Kos, Jernej and Song, Dawn},
	year = {2019},
	pages = {267--284},
}

@misc{vyas_provable_2023,
	title = {Provable {Copyright} {Protection} for {Generative} {Models}},
	url = {http://arxiv.org/abs/2302.10870},
	doi = {10.48550/arXiv.2302.10870},
	abstract = {There is a growing concern that learned conditional generative models may output samples that are substantially similar to some copyrighted data \$C\$ that was in their training set. We give a formal definition of \${\textbackslash}textit\{near access-freeness (NAF)\}\$ and prove bounds on the probability that a model satisfying this definition outputs a sample similar to \$C\$, even if \$C\$ is included in its training set. Roughly speaking, a generative model \$p\$ is \${\textbackslash}textit\{\$k\$-NAF\}\$ if for every potentially copyrighted data \$C\$, the output of \$p\$ diverges by at most \$k\$-bits from the output of a model \$q\$ that \${\textbackslash}textit\{did not access \$C\$ at all\}\$. We also give generative model learning algorithms, which efficiently modify the original generative model learning algorithm in a black box manner, that output generative models with strong bounds on the probability of sampling protected content. Furthermore, we provide promising experiments for both language (transformers) and image (diffusion) generative models, showing minimal degradation in output quality while ensuring strong protections against sampling protected content.},
	urldate = {2023-07-22},
	publisher = {arXiv},
	author = {Vyas, Nikhil and Kakade, Sham and Barak, Boaz},
	month = feb,
	year = {2023},
	note = {arXiv:2302.10870 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\A95QYQUG\\Vyas et al. - 2023 - Provable Copyright Protection for Generative Model.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\6GJ9ATAJ\\2302.html:text/html},
}

@inproceedings{kong_data_2023-1,
	title = {Data {Redaction} from {Pre}-trained {GANs}},
	doi = {10.1109/SaTML54575.2023.00048},
	abstract = {Large pre-trained generative models are known to occasionally output undesirable samples, which undermines their trustworthiness. The common way to mitigate this is to re-train them differently from scratch using different data or different regularization - which uses a lot of computational resources and does not always fully address the problem. In this work, we take a different, more compute- friendly approach and investigate how to post-edit a model after training so that it “redacts”, or refrains from outputting certain kinds of samples. We show that redaction is a fundamentally different task from data deletion, and data deletion may not always lead to redaction. We then consider Generative Adversar-ial Networks (GANs), and provide three different algorithms for data redaction that differ on how the samples to be redacted are described. Extensive evaluations on real-world image datasets show that our algorithms out-perform data deletion baselines, and are capable of redacting data while retaining high generation quality at a fraction of the cost of full re- training,},
	booktitle = {2023 {IEEE} {Conference} on {Secure} and {Trustworthy} {Machine} {Learning} ({SaTML})},
	author = {Kong, Zhifeng and Chaudhuri, Kamalika},
	month = feb,
	year = {2023},
	keywords = {Computational modeling, Costs, Data redaction, Deep learning, Machine learning algorithms, post-editing, pre-trained GANs, Systematics, Training, Training data},
	pages = {638--677},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\tsarcevic\\Zotero\\storage\\EPKZVIN6\\10136171.html:text/html;Submitted Version:C\:\\Users\\tsarcevic\\Zotero\\storage\\DEZ43GWS\\Kong and Chaudhuri - 2023 - Data Redaction from Pre-trained GANs.pdf:application/pdf},
}

@misc{rando_red-teaming_2022,
	title = {Red-{Teaming} the {Stable} {Diffusion} {Safety} {Filter}},
	url = {http://arxiv.org/abs/2210.04610},
	abstract = {Stable Diffusion is a recent open-source image generation model comparable to proprietary models such as DALLE, Imagen, or Parti. Stable Diffusion comes with a safety filter that aims to prevent generating explicit images. Unfortunately, the filter is obfuscated and poorly documented. This makes it hard for users to prevent misuse in their applications, and to understand the filter's limitations and improve it. We first show that it is easy to generate disturbing content that bypasses the safety filter. We then reverse-engineer the filter and find that while it aims to prevent sexual content, it ignores violence, gore, and other similarly disturbing content. Based on our analysis, we argue safety measures in future model releases should strive to be fully open and properly documented to stimulate security contributions from the community.},
	urldate = {2023-07-22},
	publisher = {arXiv},
	author = {Rando, Javier and Paleka, Daniel and Lindner, David and Heim, Lennart and Tramèr, Florian},
	month = nov,
	year = {2022},
	note = {arXiv:2210.04610 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\5VHE2VG9\\2210.html:text/html;Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\SFUDYJ53\\Rando et al. - 2022 - Red-Teaming the Stable Diffusion Safety Filter.pdf:application/pdf},
}

@misc{zhang_forget-me-not_2023,
	title = {Forget-{Me}-{Not}: {Learning} to {Forget} in {Text}-to-{Image} {Diffusion} {Models}},
	shorttitle = {Forget-{Me}-{Not}},
	url = {http://arxiv.org/abs/2303.17591},
	doi = {10.48550/arXiv.2303.17591},
	abstract = {The unlearning problem of deep learning models, once primarily an academic concern, has become a prevalent issue in the industry. The significant advances in text-to-image generation techniques have prompted global discussions on privacy, copyright, and safety, as numerous unauthorized personal IDs, content, artistic creations, and potentially harmful materials have been learned by these models and later utilized to generate and distribute uncontrolled content. To address this challenge, we propose {\textbackslash}textbf\{Forget-Me-Not\}, an efficient and low-cost solution designed to safely remove specified IDs, objects, or styles from a well-configured text-to-image model in as little as 30 seconds, without impairing its ability to generate other content. Alongside our method, we introduce the {\textbackslash}textbf\{Memorization Score (M-Score)\} and {\textbackslash}textbf\{ConceptBench\} to measure the models' capacity to generate general concepts, grouped into three primary categories: ID, object, and style. Using M-Score and ConceptBench, we demonstrate that Forget-Me-Not can effectively eliminate targeted concepts while maintaining the model's performance on other concepts. Furthermore, Forget-Me-Not offers two practical extensions: a) removal of potentially harmful or NSFW content, and b) enhancement of model accuracy, inclusion and diversity through {\textbackslash}textbf\{concept correction and disentanglement\}. It can also be adapted as a lightweight model patch for Stable Diffusion, allowing for concept manipulation and convenient distribution. To encourage future research in this critical area and promote the development of safe and inclusive generative models, we will open-source our code and ConceptBench at {\textbackslash}href\{https://github.com/SHI-Labs/Forget-Me-Not\}\{https://github.com/SHI-Labs/Forget-Me-Not\}.},
	urldate = {2023-07-22},
	publisher = {arXiv},
	author = {Zhang, Eric and Wang, Kai and Xu, Xingqian and Wang, Zhangyang and Shi, Humphrey},
	month = mar,
	year = {2023},
	note = {arXiv:2303.17591 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\8KTTFYCR\\Zhang et al. - 2023 - Forget-Me-Not Learning to Forget in Text-to-Image.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\YVG4C5FS\\2303.html:text/html},
}

@misc{wiggers_this_2022,
	title = {This site tells you if photos of you were used to train the {AI}},
	url = {https://techcrunch.com/2022/09/21/who-fed-the-ai/},
	abstract = {Deepfakes, AI-generated porn and a thousand more innocent uses — there’s been a lot of news about neural network-generated images. It makes sense that people started getting curious; were my photos used to train the robots? Are photos of me in the image-generating training sets? A brand new site tries to give you an answer. […]},
	language = {en-US},
	urldate = {2023-07-22},
	journal = {TechCrunch},
	author = {Wiggers, Haje Jan Kamps {and} Kyle},
	month = sep,
	year = {2022},
	file = {Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\SG9B3EPD\\who-fed-the-ai.html:text/html},
}

@misc{noauthor_have_nodate,
	title = {Have {I} {Been} {Trained}?},
	url = {https://haveibeentrained.com/},
        note = {https://haveibeentrained.com/},
        year = {2023},
	urldate = {2023-07-22},
	file = {Have I Been Trained?:C\:\\Users\\tsarcevic\\Zotero\\storage\\XXG44MZL\\haveibeentrained.com.html:text/html},
}

@misc{wiggers_deviantart_2022,
	title = {{DeviantArt} provides a way for artists to opt out of {AI} art generators},
	url = {https://techcrunch.com/2022/11/11/deviantart-provides-a-way-for-artists-to-opt-out-of-ai-art-generators/},
	abstract = {DeviantArt is introducing a way for artists to exclude their work from AI training sets. It's also launching its own generative art tool.},
	language = {en-US},
	urldate = {2023-07-22},
	journal = {TechCrunch},
	author = {Wiggers, Kyle},
	month = nov,
	year = {2022},
	file = {Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\HKDNEBKY\\deviantart-provides-a-way-for-artists-to-opt-out-of-ai-art-generators.html:text/html},
}

@misc{noauthor_secret_nodate,
	title = {The {Secret} {Sharer}: {Evaluating} and {Testing} {Unintended} {Memorization} in {Neural} {Networks} {\textbar} {USENIX}},
	url = {https://www.usenix.org/conference/usenixsecurity19/presentation/carlini},
	urldate = {2023-07-23},
	file = {The Secret Sharer\: Evaluating and Testing Unintended Memorization in Neural Networks | USENIX:C\:\\Users\\tsarcevic\\Zotero\\storage\\7QKHEQBP\\carlini.html:text/html},
}

@misc{noauthor_ayush_nodate,
	title = {Ayush {Sekhari}, {Jayadev} {Acharya}, {Gautam} {Kamath}, and {Ananda} {Theertha} {Suresh}. {Remember} what you want to forget: {Algorithms} for machine unlearning - {Google} {Search}},
	url = {https://www.google.com/search?q=Ayush+Sekhari%2C+Jayadev+Acharya%2C+Gautam+Kamath%2C+and+Ananda+Theertha+Suresh.+Remember+what+you+want+to+forget%3A+Algorithms+for+machine+unlearning&rlz=1C1GCEA_enAT1021AT1021&oq=Ayush+Sekhari%2C+Jayadev+Acharya%2C+Gautam+Kamath%2C+and+Ananda+Theertha+Suresh.+Remember+what+you+want+to+forget%3A+Algorithms+for+machine+unlearning&aqs=chrome..69i57.256j0j7&sourceid=chrome&ie=UTF-8},
	urldate = {2023-07-23},
}

@inproceedings{sekhari_remember_2021,
	title = {Remember {What} {You} {Want} to {Forget}: {Algorithms} for {Machine} {Unlearning}},
	volume = {34},
	shorttitle = {Remember {What} {You} {Want} to {Forget}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/9627c45df543c816a3ddf2d8ea686a99-Abstract.html},
	urldate = {2023-07-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sekhari, Ayush and Acharya, Jayadev and Kamath, Gautam and Suresh, Ananda Theertha},
	year = {2021},
	pages = {18075--18086},
	file = {Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\ELES8NW4\\Sekhari et al. - 2021 - Remember What You Want to Forget Algorithms for M.pdf:application/pdf},
}

@misc{noauthor_machine_nodate,
	title = {Machine {Unlearning}},
	url = {https://ieeexplore.ieee.org/abstract/document/9519428/},
	abstract = {Once users have shared their data online, it is generally difficult for them to revoke access and ask for the data to be deleted. Machine learning (ML) exacerbates this problem because any model trained with said data may have memorized it, putting users at risk of a successful privacy attack exposing their information. Yet, having models unlearn is notoriously difficult.We introduce SISA training, a framework that expedites the unlearning process by strategically limiting the influence of a data point in the training procedure. While our framework is applicable to any learning algorithm, it is designed to achieve the largest improvements for stateful algorithms like stochastic gradient descent for deep neural networks. SISA training reduces the computational overhead associated with unlearning, even in the worst-case setting where unlearning requests are made uniformly across the training set. In some cases, the service provider may have a prior on the distribution of unlearning requests that will be issued by users. We may take this prior into account to partition and order data accordingly, and further decrease overhead from unlearning.Our evaluation spans several datasets from different domains, with corresponding motivations for unlearning. Under no distributional assumptions, for simple learning tasks, we observe that SISA training improves time to unlearn points from the Purchase dataset by 4.63×, and 2.45× for the SVHN dataset, over retraining from scratch. SISA training also provides a speed-up of 1.36× in retraining for complex learning tasks such as ImageNet classification; aided by transfer learning, this results in a small degradation in accuracy. Our work contributes to practical data governance in machine unlearning.},
	language = {en-US},
	urldate = {2023-07-23},
	file = {Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\7NNPCLDI\\9519428.html:text/html},
}

@inproceedings{golatkar_eternal_2020,
	title = {Eternal {Sunshine} of the {Spotless} {Net}: {Selective} {Forgetting} in {Deep} {Networks}},
	shorttitle = {Eternal {Sunshine} of the {Spotless} {Net}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Golatkar_Eternal_Sunshine_of_the_Spotless_Net_Selective_Forgetting_in_Deep_CVPR_2020_paper.html},
	urldate = {2023-07-23},
	author = {Golatkar, Aditya and Achille, Alessandro and Soatto, Stefano},
	year = {2020},
	pages = {9304--9312},
}

@misc{neel_descent--delete_2020,
	title = {Descent-to-{Delete}: {Gradient}-{Based} {Methods} for {Machine} {Unlearning}},
	shorttitle = {Descent-to-{Delete}},
	url = {http://arxiv.org/abs/2007.02923},
	doi = {10.48550/arXiv.2007.02923},
	abstract = {We study the data deletion problem for convex models. By leveraging techniques from convex optimization and reservoir sampling, we give the first data deletion algorithms that are able to handle an arbitrarily long sequence of adversarial updates while promising both per-deletion run-time and steady-state error that do not grow with the length of the update sequence. We also introduce several new conceptual distinctions: for example, we can ask that after a deletion, the entire state maintained by the optimization algorithm is statistically indistinguishable from the state that would have resulted had we retrained, or we can ask for the weaker condition that only the observable output is statistically indistinguishable from the observable output that would have resulted from retraining. We are able to give more efficient deletion algorithms under this weaker deletion criterion.},
	urldate = {2023-07-23},
	publisher = {arXiv},
	author = {Neel, Seth and Roth, Aaron and Sharifi-Malvajerdi, Saeed},
	month = jul,
	year = {2020},
	note = {arXiv:2007.02923 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\SESU6TG8\\Neel et al. - 2020 - Descent-to-Delete Gradient-Based Methods for Mach.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\5WA6BUX4\\2007.html:text/html},
}

@misc{noauthor_piano_nodate,
	title = {Piano • {Client} {Dashboard}},
	url = {https://buy.tinypass.com/checkout/template/cacheableShow?aid=WUOCNSUgpu&templateId=OTK3MV0EY5CW&templateVariantId=OTVSL3D8ZYF30&offerId=fakeOfferId&experienceId=EXMHBEMJU9UF&iframeId=offer_30f4defe339a0d8f7268-0&displayMode=inline&pianoIdUrl=https%3A%2F%2Fauth.technologyreview.com%2Fid%2F&widget=template&url=https%3A%2F%2Fwww.technologyreview.com},
	urldate = {2023-07-23},
	file = {Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\D69JN6XR\\cacheableShow.html:text/html},
}

@misc{nolan_ai_nodate,
	title = {{AI} art generators face separate copyright lawsuits from {Getty} {Images} and a group of artists},
	url = {https://www.businessinsider.com/ai-art-artists-getty-images-lawsuits-stable-diffusion-2023-1},
	abstract = {Three artists have launched a class action against the companies behind Stable Diffusion, Midjourney, and DreamUp.},
	language = {en-US},
	urldate = {2023-07-23},
        month = Jan,
        year = {2023},
	journal = {Business Insider},
	author = {Nolan, Beatrice},
	file = {Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\3B7W6V2J\\ai-art-artists-getty-images-lawsuits-stable-diffusion-2023-1.html:text/html},
}

@misc{noauthor_proposal_2016,
	title = {Proposal for a {DIRECTIVE} {OF} {THE} {EUROPEAN} {PARLIAMENT} {AND} {OF} {THE} {COUNCIL} on copyright in the {Digital} {Single} {Market}},
	url = {https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52016PC0593},
	language = {en},
	urldate = {2023-07-25},
	year = {2016},
	file = {EUR-Lex HTML (EN):C\:\\Users\\tsarcevic\\Zotero\\storage\\6RXSU7KJ\\HTML.html:text/html;EUR-Lex PDF (EN):C\:\\Users\\tsarcevic\\Zotero\\storage\\5726GPQV\\2016 - Proposal for a DIRECTIVE OF THE EUROPEAN PARLIAMEN.pdf:application/pdf},
}

@inproceedings{rombach_high-resolution_2022,
	title = {High-{Resolution} {Image} {Synthesis} {With} {Latent} {Diffusion} {Models}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-07-26},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	year = {2022},
	pages = {10684--10695},
}

@misc{ramesh_hierarchical_2022,
	title = {Hierarchical {Text}-{Conditional} {Image} {Generation} with {CLIP} {Latents}},
	url = {http://arxiv.org/abs/2204.06125},
	doi = {10.48550/arXiv.2204.06125},
	abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
	urldate = {2023-07-26},
	publisher = {arXiv},
	author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
	month = apr,
	year = {2022},
	note = {arXiv:2204.06125 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\FDSU47YP\\Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation wit.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\TWFTR26A\\2204.html:text/html},
}

@misc{noauthor_dalle_nodate,
	title = {{DALL}·{E} 2},
	url = {https://openai.com/dall-e-2},
	abstract = {DALL·E 2 is an AI system that can create realistic images and art from a description in natural language.},
	language = {en-US},
	urldate = {2023-07-26},
	file = {Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\ZXN6ZEYJ\\dall-e-2.html:text/html},
}

@misc{noauthor_dalle2_nodate,
	title = {dalle2},
	url = {http://adityaramesh.com/posts/dalle2/dalle2.html},
	urldate = {2023-07-26},
	file = {dalle2:C\:\\Users\\tsarcevic\\Zotero\\storage\\AUHL4PWJ\\dalle2.html:text/html},
}

@misc{noauthor_stable_nodate,
	title = {Stable {Diffusion} {Online}},
	url = {https://stablediffusionweb.com/},
	urldate = {2023-07-26},
	file = {Stable Diffusion Online:C\:\\Users\\tsarcevic\\Zotero\\storage\\VLSQBWF9\\stablediffusionweb.com.html:text/html},
}

@misc{noauthor_musenet_nodate,
	title = {{MuseNet}},
	url = {https://openai.com/research/musenet},
        note = {https://openai.com/research/musenet},
        year = {2023},
	abstract = {We’ve created MuseNet, a deep neural network that can generate 4-minute musical compositions with 10 different instruments, and can combine styles from country to Mozart to the Beatles. MuseNet was not explicitly programmed with our understanding of music, but instead discovered patterns of harmony, rhythm, and style by learning to predict the next token in hundreds of thousands of MIDI files. MuseNet uses the same general-purpose unsupervised technology as GPT-2, a large-scale transformer model trained to predict the next token in a sequence, whether audio or text.},
	language = {en-US},
	urldate = {2023-07-26},
	file = {Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\CNTNJ37P\\musenet.html:text/html},
}

@misc{openai_gpt-4_2023,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2023-07-26},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\EK7VW63E\\2303.html:text/html},
}

@misc{noauthor_midjourney_nodate,
	title = {Midjourney},
	url = {https://www.midjourney.com/home/?callbackUrl=%2Fapp%2F},
        note = {https://www.midjourney.com/},
        year = {2023},
	abstract = {An independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species.},
	urldate = {2023-07-26},
	journal = {Midjourney},
	file = {Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\HA8XTPIL\\home.html:text/html},
}

@misc{noauthor_deviantart_nodate,
	title = {{DeviantArt} - {The} {Largest} {Online} {Art} {Gallery} and {Community}},
	url = {https://www.deviantart.com/},
        note = {https://www.deviantart.com/},
        year = {2023},
	urldate = {2023-07-26},
	file = {DeviantArt - The Largest Online Art Gallery and Community:C\:\\Users\\tsarcevic\\Zotero\\storage\\8GQVFBF6\\www.deviantart.com.html:text/html},
}

@misc{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	doi = {10.48550/arXiv.2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
	urldate = {2023-07-26},
	publisher = {arXiv},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv:2006.11239 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\FF9GLMCE\\Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\BJLFHX9E\\2006.html:text/html},
}

@inproceedings{ho_denoising_2020-1,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
	urldate = {2023-07-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	year = {2020},
	pages = {6840--6851},
}

@inproceedings{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Nets}},
	volume = {27},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html},
	abstract = {We propose a new framework for estimating generative models via adversarial nets, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitatively evaluation of the generated samples.},
	urldate = {2023-07-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	year = {2014},
	file = {Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\66CK4G5Y\\Goodfellow et al. - 2014 - Generative Adversarial Nets.pdf:application/pdf},
}
@misc{li2022blip,
      title={BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation}, 
      author={Junnan Li and Dongxu Li and Caiming Xiong and Steven Hoi},
      year={2022},
      eprint={2201.12086},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2023-07-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\FAC7CGTU\\Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@misc{radford_learning_2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {http://arxiv.org/abs/2103.00020},
	doi = {10.48550/arXiv.2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
	urldate = {2023-07-26},
	publisher = {arXiv},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = feb,
	year = {2021},
	note = {arXiv:2103.00020 [cs]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\tsarcevic\\Zotero\\storage\\4JBN8PQY\\2103.html:text/html},
}

@inproceedings{radford_learning_2021-1,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {https://proceedings.mlr.press/v139/radford21a.html},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
	language = {en},
	urldate = {2023-07-26},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {8748--8763},
	file = {Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\DDNBWIUW\\Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf:application/pdf;Supplementary PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\6PAHVC9A\\Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf:application/pdf},
}

@misc{noauthor_ai-generated_nodate,
	title = {{AI}-{Generated} {Art} {Won} a {Prize}. {Artists} {Aren}’t {Happy}. - {The} {New} {York} {Times}},
	url = {https://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html},
	urldate = {2023-07-26},
        author = {Kevin Roose},
        month = sep,
        year = {2022},
	file = {AI-Generated Art Won a Prize. Artists Aren’t Happy. - The New York Times:C\:\\Users\\tsarcevic\\Zotero\\storage\\UDYAQEGA\\ai-artificial-intelligence-artists.html:text/html},
}


@incollection{smits_generative_2022,
	address = {The Hague},
	series = {Information {Technology} and {Law} {Series}},
	title = {Generative {AI} and {Intellectual} {Property} {Rights}},
	isbn = {978-94-6265-523-2},
	url = {https://doi.org/10.1007/978-94-6265-523-2_17},
	abstract = {Since the inception of AI, researchers have tried to generate novel worksWork in media ranging from musicWorksMusic through text to images. The quality of worksWork produced by generativeAIGenerative AI-systems is starting to reach levels that make them usable in contexts where until now human creations are employed. In addition, new contexts are emerging in which humans unskilled in a creative domain can generate worksWork by cooperating with generative AI-toolsAITools. GenerativeAIGenerative AI could lead to an abundance of individually customized content, where worksWork are generated for a particular user in a specific situation and presented once, perhaps never to be repeated again. These developments challenge core concepts of Intellectual Property Rights: “authorship”Authorship obviously, but also “work”Work. Although the content produced by generative systems is new, these systems are often trained on a corpus of (parts of) existing worksWork produced by humans. Hence, practices of (un)authorised imitation need to be considered. In this chapter we want to study these questions, which are emerging in all creative domains, with generativeAIGenerative AI for musicWorksMusic as the central example.},
	language = {en},
	urldate = {2023-06-26},
	booktitle = {Law and {Artificial} {Intelligence}: {Regulating} {AI} and {Applying} {AI} in {Legal} {Practice}},
	publisher = {T.M.C. Asser Press},
	author = {Smits, Jan and Borghuis, Tijn},
	editor = {Custers, Bart and Fosch-Villaronga, Eduard},
	year = {2022},
	doi = {10.1007/978-94-6265-523-2_17},
	keywords = {Authorship, Era of Abundance, Generative AI, Human-Al Cooperation, Public Domain, Unauthorized Imitation, Work},
	pages = {323--344},
	file = {Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\28ZCHAPJ\\Smits and Borghuis - 2022 - Generative AI and Intellectual Property Rights.pdf:application/pdf},
}

@misc{hristov_artificial_2020,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Artificial {Intelligence} and the {Copyright} {Survey}},
	url = {https://papers.ssrn.com/abstract=3490458},
	doi = {10.2139/ssrn.3490458},
	abstract = {Artificial intelligence has emerged as a key contributor to American social, economic, and cultural development. Intelligent software increasingly plays a greater role in every creative industry. These industries rely on intellectual property protections to maintain equilibrium between productivity, remuneration, and competitiveness. American policymakers, however, have paid little attention to the intersection of artificial intelligence and copyright protection. This study collects data from fifty-seven AI scientists, tech policy experts, and copyright scholars through a survey and questionnaire. The data shows that while intelligent software is an important contributor to American cultural development, half of respondents believe that the US Copyright Office is not prepared to deal with an influx of computer-generated works. In light of rapid developments in artificial intelligence, this could present a serious challenge to the American copyright system and future advancements in the AI industry.},
	language = {en},
	urldate = {2023-11-23},
	author = {Hristov, Kalin},
	month = apr,
	year = {2020},
	keywords = {AI, Artificial Intelligence, Author, Authorship, Copyright, Copyright Act, Employee, Employer, Intellectual Property, IP, Machine Learning, Owner, Programmer, Questionnaire, Survey, United States of America, Work Made for Hire},
	file = {Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\L487T6XM\\Hristov - 2020 - Artificial Intelligence and the Copyright Survey.pdf:application/pdf},
}

@misc{cheng_innovae_2022,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {{InnoVAE}: {Generative} {AI} for {Mapping} {Patents} and {Firm} {Innovation}},
	shorttitle = {{InnoVAE}},
	url = {https://papers.ssrn.com/abstract=3868599},
	doi = {10.2139/ssrn.3868599},
	abstract = {We propose a generative AI approach (InnoVAE) to map unstructured patent text into an interpretable, spatial representation of firms' innovative activities. InnoVAE learns a vector representation of patents and places them within a "disentangled" space to facilitate managerial intuition and action. After validating the internal consistency of our approach, we apply it to three decades of AI patents to show that it can be used to construct interpretable measures, at scale, that characterize firms' AI-based IP portfolios. We demonstrate three use cases including (1) generating technology landscapes that inform businesses about their competitive positions, (2) engineering new, intuitive features from unstructured text that facilitate analysis of patent activity, and (3) augmenting patent applications to mitigate the risk of patent rejection. Our approach demonstrates the potential of generative AI methods to make actionable the vast quantities of text stored in unstructured corporate databases.},
	language = {en},
	urldate = {2023-11-23},
	author = {Cheng, Zhaoqi and Lee, Dokyun and Tambe, Prasanna},
	month = mar,
	year = {2022},
	keywords = {ChatGPT, economics of innovation, generative AI, interpretability, large language models, machine learning, managerial decision support, patents},
	file = {Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\LIQG4AQR\\Cheng et al. - 2022 - InnoVAE Generative AI for Mapping Patents and Fir.pdf:application/pdf},
}

@misc{chesterman_good_2023,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Good {Models} {Borrow}, {Great} {Models} {Steal}: {Intellectual} {Property} {Rights} and {Generative} {AI}},
	shorttitle = {Good {Models} {Borrow}, {Great} {Models} {Steal}},
	url = {https://papers.ssrn.com/abstract=4590006},
	doi = {10.2139/ssrn.4590006},
	abstract = {Two critical policy questions will determine the impact of generative AI on the knowledge economy and the creative sector.The first concerns how we think about the training of such models — in particular, whether the creators or owners of the data that are “scraped” (lawfully or unlawfully, with or without permission) should be compensated for that use.The second question revolves around the ownership of the output generated by AI, which is continually improving in quality and scale. These questions are inherently linked to the realm of intellectual property: a legal framework designed to incentivize and reward only human creativity and innovation. For some years, however, the United Kingdom has maintained a distinct category with limited rights for “computer-generated” outputs; on the input issue, the European Union and Singapore have recently introduced exceptions allowing for text and data mining (TDM) or computational data analysis of existing works.The third section of this article explores the broader implications of these policy choices, weighing the advantages of reducing the cost of content creation and the value of expertise against the potential risk to various careers and sectors of the economy, which may be rendered unsustainable.Lessons may be found in the music industry, which also went through a period of unrestrained piracy in the early digital era, epitomized by the rise and fall of the file-sharing service Napster. Similar litigation and legislation may help navigate the present uncertainty, along with an emerging market for “legitimate” models that respect the copyright of humans and are clear about the provenance of their own creations.},
	language = {en},
	urldate = {2023-11-23},
	author = {Chesterman, Simon},
	month = oct,
	year = {2023},
	keywords = {Artificial Intelligence, ChatGPT, Copyright, Generative AI, GPT-4, Intellectual Property, Large Language Models},
	file = {Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\XPWX7739\\Chesterman - 2023 - Good Models Borrow, Great Models Steal Intellectu.pdf:application/pdf},
}

@article{senftleben_generative_2023,
	title = {Generative {AI} and {Author} {Remuneration}},
	issn = {2195-0237},
	url = {https://doi.org/10.1007/s40319-023-01399-4},
	doi = {10.1007/s40319-023-01399-4},
	abstract = {With the evolution of generative AI systems, machine-made productions in the literary and artistic field have reached a level of refinement that allows them to replace human creations. The increasing sophistication of AI systems will inevitably disrupt the market for human literary and artistic works. Generative AI systems provide literary and artistic output much faster and cheaper. It is therefore foreseeable that human authors will be exposed to substitution effects. They may lose income as they are replaced by machines in sectors ranging from journalism and writing to music and visual arts. Considering this trend, the question arises whether it is advisable to take measures to compensate human authors for the reduction in their market share and income. Copyright law could serve as a tool to introduce an AI levy system and ensure the payment of equitable remuneration. In combination with mandatory collective rights management, the new revenue stream could be used to finance social and cultural funds that improve the working and living conditions of flesh-and-blood authors.},
	language = {en},
	urldate = {2023-11-23},
	journal = {IIC - International Review of Intellectual Property and Competition Law},
	author = {Senftleben, Martin},
	month = nov,
	year = {2023},
	keywords = {Art autonomy, Collective rights management, Copyright, Domaine public payant, Equitable remuneration, Freedom of expression, Levy system, Reservation of rights, Text and data mining, Three-step test},
	file = {Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\CHSKNWIH\\Senftleben - 2023 - Generative AI and Author Remuneration.pdf:application/pdf},
}

@article{fui-hoon_nah_generative_2023,
	title = {Generative {AI} and {ChatGPT}: {Applications}, challenges, and {AI}-human collaboration},
	volume = {25},
	issn = {1522-8053},
	shorttitle = {Generative {AI} and {ChatGPT}},
	url = {https://doi.org/10.1080/15228053.2023.2233814},
	doi = {10.1080/15228053.2023.2233814},
	number = {3},
	urldate = {2023-11-23},
	journal = {Journal of Information Technology Case and Application Research},
	author = {Fui-Hoon Nah, Fiona and Zheng, Ruilin and Cai, Jingyuan and Siau, Keng and Chen, Langtao},
	month = jul,
	year = {2023},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/15228053.2023.2233814},
	pages = {277--304},
	file = {Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\TZRL4XY5\\Fui-Hoon Nah et al. - 2023 - Generative AI and ChatGPT Applications, challenge.pdf:application/pdf},
}

@article{dermawan_text_nodate,
	title = {Text and data mining exceptions in the development of generative {AI} models: {What} the {EU} member states could learn from the {Japanese} “nonenjoyment” purposes?},
	volume = {n/a},
	copyright = {© 2023 The Authors. The Journal of World Intellectual Property published by John Wiley \& Sons Ltd.},
	issn = {1747-1796},
	shorttitle = {Text and data mining exceptions in the development of generative {AI} models},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jwip.12285},
	doi = {10.1111/jwip.12285},
	abstract = {The European Union (EU) text and data mining (TDM) provisions are a progressive move, but the horizon is still uncertain for both generative artificial intelligence (GenAI) models researchers and developers. This article suggests that to drive innovation and further the commitment to the digital single market, during the national implementation, EU Member States could consider taking the Japanese broad, all-encompassing and “nonenjoyment-based” TDM as an example. The Japanese “nonenjoyment” purposes, however, are not foreign to the European continental view of copyright. A similar concept can be found under the German concept of “Freier Werkgenuss” or enjoyment of the work. A flexible TDM exception built upon the German notion of nonenjoyment purposes could become an opening clause to foster innovation and creativity in the age of GenAI. Moreover, the article argues that an opening clause allowing TDM with “nonenjoyment” purposes could be permissible under the so-called three-step test. This article further suggests, if there is no political will to safeguard “the right to read should be the right to mine” and to provide a welcoming environment for GenAI researchers and developers, when shaping the legal interpretation through national case law, the EU Member States could consider the following: (1) advocate for 72 h of response if technological protection measures (TPMs) are preventing TDM, and (2) Robot Exclusion Standard (robot.txt) as a warning when TDM is not allowed on a website. It is now in the hands of the EU Member States, whether to protect the interests of rightholders or to create a balance between safeguarding “the right to read should be the right to mine,” protecting rightholders exclusivity, and creating a supportive environment for the GenAI models researcher and developers.},
	language = {en},
	number = {n/a},
	urldate = {2023-11-23},
	journal = {The Journal of World Intellectual Property},
	author = {Dermawan, Artha},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jwip.12285},
	keywords = {copyright and related rights, freier werkgenuss, generative AI models, innovation, text and data mining},
	file = {Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\6QXDX659\\Dermawan - Text and data mining exceptions in the development.pdf:application/pdf},
}

@misc{rodriguez_maffioli_copyright_2023,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Copyright in {Generative} {AI} training: {Balancing} {Fair} {Use} through {Standardization} and {Transparency}},
	shorttitle = {Copyright in {Generative} {AI} training},
	url = {https://papers.ssrn.com/abstract=4579322},
	doi = {10.2139/ssrn.4579322},
	abstract = {The rapid evolution of Generative Artificial Intelligence (GAI) has brought about transformative changes across industries, often raising challenging questions surrounding data rights, especially within the context of copyrighted content. This paper delves into the nuances of the relationship between GAI and the fair use doctrine, highlighting the complexities that emerge when copyrighted data serves as the backbone for the development of large-scale AI models. By combining Benjamin Sobel’s training data taxonomy with the distinct stages of the Generative AI cycle, a hybrid framework is presented, offering a more granulated perspective on the applicability of fair use in GAI contexts. Recognizing the inherent limitations of the current legal paradigms, the paper introduces actionable proposals, emphasizing the need for enhanced transparency, data provenance measures, and the implementation of Standardized Data Licensing Agreements (SDLAs). Such measures aim to bridge the gap between AI developers and copyright holders, facilitating smoother negotiations and fostering trust. While the core discussion revolves around the interplay of GAI and fair use, the paper acknowledges broader policy challenges in the AI domain, urging for continuous exploration. Overall, this work underscores the necessity of adaptive, collaborative, and transparent strategies in harmonizing the objectives of innovation with the imperatives of intellectual property rights in the GAI landscape.},
	language = {en},
	urldate = {2023-11-23},
	author = {Rodriguez Maffioli, Daniel},
	month = aug,
	year = {2023},
	keywords = {AI, AI training, artificial intelligence, copyright, data, data provenance, fair use, generative AI, generative artificial intelligence, scraping, standardization, transparency},
	file = {Full Text PDF:C\:\\Users\\tsarcevic\\Zotero\\storage\\RQRMSNEU\\Rodriguez Maffioli - 2023 - Copyright in Generative AI training Balancing Fai.pdf:application/pdf},
}

@inproceedings{yoon_dmgeneralizewhentheyfailtomemorize_2023,
title={Diffusion Probabilistic Models Generalize when They Fail to Memorize},
author={TaeHo Yoon and Joo Young Choi and Sehyun Kwon and Ernest K. Ryu},
booktitle={ICML 2023 Workshop on Structured Probabilistic Inference {\&} Generative Modeling},
year={2023},
url={https://openreview.net/forum?id=shciCbSk9h}
}

@misc{gu_memorization_2023,
      title={On Memorization in Diffusion Models}, 
      author={Xiangming Gu and Chao Du and Tianyu Pang and Chongxuan Li and Min Lin and Ye Wang},
      year={2023},
      eprint={2310.02664},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{pizzi_sscd_2022,
  title={A Self-Supervised Descriptor for Image Copy Detection},
  author={Pizzi, Ed and Roy, Sreya Dutta and Ravindra, Sugosh Nagavara and Goyal, Priya and Douze, Matthijs},
  journal={Proc. CVPR},
  year={2022}
}

@article{radford_clip_2021,
  author       = {Alec Radford and
                  Jong Wook Kim and
                  Chris Hallacy and
                  Aditya Ramesh and
                  Gabriel Goh and
                  Sandhini Agarwal and
                  Girish Sastry and
                  Amanda Askell and
                  Pamela Mishkin and
                  Jack Clark and
                  Gretchen Krueger and
                  Ilya Sutskever},
  title        = {Learning Transferable Visual Models From Natural Language Supervision},
  journal      = {CoRR},
  volume       = {abs/2103.00020},
  year         = {2021},
  url          = {https://arxiv.org/abs/2103.00020},
  eprinttype    = {arXiv},
  eprint       = {2103.00020},
  timestamp    = {Thu, 04 Mar 2021 17:00:40 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2103-00020.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{vandenburg_mem_202,
  title={On Memorization in Probabilistic Deep Generative Models},
  author={Gerrit J. J. van den Burg and Christopher K. I. Williams},
  booktitle={Neural Information Processing Systems},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:235359053}
}